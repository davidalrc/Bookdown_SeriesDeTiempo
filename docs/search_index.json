[["index.html", "Pronóstico de precios mayoristas de alimentos del grupo de verduras y hortalizas haciendo uso de boletines semanales del Sistema de Información de Precios y Abastecimiento del Sector Agropecuario (SIPSA) Capítulo 1 Justificación y fuentes de información", " Pronóstico de precios mayoristas de alimentos del grupo de verduras y hortalizas haciendo uso de boletines semanales del Sistema de Información de Precios y Abastecimiento del Sector Agropecuario (SIPSA) David Alejandro Rivera Correa 2023-09-10 Capítulo 1 Justificación y fuentes de información En la actualidad conocer en detalle la dinámica del precio de los alimentos resulta ser un elemento valioso para la toma de decisiones, tanto a nivel de quienes viven del agro, como para los consumidores finales; si bien los precios mayoristas representan un eslabón de la formación del precio de los alimentos, es preciso resaltar que este valor representa el ultimo tramo de formación antes de la distribución final a nivel minorista, bajo esta lógica la posibilidad de pronosticar los precios mayoristas de los alimentos permite hacer un acercamiento a posibles escenarios de privación alimentaria futura ligada al poder adquisitivo de los hogares, esto teniendo en cuenta que para el caso de alimentos perecederos como las verduras y hortalizas el mecanismo de transmisión del mercado mayorista al mercado minorista resulta ser mucho más acelerada. En concordancia a lo anterior, en este ejercicio se tomarán las series de tiempo de precios semanales de hortalizas basicas de consumo frecuente para los municipios de Armenia y Pereira: Cebolla Junca Tomate chonto Habichuela Ahuyama Las series de tiempo en mención serán tomadas de los boletines semanales del SIPSA que emite el Departamento Nacional de Estadística (DANE) comprendiendo el periodo 2016-2022 buscando realizar pronósticos de las 4 semanas siguientes de los productos de referencia. Para relacionar el precio mayorista con el precio minorista se tomara como referencia la Canasta Básica de Salud Ailmentaria (CABASA) a través de la cual se proyectarán por inflación los precios minoristas con el fin de contrastarlos con la dinámica del precio mayorista mostrado por el mercado y por los periodos de pronóstico, asi mismo se proyectará el valor total de la canasta básica estándar por medio de la inflación y se validará como varia la proporción de la participación monetaria de los 4 alimentos con el fin de determinar como se ve impactado el poder adquisitivo en relación a estos alimentos de referencia tomando siempre como base la participación relativa inicial respecto a las participaciones relativas futuras respecto al precio que toman sobre el valor total de la canasta. Al final del ejercicio se podrá evidenciar que tanto ha variado el esfuerzo monetario para preservar estas hortalizas básicas en la canasta y que tanto será el esfuerzo en las semanas pronosticadas. Conocer las variaciones de la participación monetaria de los alimentos en la canasta básica permite evidenciar que tanto menos dinero disponible tendrá el consumidor luego de comprar una cantidad “n” de un alimento, por lo tanto, si la participación de ciertos alimentos incrementa de forma desmedida la preservación de las cantidades iniciales consumidas supondrá un esfuerzo mayor y una cantidad restante menor para acceder a los demás alimentos que conforman la canasta completa, manteniendo las cantidades consumidas constantes, esto supondrá finalmente una reducción de las cantidades consumidas o inclusive la eliminación o sustitución de algunos alimentos dentro de la dieta. El ejercicio de pronóstico y análisis propuesto representa un ejercicio por medio del cual es posible acercarse al impacto del aumento de los precios mayoristas en el consumidor final lo cual se relaciona directamente con la seguridad alimentaria ligada al acceso desde una perspectiva monetaria. Para la ejecución de este ejercicio se llevarán a cabo los siguientes pasos que demarcarán la estructura del documento: Recolección, carga y transformación de los datos de precios semanales Análisis exploratorio de las series de tiempo Modelación para el pronóstico de las series de tiempo Evaluación y ajuste de hiperparámetros de los modelos Contraste entre series de tiempo y proyección de precios minoristas Análisis de la participación conjunta del grupo de 4 hortalizas sobre el valor total de la CABASA Conclusiones y recomendaciones "],["recolección-carga-y-transformación-de-los-datos-de-precios-semanales.html", "Capítulo 2 Recolección, carga y transformación de los datos de precios semanales", " Capítulo 2 Recolección, carga y transformación de los datos de precios semanales Para desarrollar los elementos mencionados en la sección introductoria es relevante tener en cuenta que el SIPSA presenta diferentes tipos de boletín, presenta boletines diarios para un conjunto predefinido de alimentos conformados por frutas, verduras y hortalizas, y seguidamente publica boletines semanales en donde esta presente una variedad mucho más amplia de alimentos, para el caso de este trabajo se hará uso de los boletines generados semanalmente. Para la recolección de los boletines semanales se llevará a cabo una descarga directa de los boletines desde el portal del SIPSA en donde se encuentran los boletines publicados semanalmente desde 2012 hasta 2023, como se ha mencionado desde un inicio aqui se tomará como referencia el periodo 2016-2023 tomando como corte el mes de Junio del año en curso; el proceso de recolección de la información da como resultado múltiples archivos en formato excel (.xlsx) con diferentes formatos y disposiciones a nivel de la distribución de información, ante lo cual se hace necesario realizar un procesamiento de todos los archivos resultantes de tal manera que se logre una fuente consolidada con la cual sea mucho más sencillo trabajar. El análisis detallado de los archivos descargados permite evidenciar que los boletines tienen algunos atributos particulares: La estructura de la tabla de los boletines no contiene explicitamente una fecha,el rango de fecha del archivo se encuentra en el nombre del fichero descargado, por ejemplo “Sem_2ene_8ene_2016.xls”. La estructura de los boletines no es uniforme a través del tiempo, entre 2016 hasta el 11 de Mayo de 2018 la estructura de los boletines resulta ser más simple ya que el archivo no contiene paginación y cuenta con titulos intermedios para separar los grupos de alimentos. Desde el 11 de Mayo la estructura cambia a documentos de excel paginados por tipo de alimento involucrando inclusive hojas adicionales relacionadas a temas de disponibilidad por asuntos de cantidades ofrecidas en la central de abastos. Para lograr un procesamiento eficiente es necesario realizar operaciones en ciclo para los archivos de tal forma que se logre llegar a un set de datos consolidado. A continuación se muestra un preview del aspecto de los sets de datos resultantes al leer un boletin tipo 1 y tipo 2: Ejemplo boletín tipo 1: import pandas as pd import numpy as np import os import openpyxl import matplotlib.pyplot as plt import seaborn as sns ## En este caso vamos a leer un boletin de 2016 archivo = pd.read_excel(&quot;C://Users//DAVID//Documents//SeriesDeTiempo_MCD2023//bookdown-seriesdetiempo_mcd//Bookdown_SeriesDeTiempo//Historico_Precios_SIPSA//2016//Sem_2ene_8ene_2016.xls&quot;,skiprows=1) archivo.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 4839 entries, 0 to 4838 ## Data columns (total 5 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Productos y mercados 4839 non-null object ## 1 Precio mínimo 4478 non-null object ## 2 Precio máximo 4478 non-null object ## 3 Precio medio 4478 non-null object ## 4 Tendencia 4478 non-null object ## dtypes: object(5) ## memory usage: 189.1+ KB archivo.head() ## Productos y mercados Precio mínimo ... Precio medio Tendencia ## 0 Acelga NaN ... NaN NaN ## 1 Armenia, Mercar 1333 ... 1400 + ## 2 Barranquilla, Barranquillita 2800 ... 3000 = ## 3 Bogotá, D.C., Corabastos 500 ... 780 - ## 4 Bucaramanga, Centroabastos 1000 ... 1000 -- ## ## [5 rows x 5 columns] Como puede observarse esta primera versión de boletión solo cuenta con 5 columnas, particularmente este tipo de boletin mezcla en una misma columna los productos y los mercados, los productos aparecen una vez se han listado todos los mercados en los que están disponibles y se caracterizan por tener valores vacios en todas las columnas cuando su nombre aparece, seguidamente, como se mencionó no hay una fecha explicita por lo tanto es necesario usar la fecha que esta demarcada en el archivo para poder ubicar los datos en el tiempo. Ejemplo boletín tipo 2: Para la lectura de este boletín es necesario utilizar varios argumentos adicionales de la función pd.read_excel, de hecho para lograr una extracción completa de los datos es necesario llevar a cabo un ciclo for, no obstante en este caso solo veremos de manera general la estructura de una hoja y los argumentos que es necesario introducir: archivo2= pd.read_excel(&quot;C://Users//DAVID//Documents//SeriesDeTiempo_MCD2023//bookdown-seriesdetiempo_mcd//Bookdown_SeriesDeTiempo//Historico_Precios_SIPSA//2018//Boletin_Tipo2//Sem_01dic__07dic_2018.xlsx&quot;,skiprows=9,sheet_name=1).dropna(axis=1,how=&quot;all&quot;) archivo2.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 945 entries, 0 to 944 ## Data columns (total 6 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Producto 945 non-null object ## 1 Mercado mayorista 945 non-null object ## 2 Precio mínimo 945 non-null int64 ## 3 Precio máximo 945 non-null int64 ## 4 Precio medio 945 non-null int64 ## 5 Tendencia 945 non-null object ## dtypes: int64(3), object(3) ## memory usage: 44.4+ KB archivo2.head() ## Producto Mercado mayorista ... Precio medio Tendencia ## 0 Acelga Armenia, Mercar ... 1022 -- ## 1 Acelga Barranquilla, Barranquillita ... 3067 + ## 2 Acelga Bogotá, D.C., Corabastos ... 583 + ## 3 Acelga Bucaramanga, Centroabastos ... 1125 ++ ## 4 Acelga Chiquinquirá (Boyacá) ... 1067 = ## ## [5 rows x 6 columns] Se puede apreciar que a diferencia del boletin 1, este boletín separa el producto del mercado mayorista lo cual lo acerca a la estructura ideal del set de datos, no obstante la complejidad de este conjunto de información es que involucra multiples hojas por cada archivo procesado, y además cada hoja en ocasiones tiene ciertas particularidades. Para facilitar el ejercicio, toda la labor de pre-procesamiento y limpieza inicial han sido realizadas previamente, en esta sección se explican las funciones diseñadas para llevar a cabo este proceso, las porciones de codigo aqui mostradas no son ejecutables simplemente son ilustrativas. Los paquetes utilizados para llevar a cabo el procesamiento de los boletines han sido pandas, numpy y os. 2.0.1 Funciones para el pre-procesamiento y limpieza de los boletines para el periodo 2016 a 11 de Mayo de 2018 La primera función que se crea se denomina bolsipsatipo1_proc(ruta) el argumento de esta función es la ruta en la cual se encuentra el boletin a procesar, a continuación se muestra el codigo de la función: def bolsipsatipo1_proc(ruta): nombre=os.path.split(ruta)[1].replace(&quot;.xlsx&quot; or &quot;.xls&quot;,&quot;&quot;) boletin= pd.read_excel(ruta,skiprows=1) boletin[&quot;periodo&quot;]=nombre boletin= boletin[(boletin[&quot;Productos y mercados&quot;].str.find(&quot;Cuadro&quot;)==-1)\\ &amp; (boletin[&quot;Productos y mercados&quot;].str.find(&quot;Productos&quot;)==-1)] boletin[&quot;Tendencia&quot;]=boletin[&quot;Tendencia&quot;].fillna(&quot;Producto&quot;) boletin[&quot;Producto&quot;]=np.nan producto=None for index in boletin.index: if boletin.loc[index,&quot;Tendencia&quot;]==&quot;Producto&quot;: producto=boletin.loc[index,&quot;Productos y mercados&quot;] boletin.loc[index,&quot;Producto&quot;]= producto else: boletin.loc[index,&quot;Producto&quot;]= producto boletin= boletin[(boletin[&quot;Tendencia&quot;]!=&quot;Producto&quot;)] boletin= boletin.rename(columns={&quot;Productos y mercados&quot;:&quot;Mercado mayorista&quot;}) return boletin Esta primera función inicialmente captura el nombre del archivo removiendo la extensión, posteriormente realiza una lectura del archivo que hay en la ruta omitiendo la lectura de la primera fila y adicionalmente crea una columna denominada periodo en donde se coloca el nombre extraido del archivo que esta siendo procesado, seguidamente omite las filas que contengan los strings “Cuadro” y “Producto” creando una columna denominada tendencia en donde copia el contenido de la columna y lo rellena con la palabra “Producto” en los espacios en blanco, finalmente se crea una columna denominada producto con valores “nan” la finalidad de esta columna es lograr separar el nombre del mercado mayorista del producto. Para lograr separar el nombre del producto de la columna “Productos y mercados” se recorre el dataset y se pone el nombre del producto cada vez que aparece el nombre “Productos y mercados” a partir de esto se comienza a poblar la columna producto en donde queda aislado el nombre del producto; finalmente se eliminan las columnas que quedaron con el nombre “Producto” y se renombra la columna “Productos y mercados” y pasa a llamarse “Mercado mayorista”, finalmente la función devuelve un dataframe denominado “boletin”. Esta primera función representa el procesamiento de una ruta que haga referencia a un archivo, no obstante para lograr un procesamiento en serie es relevante lograr un procesamiento “por carpeta” que permita procesar directamente los archivos por año, para logra esto se crea la función proc_carpetassipsat1(ruta_carpeta) cuyo argumento es la ruta a una carpeta en particular, el código de la función es el siguiente: def proc_carpetassipsat1(ruta_carpeta): ficheros=os.listdir(ruta_carpeta) lista_rutas=[] ruta_entera=None for file in ficheros: ruta_entera= ruta_carpeta+&quot;\\\\&quot;+file lista_rutas.append(ruta_entera) lista_dataframes=[] for ruta in lista_rutas: print(ruta) boletin_procesado=bolsipsatipo1_proc(ruta) lista_dataframes.append(boletin_procesado) boletin_consolidado= pd.concat(lista_dataframes) return boletin_consolidado La función anterior toma la ruta de una carpeta y partir del paquete “os” genera una lista de los archivos que hay dentro de la misma, asi mismo, al inicio se crea una lista vacia de rutas y una variable denominada ruta_entera que servirá para almacenar la ruta completa de cada archivo individual que hay en la carpeta, una vez hecho esto un ciclo for recorre la lista de ficheros que hay en la carpeta y por medio de esta crea la ruta de cada archivo que es almacenada en la variable “lista_rutas” que finalmente contendrá la ruta individual de cada archivo. Una vez se ejecuta el primer ciclo for se declara la variable lista_dataframes que almacenerá cada uno de los boletines procesados; una vez declarada la variable antes mencionada inicia un nuevo ciclo for que recorre “lista_rutas” en donde a cada archivo individual se le aplica la función bolsipsatipo1_proc() y cada dataframe resultante se almacena en la lista “lista_dataframes” que luego es concatenada dando lugar a un dataframe consolidado para los reportes que reposaban en la carpeta. Finalmente para optimizar aun más el procesamiento se crea la función procesamiento_carpetas_bolSIPSAT1(lista_carpetas) que tiene como argumento una lista de rutas de carpetas, la finalidad de esta función es usar de manera simultanea las funciones anteriormente creadas y acelerar el procesamiento de los archivos buscando la consolidación automatica de varios años de información ahorrando repetición de lineas de código, la función desarrollada es la siguiente: def procesamiento_carpetas_bolSIPSAT1(lista_carpetas): if type(lista_carpetas) is list: lista_consolidados= [] for carpeta in lista_carpetas: print(&quot;se esta procesando la carpeta de boletines tipo 1: &quot;,carpeta) consolidado= proc_carpetassipsat1(carpeta) lista_consolidados.append(consolidado) consolidado_carpetas_entrada= pd.concat(lista_consolidados) return consolidado_carpetas_entrada else: raise TypeError(&quot;Only list objects are accepted as input&quot;) Esta función toma una lista de carpetas con boletines del SIPSA y por medio de un ciclo for aplica a cada una de las carpetas la función proc_carpetassipsat1(carpeta) a través de la aplicación de esta función de manera iterativa genera una lista de dataframes de boletines semanales consolidados por año y luego une todos los años procesados en un solo dataframe. Las funciones descritas en esta sección permiten hacer un procesamiento en serie de los boletines tipo 1, a continuación veremos las funciones desarrolladas para el tratamiento de los bolestines tipo 2. 2.0.2 Funciones para el pre-procesamiento y limpieza de los boletines para el periodo de Mayo de 2018 a Diciembre de 2022 Como se mencionó con anterioridad los boletines tipo II tienen un grado mayor de complejidad al tratarse de documentos de excel paginados, en medio de esto hay una mayor cantidad de excepciones y casos a considerar que llevan a que las funciones de procesamiento sean mucho más robustas y complejas. Para este caso la dinámica de desarrollo de funciones es la misma, es decir, va de una función inicial para el procesamiento de cada boletín, luego una función para procesamiento a nivel de carpetas y finalmente una función para procesamiento de una lista de carpetas. Veamos la función construida para la limpieza de cada boletín: def bolsipsatipo2_2018_proc(ruta): nombre=os.path.split(ruta)[1].replace(&quot;.xlsx&quot; or &quot;.xls&quot;,&quot;&quot;) if &quot;Sem_12may__18may_2018_int&quot; in nombre: boletin= pd.read_excel(ruta,sheet_name=None,usecols=&quot;B:G&quot;) boletin= {key:val for key,val in boletin.items() if not key.endswith(&quot;ce&quot;)} lista_df=[] for hoja,tabla in boletin.items(): tabla=tabla.dropna(axis=1,how=&quot;all&quot;) lista_df.append(tabla) df_consolidado=pd.concat(lista_df) df_consolidado[&quot;periodo&quot;]=nombre print(df_consolidado.columns) else: boletin= pd.read_excel(ruta,skiprows=9,sheet_name=None,usecols=&quot;A:F&quot;) boletin= {key:val for key,val in boletin.items() if not key.endswith(&quot;ce&quot;)} lista_df=[] for hoja,tabla in boletin.items(): tabla=tabla.dropna(axis=1,how=&quot;all&quot;) lista_columnas=tabla.columns print(lista_columnas,nombre) if (&#39; Pesos por kilogramo&#39; in lista_columnas) or\\ (&#39; Pesos por kilogramo*&#39; in lista_columnas): print(&quot;variacion de nombre de columnas - inicia modificación&quot;) tabla.columns.values[0]=&#39;Producto&#39; tabla.columns.values[1]=&#39;Mercado mayorista&#39; tabla.columns.values[2]=&#39;Precio mínimo&#39; tabla.columns.values[3]=&#39;Precio máximo&#39; tabla.columns.values[4]=&#39;Precio medio&#39; tabla.columns.values[5]=&#39;Tendencia&#39; print(tabla.head(1)) tabla=tabla.drop(index=0) #print(tabla) lista_df.append(tabla) df_consolidado=pd.concat(lista_df) df_consolidado[&quot;periodo&quot;]=nombre #print(df_consolidado.columns) #print(df_consolidado) elif (&#39;GRUPO&#39; in lista_columnas) or (&#39;TOTAL MERCADOS&#39; in lista_columnas): print(&quot;sección de abastecimiento semanal innecesaria - se da paso a sección siguiente&quot;) pass elif lista_columnas[5]==&quot;Precio mínimo&quot;: print(&quot;sección de arroz en molino innecesaria - se da paso a sección siguiente&quot;) pass elif &#39; Pesos por tonelada&#39; in lista_columnas: print(&quot;sección de arroz en molino innecesaria - se da paso a sección siguiente&quot;) pass elif &#39;Unnamed: 1&#39; in lista_columnas and tabla.shape[0]&gt;100: print(&quot;variacion de nombre de columnas por anomalia de fila adicional - inicia modificación&quot;) tabla.columns.values[0]=&#39;Producto&#39; tabla.columns.values[1]=&#39;Mercado mayorista&#39; tabla.columns.values[2]=&#39;Precio mínimo&#39; tabla.columns.values[3]=&#39;Precio máximo&#39; tabla.columns.values[4]=&#39;Precio medio&#39; tabla.columns.values[5]=&#39;Tendencia&#39; print(tabla.head(1)) tabla=tabla.drop(index=0) print(tabla) lista_df.append(tabla) df_consolidado=pd.concat(lista_df) df_consolidado[&quot;periodo&quot;]=nombre print(df_consolidado.columns) print(df_consolidado) else: print(&quot;sin variacion de nombre de columnas&quot;) lista_df.append(tabla) df_consolidado=pd.concat(lista_df) df_consolidado[&quot;periodo&quot;]=nombre print(df_consolidado.columns) return df_consolidado La función anterior realiza tareas similares a la función para boletín tipo 1, no obstante esta considera varios escenarios condicionales, de manera general la función recorre el diccionario resultante de la lectura del archivo de excel en medio de este proceso se excluye la hoja de “Indice” para procesar unicamente hojas que contengan datos, una vez hecho esto se realizan transformaciones orientadas a tener un dataset uniforme que permita una armonización con el tipo de dataset generado en el caso de los boletines tipo 1, algunas de las condiciones que maneja la función son las siguientes: Si el fichero corresponde al boletin Sem_12may__18may_2018_int se ejecuta un procesamiento normal recorriendo las hojas del docuemento excel y generando un dataset consolidado. En caso de no pertenecer al caso anterior se procede a leer el excel y cada una de sus hojas pero se generan varios casos de procesamiento según los elementos que se encuentren en las columnas: Si en las columnas aparece una columna llamada “Pesos por kilogramo” se procede a renombrar las columnas del set de datos. Si en las columnas aparece “GRUPO” o “TOTAL MERCADOS” se omite el procesamiento de la hoja. Si en la columna 6 del set de datos aparece el nombre “Precio minimo” o “Pesos por tonelada” tambien se procede a omitir el procesamiento de la hoja del libro de excel. Si aparece el nombre de columna “Unnamed: 1” signifca que hay una fila adicional que impide que las variables queden debidamente nombradas, en este caso tambien se renombran las columnas. Si no presenta ninguno de los casos anteriores se realiza un procesamiento normal de la hoja y se procede a añadir el dataframe resultante al consolidado del boletin. La aplicación de la función da como resultado un dataframe consolidado en donde se encuentra toda la información de las hojas del boletin procesadas. Ahora veamos la función que procesa en serie varios boletines contenidos en una carpeta: def proc_carpetassipsat2(ruta_carpeta): ficheros=os.listdir(ruta_carpeta) lista_rutas=[] ruta_entera=None for file in ficheros: ruta_entera= ruta_carpeta+&quot;\\\\&quot;+file lista_rutas.append(ruta_entera) lista_dataframes=[] for ruta in lista_rutas: print(ruta) boletin_procesado=bolsipsatipo2_2018_proc(ruta) lista_dataframes.append(boletin_procesado) boletin_consolidado= pd.concat(lista_dataframes) boletin_consolidado= boletin_consolidado.reindex(columns=[&quot;Mercado mayorista&quot;,&quot;Precio mínimo&quot;,&quot;Precio máximo&quot;,&quot;Precio medio&quot;, &quot;Tendencia&quot;,&quot;periodo&quot;,&quot;Producto&quot;]) boletin_consolidado= boletin_consolidado[boletin_consolidado[&quot;Producto&quot;].isin([&quot;Menor a -20,01% ---&quot;, &quot;Entre -10,01% y -20% --&quot;, &quot;Entre - 0,01% y -10% -&quot;, &quot;Variacion igual a 0% =&quot;, &quot;Entre 0,01% y 10% +&quot;, &quot;Entre 10,01% y 20% ++&quot;, &quot;Mayor a 20,01% +++&quot;, &quot;n.d: no disponible&quot;, &quot;Tendencia*&quot;])==False] boletin_consolidado= boletin_consolidado[boletin_consolidado[&quot;Producto&quot;].str.find(&quot;precios&quot;)==-1] return boletin_consolidado Esta función cumple el mismo ciclo de procesamiento que en el caso del boletín 1, no obstante presenta como excepción el hecho de que filtra o elimina algunas filas que contienen información relacionada a las “convenciones de tendencia” que tiene el boletín del SIPSA, finalmente esta función recorre los ficheros de los carpeta, genera una lista de rutas, usa la función para el procesamiento de boletines tipo 2 y concatena los dataframes resultantes en un solo dataframe. Finalmente veamos la función para el procesamiento de un listado de carpetas: def procesamiento_carpetas_bolSIPSAT2(lista_carpetas): if type(lista_carpetas) is list: lista_consolidados= [] for carpeta in lista_carpetas: print(&quot;se esta procesando la carpeta de boletines tipo 2: &quot;,carpeta) consolidado= proc_carpetassipsat2(carpeta) lista_consolidados.append(consolidado) consolidado_carpetas_entrada= pd.concat(lista_consolidados) return consolidado_carpetas_entrada else: raise TypeError(&quot;Only list objects are accepted as input&quot;) Esta función en términos de pasos de procesamiento no tiene grandes diferencias al proceso realizado para boletines tipo 1, la única variación que presenta es el uso de las funciones especificas para este tipo de boletín. El uso de los conjuntos de funciones mencionados en cada carpeta en donde se alojan los boletines descargados del SIPSA dan lugar a dos sets de datos consolidados que luego se concatenan para dar lugar a un conjunto de datos que consolida los boletines desde 2016 hasta 2022. El conjunto de datos consolidado final se denomina “SIPSA_2016to2022.csv” y contiene las siguientes columnas: Mercado mayorista Precio mínimo Precio máximo Precio medio Tendencia periodo Producto 2.0.3 Limpieza de set de datos consolidado de boletines SIPSA 2016 a 2022 Para este proceso de limpieza se hace uso del set de datos “SIPSA_2016to2022.csv” que es el resultado del procesamiento previo realizado con las funciones explicadas en la sección anterior del documento, la limpieza de este dataset comprende la eliminación de registros inusuales producto de la consolidación, y tiene como objetivo principal normalizar las fechas, es decir, poder pasar las mismas de un “nombre de archivo” a una fecha que sea efectivamente manejable y permita un proceso posterior exitoso a nivel de análisis. Como primer paso se cargan algunas librerias y el dataset que se almacenará en la variable “df_sipsa” : from datetime import datetime, timedelta import re df_sipsa= pd.read_csv(&quot;C://Users//DAVID//Documents//SeriesDeTiempo_MCD2023//bookdown-seriesdetiempo_mcd//Bookdown_SeriesDeTiempo//FuenteConsolidadaSIPSA//SIPSA_2016to2022.csv&quot;,sep=&quot;|&quot;,low_memory=False) df_sipsa.drop(columns={&quot;Unnamed: 0&quot;},inplace=True) print(f&quot;El set de datos cuenta con {df_sipsa.shape[0]} registros y {df_sipsa.shape[1]} columnas \\n&quot;) ## El set de datos cuenta con 1658625 registros y 7 columnas df_sipsa.head() ## Mercado mayorista ... Producto ## 0 Armenia, Mercar ... Acelga ## 1 Barranquilla, Barranquillita ... Acelga ## 2 Bogotá, D.C., Corabastos ... Acelga ## 3 Bucaramanga, Centroabastos ... Acelga ## 4 Chiquinquirá (Boyacá) ... Acelga ## ## [5 rows x 7 columns] Como se puede apreciar el set de datos consolidado producto del procesamiento contiene más de un millón de registros y cuenta con las columnas que se habian mencionado al final de la sección anterior, la información aqui consolidada cuenta con información de 140 mercados mayoristas correspondientes a varios municipios del pais, y información relacionada a 672 productos en diversas frecuencias y disponibilidades según la central de abastos. Para iniciar este proceso de limpieza primero se procederá a limpiar registros inusuales en las columnas diferentes a “Periodo, para facilitar este proceso se hará uso del método unique() con el fin de conocer los registros únicos que hay en cada columna y a partir de esto determinar que debe eliminarse u omitirse en caso de ser necesario. df_sipsa[&quot;Mercado mayorista&quot;].unique() ## array([&#39;Armenia, Mercar&#39;, &#39;Barranquilla, Barranquillita&#39;, ## &#39;Bogotá, D.C., Corabastos&#39;, &#39;Bucaramanga, Centroabastos&#39;, ## &#39;Chiquinquirá (Boyacá)&#39;, &#39;Duitama (Boyacá)&#39;, &#39;Florencia (Caquetá)&#39;, ## &#39;Ibagué, Plaza La 21&#39;, &#39;Ipiales (Nariño), Ipiales somos todos&#39;, ## &#39;Medellín, Central Mayorista de Antioquia&#39;, &#39;Neiva, Surabastos&#39;, ## &#39;Pamplona (Norte de Santander)&#39;, &#39;Pasto, El Potrerillo&#39;, ## &#39;Sogamoso (Boyacá)&#39;, &#39;Tunja, Complejo de Servicios del Sur&#39;, ## &#39;Ubaté (Cundinamarca)&#39;, &#39;Villavicencio, CAV&#39;, &#39;Yopal (Casanare)&#39;, ## &#39;Arauca (Arauca)&#39;, &#39;Barranquilla, Granabastos&#39;, ## &#39;Buenaventura (Valle del Cauca)&#39;, &#39;Cali, Cavasa&#39;, ## &#39;Cali, Santa Helena&#39;, &#39;Cartagena, Bazurto&#39;, ## &#39;Cartago (Valle del Cauca)&#39;, &#39;Cúcuta, Cenabastos&#39;, ## &#39;Manizales (Caldas)&#39;, &#39;Montería (Córdoba)&#39;, ## &#39;Palmira (Valle del Cauca)&#39;, &#39;Pereira, La 41&#39;, &#39;Pereira, Mercasa&#39;, ## &#39;Popayán (Cauca)&#39;, &#39;Rionegro (Antioquia)&#39;, &#39;San Gil (Santander)&#39;, ## &#39;Santa Marta (Magdalena)&#39;, &#39;Sincelejo (Sucre)&#39;, ## &#39;Socorro (Santander)&#39;, &#39;Tuluá (Valle del Cauca)&#39;, ## &#39;Valledupar, Mercabastos&#39;, &#39;Valledupar, Mercado Nuevo&#39;, ## &#39;Túquerres (Nariño)&#39;, &#39;San Andrés de Tumaco (Nariño)&#39;, ## &#39;Marinilla (Antioquia)&#39;, ## &#39;Medellín, Plaza Minorista &quot;José María Villa&quot;&#39;, ## &#39;El Carmen de Viboral (Antioquia)&#39;, &#39;El Santuario (Antioquia)&#39;, ## &#39;La Ceja (Antioquia)&#39;, &#39;San Vicente (Antioquia)&#39;, ## &#39;Sonsón (Antioquia)&#39;, &#39;La Unión (Nariño)&#39;, &#39;Armenia, Retiro&#39;, ## &#39;Peñol (Antioquia)&#39;, &#39;Santa Bárbara (Antioquia)&#39;, ## &#39;Cúcuta, La Nueva Sexta&#39;, &#39;Bucaramanga, Mercados del centro&#39;, ## &#39;Ipiales (Nariño), Centro de acopio&#39;, &#39;La Unión (Antioquia)&#39;, ## &#39;Bogotá, D.C., Paloquemao&#39;, &#39;Bogotá, D.C., Plaza España&#39;, ## &#39;Cali, La Floresta&#39;, &#39;Cali, Siloé&#39;, ## &#39;Bogotá, D.C., Plaza Las Flores&#39;, &#39;Cali, Galeria Alameda&#39;, ## &#39;Bogotá, D.C., Frigorífico Ble Ltda.&#39;, ## &#39;Bogotá, D.C., Frigorífico Guadalupe&#39;, &#39;Bucaramanga, Frigoríficos&#39;, ## &#39;Armenia, Frigocafé&#39;, &#39;Malambo (Atlántico), Carnes y carnes&#39;, ## &#39;Ibagué, Frigorífico Carlima&#39;, &#39;Montería, Frigosinú&#39;, ## &#39;Charalá (Santander)&#39;, &#39;Güepsa (Santander)&#39;, &#39;Moniquirá (Boyacá)&#39;, ## &#39;San Gil (Santander), panela&#39;, &#39;Santana (Boyacá)&#39;, ## &#39;Vélez (Santander)&#39;, &#39;Ancuyá (Nariño)&#39;, &#39;Caparrapí (Cundinamarca)&#39;, ## &#39;Consacá (Nariño)&#39;, &#39;Nocaima (Cundinamarca)&#39;, &#39;Sandoná (Nariño)&#39;, ## &#39;Villeta (Cundinamarca)&#39;, &#39;Yolombó (Antioquia)&#39;, &#39;Pasto (Nariño)&#39;, ## &#39;Tunja (Boyacá)&#39;, &#39;San Marcos, Arrocera Formosa&#39;, ## &#39;Ancuya (Nariño)&#39;, &#39;Producto&#39;, &#39;Pasto (Nariño), El Potrerillo&#39;, ## &#39;Cali, Galería Alameda&#39;, &#39;Manizales, Centro Galerías&#39;, ## &#39;Montería, Mercado del Sur&#39;, ## &#39;Popayán, Plaza de mercado del barrio Bolívar&#39;, ## &#39;Sincelejo, Nuevo Mercado&#39;, &#39;San Gil (Santander) &#39;, nan, ## &#39;Bogotá, D.C., Frigorífico Ble Ltda,&#39;, ## &#39;Tibasosa (Boyacá), Coomproriente&#39;, ## &#39;San Sebastián de Mariquita (Tolima)&#39;, &#39;Aguazul (Casanare)&#39;, ## &#39;Alvarado (Tolima)&#39;, &#39;Bucaramanga (Santander)&#39;, ## &#39;Campoalegre (Huila)&#39;, &#39;Cúcuta (Norte de Santander)&#39;, ## &#39;Espinal (Tolima)&#39;, &#39;Granada (Meta)&#39;, &#39;Ibagué (Tolima)&#39;, ## &#39;Lérida (Tolima)&#39;, &#39;Neiva (Huila)&#39;, &#39;Purificación (Tolima)&#39;, ## &#39;Rivera (Huila)&#39;, &#39;Sahagún (Córdoba)&#39;, &#39;San Martín (Meta)&#39;, ## &#39;Venadillo (Tolima)&#39;, &#39;Villanueva (Casanare)&#39;, ## &#39;Villavicencio (Meta)&#39;, &#39;Acacías (Meta)&#39;, ## &#39;Barranquilla (Atlántico)&#39;, &#39;Magangué (Bolívar)&#39;, ## &#39;Planeta Rica (Córdoba)&#39;, &#39;San Bernardo del Viento (Córdoba)&#39;, ## &#39;San Marcos (Sucre)&#39;, &#39;Valledupar (Cesar)&#39;, &#39;Cali, Santa Elena&#39;, ## &#39;Popayán, Plaza de Mercado del Barrio Bolívar&#39;, ## &#39;San Sebastián de Mariquita (Tolima), panela&#39;, ## &#39;Duitama (Boyacá), Mercaplaza&#39;, ## &#39;Ipiales (Nariño), Ipiales Somos Todos&#39;, ## &#39;Bucaramanga, Mercados del Centro&#39;, ## &#39;Ipiales (Nariño), Centro de Acopio&#39;, &#39;Malambo, Atlantico&#39;, ## &#39;San Gil (Santander), Panela&#39;, &#39;San Gil (Santander)panela&#39;, ## &#39;Bogotá, D,C,, Corabastos&#39;, &#39;Bogotá, D,C,, Paloquemao&#39;, ## &#39;Bogotá, D,C,, Plaza España&#39;, &#39;Bogotá, D,C,, Plaza Las Flores&#39;, ## &#39;Bogotá, D,C,, Frigorífico Ble Ltda,&#39;, ## &#39;Bogotá, D,C,, Frigorífico Guadalupe&#39;, ## &#39;Malambo (Atlántico), Carnes y Carnes&#39;], dtype=object) Al revisar el listado de registros se puede apreciar que el contenido de la columna es consistente, todos los registros únicos disponibles corresponden a centrales de abasto, por lo tanto no es necesario en este caso realizar alguna operación de limpieza. Ahora procedamos a revisar las columnas de “precio” con el fin de identificar si tienen algun inconveniente: df_sipsa[&quot;Precio mínimo&quot;].unique() ## array([&#39;1067&#39;, &#39;2400&#39;, &#39;800&#39;, ..., &#39;12627&#39;, &#39;13966&#39;, &#39;51994&#39;], ## dtype=object) df_sipsa[&quot;Precio máximo&quot;].unique() ## array([&#39;1200&#39;, &#39;2500&#39;, &#39;1000&#39;, ..., &#39;6318&#39;, &#39;14137&#39;, &#39;12637&#39;], ## dtype=object) df_sipsa[&quot;Precio medio&quot;].unique() ## array([&#39;1089&#39;, &#39;2458&#39;, &#39;957&#39;, ..., &#39;14032&#39;, &#39;53181&#39;, &#39;43204&#39;], ## dtype=object) Al revisar las variables se puede apreciar que los precios tienen un error de formateo dado que están en formato de “string”, lo cual indica que es necesario posteriormente llevar a cabo una operación de casteo que lleve estos valores precio a valores númericos. Ahora se procede a revisar la columna de tendencia: df_sipsa[&quot;Tendencia&quot;].unique() ## array([&#39;++&#39;, &#39;+&#39;, &#39;-&#39;, &#39;=&#39;, &#39;+++&#39;, &#39;n.d.&#39;, &#39;--&#39;, &#39;---&#39;, &#39;Tend&#39;, nan, ## &#39;n,d,&#39;], dtype=object) Para el caso de esta columna se pueden apreciar valores inusuales como ‘Tend’ y ‘n,d,’ lo cual hace necesario validar las columnas con estos registros con el fin si es necesario eliminar filas o simplemente reformatear registros. Ahora procedamos a revisar la columna “Producto” df_sipsa[&quot;Producto&quot;].unique() ## array([&#39;Acelga&#39;, &#39;Ahuyama&#39;, &#39;Ahuyamín (Sakata)&#39;, &#39;Ají topito dulce&#39;, ## &#39;Ajo&#39;, &#39;Ajo importado&#39;, &#39;Apio&#39;, &#39;Arveja verde en vaina&#39;, ## &#39;Arveja verde en vaina pastusa&#39;, &#39;Berenjena&#39;, &#39;Brócoli&#39;, ## &#39;Calabacín&#39;, &#39;Calabaza&#39;, &#39;Cebolla cabezona blanca&#39;, ## &#39;Cebolla cabezona blanca bogotana&#39;, ## &#39;Cebolla cabezona blanca importada&#39;, ## &#39;Cebolla cabezona blanca pastusa&#39;, &#39;Cebolla cabezona roja&#39;, ## &#39;Cebolla cabezona roja importada&#39;, &#39;Cebolla cabezona roja ocañera&#39;, ## &#39;Cebolla cabezona roja peruana&#39;, &#39;Cebolla junca&#39;, ## &#39;Cebolla junca Aquitania&#39;, &#39;Cebolla junca Berlín&#39;, ## &#39;Cebolla junca pastusa&#39;, &#39;Cebolla junca Tenerife&#39;, ## &#39;Cebolla puerro&#39;, &#39;Cebollín chino&#39;, &#39;Chócolo mazorca&#39;, &#39;Cidra&#39;, ## &#39;Cilantro&#39;, &#39;Coles&#39;, &#39;Coliflor&#39;, &#39;Espinaca&#39;, &#39;Fríjol verde bolo&#39;, ## &#39;Fríjol verde cargamanto&#39;, &#39;Fríjol verde en vaina&#39;, &#39;Haba verde&#39;, ## &#39;Habichuela&#39;, &#39;Habichuela larga&#39;, &#39;Lechuga Batavia&#39;, ## &#39;Lechuga crespa morada&#39;, &#39;Lechuga crespa verde&#39;, &#39;Pepino cohombro&#39;, ## &#39;Pepino de rellenar&#39;, &#39;Perejil&#39;, &#39;Pimentón&#39;, &#39;Pimentón verde&#39;, ## &#39;Rábano rojo&#39;, &#39;Remolacha&#39;, &#39;Remolacha bogotana&#39;, ## &#39;Remolacha regional&#39;, &#39;Repollo blanco&#39;, &#39;Repollo blanco bogotano&#39;, ## &#39;Repollo morado&#39;, &#39;Repollo verde&#39;, &#39;Tomate chonto&#39;, ## &#39;Tomate chonto regional&#39;, &#39;Tomate larga vida&#39;, &#39;Tomate riñón&#39;, ## &#39;Tomate riñón valluno&#39;, &#39;Tomate Riogrande&#39;, ## &#39;Tomate Riogrande bumangués&#39;, &#39;Tomate Riogrande ocañero&#39;, ## &#39;Zanahoria&#39;, &#39;Zanahoria bogotana&#39;, &#39;Zanahoria larga vida&#39;, ## &#39;Aguacate común&#39;, &#39;Aguacate Hass&#39;, &#39;Aguacate papelillo&#39;, &#39;Badea&#39;, ## &#39;Banano bocadillo&#39;, &#39;Banano criollo&#39;, &#39;Banano Urabá&#39;, &#39;Borojó&#39;, ## &#39;Breva&#39;, &#39;Ciruela importada&#39;, &#39;Ciruela roja&#39;, &#39;Coco&#39;, &#39;Curuba&#39;, ## &#39;Durazno importado&#39;, &#39;Durazno nacional&#39;, &#39;Feijoa&#39;, &#39;Fresa&#39;, ## &#39;Granadilla&#39;, &#39;Guanábana&#39;, &#39;Guayaba agria&#39;, &#39;Guayaba Atlántico&#39;, ## &#39;Guayaba común&#39;, &#39;Guayaba manzana&#39;, &#39;Guayaba pera&#39;, ## &#39;Guayaba pera valluna&#39;, &#39;Gulupa&#39;, &#39;Higo&#39;, &#39;Kiwi&#39;, &#39;Limón común&#39;, ## &#39;Limón común Ciénaga&#39;, &#39;Limón común valluno&#39;, &#39;Limón mandarino&#39;, ## &#39;Limón Tahití&#39;, &#39;Lulo&#39;, &#39;Mandarina Arrayana&#39;, &#39;Mandarina común&#39;, ## &#39;Mandarina Oneco&#39;, &#39;Mango común&#39;, &#39;Mango de azúcar&#39;, ## &#39;Mango manzano&#39;, &#39;Mango reina&#39;, &#39;Mango Tommy&#39;, &#39;Mango Yulima&#39;, ## &#39;Manzana nacional&#39;, &#39;Manzana roja importada&#39;, ## &#39;Manzana royal gala importada&#39;, &#39;Manzana verde importada&#39;, ## &#39;Maracuyá&#39;, &#39;Maracuyá antioqueño&#39;, &#39;Maracuyá huilense&#39;, ## &#39;Maracuyá santandereano&#39;, &#39;Maracuyá valluno&#39;, &#39;Melón Cantalup&#39;, ## &#39;Mora de Castilla&#39;, &#39;Naranja común&#39;, &#39;Naranja Sweet&#39;, ## &#39;Naranja Valencia&#39;, &#39;Papaya hawaiana&#39;, &#39;Papaya Maradol&#39;, ## &#39;Papaya melona&#39;, &#39;Papaya redonda&#39;, &#39;Papaya tainung&#39;, &#39;Patilla&#39;, ## &#39;Patilla baby&#39;, &#39;Pera importada&#39;, &#39;Pera nacional&#39;, &#39;Piña gold&#39;, ## &#39;Piña manzana&#39;, &#39;Piña perolera&#39;, &#39;Pitahaya&#39;, &#39;Tangelo&#39;, ## &#39;Tomate de árbol&#39;, &#39;Uchuva con cáscara&#39;, &#39;Uva importada&#39;, ## &#39;Uva Isabela&#39;, &#39;Uva negra&#39;, &#39;Uva red globe nacional&#39;, &#39;Uva roja&#39;, ## &#39;Uva verde&#39;, &#39;Zapote&#39;, &#39;Arracacha amarilla&#39;, &#39;Arracacha blanca&#39;, ## &#39;Ñame criollo&#39;, &#39;Ñame diamante&#39;, &#39;Ñame espino&#39;, &#39;Papa Betina&#39;, ## &#39;Papa capira&#39;, &#39;Papa criolla limpia&#39;, &#39;Papa criolla sucia&#39;, ## &#39;Papa ICA-Huila&#39;, &#39;Papa Morasurco&#39;, &#39;Papa nevada&#39;, ## &#39;Papa parda pastusa&#39;, &#39;Papa Puracé&#39;, &#39;Papa roja peruana&#39;, ## &#39;Papa rubí&#39;, &#39;Papa R-12 negra&#39;, &#39;Papa R-12 roja&#39;, &#39;Papa sabanera&#39;, ## &#39;Papa San Félix&#39;, &#39;Papa superior&#39;, &#39;Papa suprema&#39;, ## &#39;Papa tocarreña&#39;, &#39;Papa única&#39;, &#39;Plátano comino&#39;, ## &#39;Plátano dominico hartón maduro&#39;, &#39;Plátano dominico hartón verde&#39;, ## &#39;Plátano dominico verde&#39;, &#39;Plátano guineo&#39;, ## &#39;Plátano hartón maduro&#39;, &#39;Plátano hartón verde&#39;, ## &#39;Plátano hartón verde ecuatoriano&#39;, ## &#39;Plátano hartón verde Eje Cafetero&#39;, ## &#39;Plátano hartón verde llanero&#39;, &#39;Ulluco&#39;, &#39;Yuca chirosa&#39;, ## &#39;Yuca criolla&#39;, &#39;Yuca ICA&#39;, &#39;Yuca llanera&#39;, ## &#39;Arroz blanco importado&#39;, &#39;Arroz de primera&#39;, &#39;Arroz de segunda&#39;, ## &#39;Arroz excelso&#39;, &#39;Arroz sopa cristal&#39;, ## &#39;Arveja amarilla seca importada&#39;, &#39;Arveja verde seca importada&#39;, ## &#39;Cuchuco de cebada&#39;, &#39;Cuchuco de maíz&#39;, &#39;Fríjol bolón&#39;, ## &#39;Fríjol cabeza negra importado&#39;, &#39;Fríjol cabeza negra nacional&#39;, ## &#39;Fríjol calima&#39;, &#39;Fríjol cargamanto blanco&#39;, ## &#39;Fríjol cargamanto rojo&#39;, &#39;Fríjol nima calima&#39;, ## &#39;Fríjol palomito importado&#39;, &#39;Fríjol radical&#39;, ## &#39;Fríjol Uribe rosado&#39;, &#39;Fríjol Zaragoza&#39;, &#39;Garbanzo importado&#39;, ## &#39;Lenteja importada&#39;, &#39;Maíz amarillo cáscara&#39;, ## &#39;Maíz amarillo cáscara importado&#39;, &#39;Maíz amarillo trillado&#39;, ## &#39;Maíz blanco cáscara&#39;, &#39;Maíz blanco retrillado&#39;, ## &#39;Maíz blanco trillado&#39;, &#39;Maíz pira&#39;, &#39;Huevo blanco A&#39;, ## &#39;Huevo blanco AA&#39;, &#39;Huevo blanco B&#39;, &#39;Huevo blanco extra&#39;, ## &#39;Huevo rojo A&#39;, &#39;Huevo rojo AA&#39;, &#39;Huevo rojo B&#39;, ## &#39;Huevo rojo extra&#39;, &#39;Leche en polvo&#39;, &#39;Queso campesino&#39;, ## &#39;Queso Caquetá&#39;, &#39;Queso costeño&#39;, &#39;Queso cuajada&#39;, ## &#39;Queso doble crema&#39;, &#39;Alas de pollo con costillar&#39;, ## &#39;Alas de pollo sin costillar&#39;, &#39;Carne de cerdo, brazo con hueso&#39;, ## &#39;Carne de cerdo, brazo sin hueso&#39;, ## &#39;Carne de cerdo, cabeza de lomo&#39;, &#39;Carne de cerdo, costilla&#39;, ## &#39;Carne de cerdo en canal&#39;, &#39;Carne de cerdo, espinazo&#39;, ## &#39;Carne de cerdo, lomo con hueso&#39;, &#39;Carne de cerdo, lomo sin hueso&#39;, ## &#39;Carne de cerdo, pernil con hueso&#39;, ## &#39;Carne de cerdo, pernil sin hueso&#39;, ## &#39;Carne de cerdo, tocineta plancha&#39;, ## &#39;Carne de cerdo, tocino barriga&#39;, &#39;Carne de cerdo, tocino papada&#39;, ## &#39;Carne de res, bola de brazo&#39;, &#39;Carne de res, bola de pierna&#39;, ## &#39;Carne de res, bota&#39;, &#39;Carne de res, cadera&#39;, ## &#39;Carne de res, centro de pierna&#39;, &#39;Carne de res, chatas&#39;, ## &#39;Carne de res, cogote&#39;, &#39;Carne de res, costilla&#39;, ## &#39;Carne de res en canal&#39;, &#39;Carne de res, falda&#39;, ## &#39;Carne de res, lomo de brazo&#39;, &#39;Carne de res, lomo fino&#39;, ## &#39;Carne de res molida, murillo&#39;, &#39;Carne de res, morrillo&#39;, ## &#39;Carne de res, muchacho&#39;, &#39;Carne de res, murillo&#39;, ## &#39;Carne de res, paletero&#39;, &#39;Carne de res, pecho&#39;, ## &#39;Carne de res, punta de anca&#39;, &#39;Carne de res, sobrebarriga&#39;, ## &#39;Menudencias de pollo&#39;, &#39;Muslos de pollo con rabadilla&#39;, ## &#39;Muslos de pollo sin rabadilla&#39;, &#39;Pechuga de pollo&#39;, ## &#39;Pierna pernil con rabadilla&#39;, &#39;Pierna pernil sin rabadilla&#39;, ## &#39;Piernas de pollo&#39;, &#39;Pollo entero congelado sin vísceras&#39;, ## &#39;Pollo entero fresco con vísceras&#39;, ## &#39;Pollo entero fresco sin vísceras&#39;, &#39;Rabadillas de pollo&#39;, ## &#39;Almejas con concha&#39;, &#39;Almejas sin concha&#39;, ## &#39;Bagre rayado en postas congelado&#39;, ## &#39;Bagre rayado entero congelado&#39;, &#39;Bagre rayado entero fresco&#39;, ## &#39;Blanquillo entero fresco&#39;, &#39;Bocachico criollo fresco&#39;, ## &#39;Bocachico importado congelado&#39;, &#39;Cachama de cultivo fresca&#39;, ## &#39;Calamar anillos&#39;, &#39;Calamar blanco entero&#39;, ## &#39;Calamar morado entero&#39;, &#39;Camarón tigre precocido seco&#39;, ## &#39;Camarón tití precocido seco&#39;, &#39;Capaz Magdalena fresco&#39;, ## &#39;Cazuela de mariscos (paquete)&#39;, ## &#39;Corvina, filete congelado nacional&#39;, &#39;Langostino U12&#39;, ## &#39;Langostino 16-20&#39;, &#39;Merluza, filete importado&#39;, ## &#39;Merluza, filete nacional&#39;, &#39;Mojarra lora entera congelada&#39;, ## &#39;Mojarra lora entera fresca&#39;, &#39;Nicuro fresco&#39;, &#39;Palmitos de mar&#39;, ## &#39;Pargo rojo entero congelado&#39;, &#39;Pargo rojo entero fresco&#39;, ## &#39;Pargo rojo platero&#39;, &#39;Pescado cabezas&#39;, ## &#39;Róbalo, filete congelado&#39;, &#39;Salmón, filete congelado&#39;, ## &#39;Sierra entera congelada&#39;, &#39;Tilapia, filete congelado&#39;, ## &#39;Tilapia, lomitos&#39;, &#39;Tilapia roja entera congelada&#39;, ## &#39;Tilapia roja entera fresca&#39;, &#39;Toyo blanco, filete congelado&#39;, ## &#39;Trucha en corte mariposa&#39;, &#39;Trucha entera fresca&#39;, ## &#39;Aceite girasol&#39;, &#39;Aceite soya&#39;, &#39;Aceite vegetal mezcla&#39;, ## &#39;Arveja enlatada&#39;, &#39;Avena en hojuelas&#39;, &#39;Avena molida&#39;, ## &#39;Azúcar morena&#39;, &#39;Azúcar refinada&#39;, &#39;Azúcar sulfitada&#39;, ## &#39;Bocadillo veleño&#39;, &#39;Café instantáneo&#39;, &#39;Café molido&#39;, ## &#39;Chocolate amargo&#39;, &#39;Chocolate dulce&#39;, &#39;Chocolate instantáneo&#39;, ## &#39;Color (bolsita)&#39;, &#39;Fécula de maíz&#39;, &#39;Fríjol enlatado&#39;, ## &#39;Galletas dulces redondas con crema&#39;, &#39;Galletas saladas&#39;, ## &#39;Gelatina&#39;, &#39;Harina de trigo&#39;, &#39;Harina precocida de maíz&#39;, ## &#39;Jugo de frutas&#39;, &#39;Jugo instantáneo (sobre)&#39;, ## &#39;Lomitos de atún en lata&#39;, &#39;Maíz enlatado&#39;, &#39;Manteca&#39;, &#39;Margarina&#39;, ## &#39;Mayonesa doy pack&#39;, &#39;Mostaza doy pack&#39;, &#39;Panela cuadrada blanca&#39;, ## &#39;Panela cuadrada morena&#39;, &#39;Panela en pastilla&#39;, ## &#39;Panela redonda blanca&#39;, &#39;Panela redonda morena&#39;, ## &#39;Pastas alimenticias&#39;, &#39;Sal yodada&#39;, &#39;Salsa de tomate doy pack&#39;, ## &#39;Sardinas en lata&#39;, &#39;Sopa de pollo (caja)&#39;, &#39;Vinagre&#39;, ## &#39;Mango costeño&#39;, &#39;Mango Kent&#39;, nan, ## &#39;Basa, filete congelado importado&#39;, ## &#39;Basa, entero congelado importado&#39;, ## &#39;Menor a -12% ---&#39;, ## &#39;Entre -12% y -7% --&#39;, ## &#39;Entre -6,99% y -3% -&#39;, ## &#39;Entre -2,99% y 3% =&#39;, ## &#39;Entre 3,01% y 7% +&#39;, ## &#39;Entre 7,01% y 12% ++&#39;, ## &#39;Mayor a 12% +++&#39;, &#39;c&#39;, ## &#39;Arroz blanco empacado&#39;, &#39;Arroz blanco en bulto&#39;, &#39;Arroz cristal&#39;, ## &#39;Arroz granza&#39;, &#39;Arroz paddy verde&#39;, &#39;Harina de arroz&#39;, ## &#39;Menor a -12,01% ---&#39;, ## &#39;Entre -7,01% y -12% --&#39;, &#39;Entre -3,01% y -7% -&#39;, ## &#39;Entre 3% y -3% =&#39;, ## &#39;Entre 3,01% y 7% +&#39;, &#39;Entre 7,01% y 12% ++&#39;, ## &#39;Mayor a 12,01% +++&#39;, &#39;Papaya Paulina&#39;, ## &#39;Mandarina arrayana&#39;, &#39;Aceite de palma&#39;, &#39;Ají Topito Dulce&#39;, ## &#39;Ajo Importado&#39;, &#39;Arveja Verde en Vaina&#39;, ## &#39;Arveja Verde en Vaina Pastusa&#39;, &#39;Cebolla Cabezona Blanca&#39;, ## &#39;Cebolla Cabezona Blanca Bogotana&#39;, ## &#39;Cebolla Cabezona Blanca Importada&#39;, ## &#39;Cebolla Cabezona Blanca Pastusa&#39;, &#39;Cebolla Cabezona Roja&#39;, ## &#39;Cebolla Cabezona Roja Importada&#39;, &#39;Cebolla Cabezona Roja Ocañera&#39;, ## &#39;Cebolla Cabezona Roja Peruana&#39;, &#39;Cebolla Junca&#39;, ## &#39;Cebolla Junca Aquitania&#39;, &#39;Cebolla Junca Berlín&#39;, ## &#39;Cebolla Junca Pastusa&#39;, &#39;Cebolla Junca Tenerife&#39;, ## &#39;Cebolla Puerro&#39;, &#39;Cebollín Chino&#39;, &#39;Chócolo Mazorca&#39;, ## &#39;Fríjol Verde Bolo&#39;, &#39;Fríjol Verde Cargamanto&#39;, ## &#39;Fríjol Verde en Vaina&#39;, &#39;Haba Verde&#39;, &#39;Habichuela Larga&#39;, ## &#39;Lechuga Crespa Verde&#39;, &#39;Pepino Cohombro&#39;, &#39;Pepino de Rellenar&#39;, ## &#39;Pimentón Verde&#39;, &#39;Rábano Rojo&#39;, &#39;Remolacha Bogotana&#39;, ## &#39;Remolacha Regional&#39;, &#39;Repollo Blanco&#39;, &#39;Repollo Blanco Bogotano&#39;, ## &#39;Repollo Blanco Valluno&#39;, &#39;Repollo Morado&#39;, &#39;Repollo Verde&#39;, ## &#39;Tomate Chonto&#39;, &#39;Tomate Chonto Regional&#39;, &#39;Tomate Chonto Valluno&#39;, ## &#39;Tomate Larga Vida&#39;, &#39;Tomate Riñón&#39;, &#39;Tomate Riñón Valluno&#39;, ## &#39;Tomate Riogrande Bumangués&#39;, &#39;Tomate Riogrande Ocañero&#39;, ## &#39;Zanahoria Bogotana&#39;, &#39;Zanahoria Larga Vida&#39;, &#39;Aguacate Común&#39;, ## &#39;Aguacate Papelillo&#39;, &#39;Banano Bocadillo&#39;, &#39;Banano Criollo&#39;, ## &#39;Ciruela Importada&#39;, &#39;Ciruela Roja&#39;, &#39;Durazno Nacional&#39;, ## &#39;Guayaba Agria&#39;, &#39;Guayaba Común&#39;, &#39;Guayaba Manzana&#39;, ## &#39;Guayaba Pera&#39;, &#39;Guayaba Pera Valluna&#39;, &#39;Limón Común&#39;, ## &#39;Limón Común Ciénaga&#39;, &#39;Limón Común Valluno&#39;, &#39;Limón Mandarino&#39;, ## &#39;Mandarina Común&#39;, &#39;Mango Común&#39;, &#39;Mango de Azúcar&#39;, ## &#39;Mango Manzano&#39;, &#39;Mango Reina&#39;, &#39;Manzana Nacional&#39;, ## &#39;Manzana Roja Importada&#39;, &#39;Manzana Royal Gala Importada&#39;, ## &#39;Manzana Verde Importada&#39;, &#39;Maracuyá Antioqueño&#39;, ## &#39;Maracuyá Huilense&#39;, &#39;Maracuyá Santandereano&#39;, &#39;Maracuyá Valluno&#39;, ## &#39;Naranja Común&#39;, &#39;Papaya Hawaiana&#39;, &#39;Papaya Tainung&#39;, ## &#39;Patilla Baby&#39;, &#39;Pera Importada&#39;, &#39;Pera Nacional&#39;, &#39;Piña Gold&#39;, ## &#39;Piña Manzana&#39;, &#39;Piña Perolera&#39;, &#39;Tomate de Árbol&#39;, ## &#39;Uchuva con Cáscara&#39;, &#39;Uva Importada&#39;, &#39;Uva Red Globe Nacional&#39;, ## &#39;Uva Roja&#39;, &#39;Uva Verde&#39;, &#39;Arracacha Amarilla&#39;, &#39;Arracacha Blanca&#39;, ## &#39;Ñame Criollo&#39;, &#39;Ñame Diamante&#39;, &#39;Ñame Espino&#39;, &#39;Papa Capira&#39;, ## &#39;Papa Criolla Limpia&#39;, &#39;Papa Criolla Sucia&#39;, &#39;Papa Ica-Huila&#39;, ## &#39;Papa Nevada&#39;, &#39;Papa Parda Pastusa&#39;, &#39;Papa R-12 Negra&#39;, ## &#39;Papa R-12 Roja&#39;, &#39;Papa Roja Peruana&#39;, &#39;Papa Rubí&#39;, ## &#39;Papa Sabanera&#39;, &#39;Papa Superior&#39;, &#39;Papa Suprema&#39;, &#39;Papa Única&#39;, ## &#39;Plátano Comino&#39;, &#39;Plátano Dominico Hartón Maduro&#39;, ## &#39;Plátano Dominico Hartón Verde&#39;, &#39;Plátano Dominico Verde&#39;, ## &#39;Plátano Guineo&#39;, &#39;Plátano Hartón Maduro&#39;, &#39;Plátano Hartón Verde&#39;, ## &#39;Plátano Hartón Verde Ecuatoriano&#39;, ## &#39;Plátano Hartón Verde Eje Cafetero&#39;, ## &#39;Plátano Hartón Verde Llanero&#39;, &#39;Yuca Chirosa&#39;, &#39;Yuca Criolla&#39;, ## &#39;Yuca Ica&#39;, &#39;Yuca Llanera&#39;, &#39;Arroz Blanco Importado&#39;, ## &#39;Arroz de Primera&#39;, &#39;Arroz de Segunda&#39;, &#39;Arroz Excelso&#39;, ## &#39;Arroz Sopa Cristal&#39;, &#39;Arveja Amarilla Seca Importada&#39;, ## &#39;Arveja Enlatada&#39;, &#39;Arveja Verde Seca Importada&#39;, ## &#39;Cuchuco de Cebada&#39;, &#39;Cuchuco de Maíz&#39;, &#39;Fríjol Bolón&#39;, ## &#39;Fríjol Cabeza Negra Importado&#39;, &#39;Fríjol Cabeza Negra Nacional&#39;, ## &#39;Fríjol Calima&#39;, &#39;Fríjol Cargamanto Blanco&#39;, ## &#39;Fríjol Cargamanto Rojo&#39;, &#39;Fríjol Enlatado&#39;, &#39;Fríjol Nima Calima&#39;, ## &#39;Fríjol Palomito Importado&#39;, &#39;Fríjol Radical&#39;, ## &#39;Fríjol Uribe Rosado&#39;, &#39;Garbanzo Importado&#39;, &#39;Lenteja Importada&#39;, ## &#39;Maíz Amarillo Cáscara&#39;, &#39;Maíz Amarillo Cáscara Importado&#39;, ## &#39;Maíz Amarillo Trillado&#39;, &#39;Maíz Blanco Cáscara&#39;, ## &#39;Maíz Blanco Retrillado&#39;, &#39;Maíz Blanco Trillado&#39;, &#39;Maíz Pira&#39;, ## &#39;Huevo Blanco A&#39;, &#39;Huevo Blanco AA&#39;, &#39;Huevo Blanco B&#39;, ## &#39;Huevo Blanco Extra&#39;, &#39;Huevo Rojo A&#39;, &#39;Huevo Rojo AA&#39;, ## &#39;Huevo Rojo B&#39;, &#39;Huevo Rojo Extra&#39;, &#39;Leche en Polvo&#39;, ## &#39;Queso Campesino&#39;, &#39;Queso Costeño&#39;, &#39;Queso Cuajada&#39;, ## &#39;Queso Doble Crema&#39;, &#39;Alas de Pollo con Costillar&#39;, ## &#39;Alas de Pollo sin Costillar&#39;, &#39;Carne de Cerdo en Canal&#39;, ## &#39;Carne de Cerdo, Brazo con Hueso&#39;, ## &#39;Carne de Cerdo, Brazo sin Hueso&#39;, ## &#39;Carne de Cerdo, Cabeza de Lomo&#39;, &#39;Carne de Cerdo, Costilla&#39;, ## &#39;Carne de Cerdo, Espinazo&#39;, &#39;Carne de Cerdo, Lomo con Hueso&#39;, ## &#39;Carne de Cerdo, Lomo sin Hueso&#39;, ## &#39;Carne de Cerdo, Pernil con Hueso&#39;, ## &#39;Carne de Cerdo, Pernil sin Hueso&#39;, ## &#39;Carne de Cerdo, Tocineta Plancha&#39;, ## &#39;Carne de Cerdo, Tocino Barriga&#39;, &#39;Carne de Cerdo, Tocino Papada&#39;, ## &#39;Carne de Res en Canal&#39;, &#39;Carne de Res Molida, Murillo&#39;, ## &#39;Carne de Res, Bola de Brazo&#39;, &#39;Carne de Res, Bola de Pierna&#39;, ## &#39;Carne de Res, Bota&#39;, &#39;Carne de Res, Cadera&#39;, ## &#39;Carne de Res, Centro de Pierna&#39;, &#39;Carne de Res, Chatas&#39;, ## &#39;Carne de Res, Cogote&#39;, &#39;Carne de Res, Costilla&#39;, ## &#39;Carne de Res, Falda&#39;, &#39;Carne de Res, Lomo de Brazo&#39;, ## &#39;Carne de Res, Lomo Fino&#39;, &#39;Carne de Res, Morrillo&#39;, ## &#39;Carne de Res, Muchacho&#39;, &#39;Carne de Res, Murillo&#39;, ## &#39;Carne de Res, Paletero&#39;, &#39;Carne de Res, Pecho&#39;, ## &#39;Carne de Res, Punta de Anca&#39;, &#39;Carne de Res, Sobrebarriga&#39;, ## &#39;Menudencias de Pollo&#39;, &#39;Muslos de Pollo con Rabadilla&#39;, ## &#39;Muslos de Pollo sin Rabadilla&#39;, &#39;Pechuga de Pollo&#39;, ## &#39;Pierna Pernil con Rabadilla&#39;, &#39;Pierna Pernil sin Rabadilla&#39;, ## &#39;Piernas de Pollo&#39;, &#39;Pollo Entero Congelado sin Vísceras&#39;, ## &#39;Pollo Entero Fresco sin Vísceras&#39;, &#39;Rabadillas de Pollo&#39;, ## &#39;Almejas con Concha&#39;, &#39;Almejas sin Concha&#39;, ## &#39;Bagre Rayado en Postas Congelado&#39;, ## &#39;Bagre Rayado Entero Congelado&#39;, &#39;Bagre Rayado Entero Fresco&#39;, ## &#39;Basa, Entero Congelado Importado&#39;, ## &#39;Basa, Filete Congelado Importado&#39;, &#39;Blanquillo Entero Fresco&#39;, ## &#39;Bocachico Criollo Fresco&#39;, &#39;Bocachico Importado Congelado&#39;, ## &#39;Cachama de Cultivo Fresca&#39;, &#39;Calamar Anillos&#39;, ## &#39;Calamar Blanco Entero&#39;, &#39;Calamar Morado Entero&#39;, ## &#39;Camarón Tigre Precocido Seco&#39;, &#39;Camarón Tití Precocido Seco&#39;, ## &#39;Capaz Magdalena Fresco&#39;, &#39;Cazuela de Mariscos (Paquete)&#39;, ## &#39;Corvina, Filete Congelado Nacional&#39;, &#39;Merluza, Filete Importado&#39;, ## &#39;Merluza, Filete Nacional&#39;, &#39;Mojarra Lora Entera Congelada&#39;, ## &#39;Mojarra Lora Entera Fresca&#39;, &#39;Nicuro Fresco&#39;, &#39;Palmitos de Mar&#39;, ## &#39;Pargo Rojo Entero Congelado&#39;, &#39;Pargo Rojo Platero&#39;, ## &#39;Pescado Cabezas&#39;, &#39;Róbalo, Filete Congelado&#39;, ## &#39;Salmón, Filete Congelado&#39;, &#39;Sierra Entera Congelada&#39;, ## &#39;Tilapia Roja Entera Congelada&#39;, &#39;Tilapia Roja Entera Fresca&#39;, ## &#39;Tilapia, Filete Congelado&#39;, &#39;Trucha en Corte Mariposa&#39;, ## &#39;Trucha Entera Fresca&#39;, &#39;Aceite Girasol&#39;, &#39;Aceite Soya&#39;, ## &#39;Aceite de Palma&#39;, &#39;Aceite Vegetal Mezcla&#39;, &#39;Avena en Hojuelas&#39;, ## &#39;Avena Molida&#39;, &#39;Azúcar Morena&#39;, &#39;Azúcar Refinada&#39;, ## &#39;Azúcar Sulfitada&#39;, &#39;Bocadillo Veleño&#39;, &#39;Café Instantáneo&#39;, ## &#39;Café Molido&#39;, &#39;Chocolate Amargo&#39;, &#39;Chocolate Dulce&#39;, ## &#39;Chocolate Instantáneo&#39;, &#39;Color (Bolsita)&#39;, &#39;Fécula de Maíz&#39;, ## &#39;Galletas Dulces Redondas con Crema&#39;, &#39;Galletas Saladas&#39;, ## &#39;Harina de Trigo&#39;, &#39;Harina Precocida de Maíz&#39;, &#39;Jugo de Frutas&#39;, ## &#39;Jugo Instantáneo (Sobre)&#39;, &#39;Lomitos de Atún en Lata&#39;, ## &#39;Maíz Enlatado&#39;, &#39;Mayonesa Doy Pack&#39;, &#39;Mostaza Doy Pack&#39;, ## &#39;Panela Cuadrada Blanca&#39;, &#39;Panela Cuadrada Morena&#39;, ## &#39;Panela en Pastilla&#39;, &#39;Panela Redonda Blanca&#39;, ## &#39;Panela Redonda Morena&#39;, &#39;Pastas Alimenticias&#39;, &#39;Sal Yodada&#39;, ## &#39;Salsa de Tomate Doy Pack&#39;, &#39;Sardinas en Lata&#39;, ## &#39;Sopa de Pollo (Caja)&#39;, &#39;Repollo blanco valluno&#39;, ## &#39;Tomate chonto valluno&#39;], dtype=object) Se identifican registros intrusos que no lograron ser depurados totalmente en la consolidación, estos corresponden a secciones de explicación de convenciones de tendencia: Menor a -12% —‘, ’Entre -12% y -7% –’, ‘Entre -6,99% y -3% -’, ‘Entre -2,99% y 3% =’, ‘Entre 3,01% y 7% +’, ‘Entre 7,01% y 12% ++’, ‘Mayor a 12% +++’, ‘c’, ‘Menor a -12,01% —’, ‘Entre -7,01% y -12% –’, ‘Entre -3,01% y -7% -’, ‘Entre 3% y -3% =’, ‘Entre 3,01% y 7% +’, ‘Entre 7,01% y 12% ++’, ‘Mayor a 12,01% +++’, Se hace necesario limpiar estos registros ya que generan espacios con “nan” en las demás columnas, además no aportan a los datos. A partir de los hallazgos realizados en la exploración de los registros en cada columna se crean las siguientes lineas de código con el fin de corregir los errores identificados: # se procede a eliminar los registros de la columna tendencia con valor &quot;Tend&quot; df_sipsa= df_sipsa[df_sipsa[&quot;Tendencia&quot;]!=&quot;Tend&quot;] # se procede a reemplazar los registros de la columna tendencia con valor &quot;n,d&quot; df_sipsa.loc[&quot;Tendencia&quot;]= df_sipsa[&quot;Tendencia&quot;].replace(&#39;n,d,&#39;,&quot;n.d&quot;) # Se eliminan los registros intrusos que quedaron producto de convenciones que quedaron # residualmente en el set de datos consolidado lista_anomalias=[&#39;Menor a -12% ---&#39;, &#39;Entre -12% y -7% --&#39;, &#39;Entre -6,99% y -3% -&#39;, &#39;Entre -2,99% y 3% =&#39;, &#39;Entre 3,01% y 7% +&#39;, &#39;Entre 7,01% y 12% ++&#39;, &#39;Mayor a 12% +++&#39;, &#39;Menor a -12,01% ---&#39;, &#39;Entre -7,01% y -12% --&#39;, &#39;Entre -3,01% y -7% -&#39;, &#39;Entre 3% y -3% =&#39;, &#39;Entre 3,01% y 7% +&#39;, &#39;Entre 7,01% y 12% ++&#39;, &#39;Mayor a 12,01% +++&#39;, &#39;c&#39;] df_sipsa= df_sipsa[df_sipsa[&quot;Producto&quot;].isin(lista_anomalias)==False] df_sipsa.shape ## (1657994, 7) Despues de aplicar las operaciones de limpieza anteriores el set de datos queda con un total de 1’657.994 registros. Dado que se ha finalizado la revisión de las columnas diferentes a periodo, ahora se procede a revisar el estado de los registros de la columna “Periodo” los cuales corresponden a los nombres de los ficheros que fueron procesados, de estos nombre se deberá extraer una fecha de referencia que sirva como base para la serie de tiempo: df_sipsa[&quot;periodo&quot;].unique() ## array([&#39;Sem_10dic_16dic_2016.xls&#39;, &#39;Sem_10sep_16sep_2016.xls&#39;, ## &#39;Sem_11jun_17jun_2016.xls&#39;, &#39;Sem_12mar_18mar_2016.xls&#39;, ## &#39;Sem_12nov_18nov_2016.xls&#39;, &#39;Sem_13ago_19ago_2016.xls&#39;, ## &#39;Sem_13feb_19feb_2016.xls&#39;, &#39;Sem_14may_20may_2016.xls&#39;, ## &#39;Sem_15oct_21oct_2016.xls&#39;, &#39;Sem_16abr_22abr_2016.xls&#39;, ## &#39;Sem_16ene_22ene_2016.xls&#39;, &#39;Sem_16jul_22jul_2016.xls&#39;, ## &#39;Sem_17dic_23dic_2016.xls&#39;, &#39;Sem_17sep_23sept_2016.xls&#39;, ## &#39;Sem_18jun_24jun_2016.xls&#39;, &#39;Sem_19mar_23mar_2016.xls&#39;, ## &#39;Sem_19nov_25nov_2016.xls&#39;, &#39;Sem_1oct_7oct_2016.xls&#39;, ## &#39;Sem_20ago_26ago_2016.xls&#39;, &#39;Sem_20feb_26feb_2016.xls&#39;, ## &#39;Sem_21may_27may_2016.xls&#39;, &#39;Sem_22oct_28oct_2016.xls&#39;, ## &#39;Sem_23abr_29abr_2016.xls&#39;, &#39;Sem_23ene_29ene_2016.xls&#39;, ## &#39;Sem_23jul_29jul_2016.xls&#39;, &#39;Sem_24dic_30dic_2016.xls&#39;, ## &#39;Sem_24jun_1jul_2016.xls&#39;, &#39;Sem_24sep_30sep_2016.xls&#39;, ## &#39;Sem_26mar_1abr_2016.xls&#39;, &#39;Sem_26nov_2dic_2016.xls&#39;, ## &#39;Sem_27ago_2sep_2016.xls&#39;, &#39;Sem_27feb_4mar_2016.xls&#39;, ## &#39;Sem_28may_3jun_2016.xls&#39;, &#39;Sem_29oct_04nov_2016.xls&#39;, ## &#39;Sem_2abr_8abr_2016.xls&#39;, &#39;Sem_2ene_8ene_2016.xls&#39;, ## &#39;Sem_2jul_8jul_2016.xls&#39;, &#39;Sem_30abr_06may_2016.xls&#39;, ## &#39;Sem_30ene_5feb_2016.xls&#39;, &#39;Sem_30jul_5ago_2016.xls&#39;, ## &#39;Sem_3dic_9dic_2016.xls&#39;, &#39;Sem_3sep_9sep_2016.xls&#39;, ## &#39;Sem_4jun_10jun_2016.xls&#39;, &#39;Sem_5mar_11mar_2016.xls&#39;, ## &#39;Sem_5nov_11nov_2016.xls&#39;, &#39;Sem_6ago_12ago_2016.xls&#39;, ## &#39;Sem_6feb_12feb_2016.xls&#39;, &#39;Sem_7may_13may_2016.xls&#39;, ## &#39;Sem_8oct_14oct_2016.xls&#39;, &#39;Sem_9abr_15abr_2016.xls&#39;, ## &#39;Sem_9ene_15ene_2016.xls&#39;, &#39;Sem_9jul_15jul_2016.xls&#39;, ## &#39;Sem_10jun_16jun_2017.xls&#39;, &#39;Sem_11feb_17feb_2017.xls&#39;, ## &#39;Sem_11mar_17mar_2017.xls&#39;, &#39;Sem_11nov_17nov_2017.xls&#39;, ## &#39;Sem_12ago_18ago_2017.xls&#39;, &#39;Sem_13abr_21abr_2017.xls&#39;, ## &#39;Sem_13may_19may_2017.xls&#39;, &#39;Sem_14ene_2017_20ene_2017.xls&#39;, ## &#39;Sem_14oct_2017_20oct_2017&#39;, &#39;Sem_15jul_21jul_2017.xls&#39;, ## &#39;Sem_16dic_22dic_2017.xls&#39;, &#39;Sem_16sep_22sep_2017&#39;, ## &#39;Sem_17jun_23jun_2017.xls&#39;, &#39;Sem_18feb_24feb_2017.xls&#39;, ## &#39;Sem_18mar_24mar_2016.xls&#39;, &#39;Sem_18nov_24nov_2017.xls&#39;, ## &#39;Sem_19ago_25ago_2017.xls&#39;, &#39;Sem_1abr_7abr_2017.xls&#39;, ## &#39;Sem_1jul_7jul_2017.xls&#39;, &#39;Sem_20may_26may_2017.xls&#39;, ## &#39;Sem_21ene_27ene_2017.xls&#39;, &#39;Sem_21oct_27oct_2017.xls&#39;, ## &#39;Sem_22abr_28abr_2017.xls&#39;, &#39;Sem_22jul_28jul_2017.xls&#39;, ## &#39;Sem_23dic_29dic_2017.xls&#39;, &#39;Sem_23sep_29sep_2017.xls&#39;, ## &#39;Sem_24jun_30jun_2017.xls&#39;, &#39;Sem_25feb_3mar_2017.xls&#39;, ## &#39;Sem_25mar_31mar_2017.xls&#39;, &#39;Sem_25nov_1dic_2017.xls&#39;, ## &#39;Sem_26ago_1sep_2017.xls&#39;, &#39;Sem_27may_2jun_2017.xls&#39;, ## &#39;Sem_28ene_3feb_2017.xls&#39;, &#39;Sem_28oct_3nov_2017.xls&#39;, ## &#39;Sem_29abr_5may_2017.xls&#39;, &#39;Sem_29jul_4ago_2017.xls&#39;, ## &#39;Sem_2dic_7dic_2017.xls&#39;, &#39;Sem_2sep_8sep_2017.xls&#39;, ## &#39;Sem_30sep_6oct_2017.xls&#39;, &#39;Sem_31dic_2016_6ene_2017.xls&#39;, ## &#39;Sem_3jun_9jun_2017.xls&#39;, &#39;Sem_4feb_10feb_2017.xls&#39;, ## &#39;Sem_4mar_10mar_2017.xls&#39;, &#39;Sem_4nov_2017_10nov_2017&#39;, ## &#39;Sem_5ago_11ago_2017.xls&#39;, &#39;Sem_6may_12may_2017.xls&#39;, ## &#39;Sem_7ene_2017_13ene_2017.xls&#39;, &#39;Sem_7oct_13oct_2017.xls&#39;, ## &#39;Sem_8abr_12abr_2017.xls&#39;, &#39;Sem_8jul_14jul_2017.xls&#39;, ## &#39;Sem_9dic_15dic_2017.xls&#39;, &#39;Sem_9sep_15sep_2017.xls&#39;, ## &#39;Sem_03mar__09mar_2018&#39;, &#39;Sem_05may__11may_2018&#39;, ## &#39;Sem_07abr__13abr_2018&#39;, &#39;Sem_10feb_16feb_2018.xls&#39;, ## &#39;Sem_10mar__16mar_2018&#39;, &#39;Sem_13ene_19ene_2018.xls&#39;, ## &#39;Sem_14abr__20abr_2018&#39;, &#39;Sem_17feb_23feb_2018.xls&#39;, ## &#39;Sem_17mar__23mar_2018&#39;, &#39;Sem_20ene_26ene_2018.xls&#39;, ## &#39;Sem_21abr__27abr_2018&#39;, &#39;Sem_24feb__02mar_2018&#39;, ## &#39;Sem_24mar__28mar_2018&#39;, &#39;Sem_27ene_2feb_2018.xls&#39;, ## &#39;Sem_28abr__04may_2018&#39;, &#39;Sem_30dic_5ene_2018.xls&#39;, ## &#39;Sem_31mar__06abr_2018&#39;, &#39;Sem_3feb_9feb_2018.xls&#39;, ## &#39;Sem_6ene_12ene_2018.xls&#39;, &#39;Sem_01dic__07dic_2018&#39;, ## &#39;Sem_01sep__07sep_2018&#39;, &#39;Sem_02jun__08jun_2018&#39;, ## &#39;Sem_03nov__09nov_2018&#39;, &#39;Sem_04ago__10ago_2018&#39;, ## &#39;Sem_06oct__12oct_2018&#39;, &#39;Sem_07jul__13jul_2018&#39;, ## &#39;Sem_08dic__14dic_2018&#39;, &#39;Sem_08sep__14sep_2018&#39;, ## &#39;Sem_09jun__15jun_2018&#39;, &#39;Sem_10nov__16nov_2018&#39;, ## &#39;Sem_11ago__17ago_2018&#39;, &#39;Sem_12may__18may_2018_int&#39;, ## &#39;Sem_13oct__19oct_2018&#39;, &#39;Sem_14jul__19jul_2018&#39;, ## &#39;Sem_15dic__21dic_2018&#39;, &#39;Sem_15sep__21sep_2018&#39;, ## &#39;Sem_16jun__22jun_2018&#39;, &#39;Sem_17nov__23nov_2018&#39;, ## &#39;Sem_18ago__24ago_2018&#39;, &#39;Sem_19may__25may_2018&#39;, ## &#39;Sem_20jul__27jul_2018&#39;, &#39;Sem_20oct__26oct_2018&#39;, ## &#39;Sem_22dic__28dic_2018&#39;, &#39;Sem_22sep__28sep_2018&#39;, ## &#39;Sem_23jun__29jun_2018&#39;, &#39;Sem_24nov__30nov_2018&#39;, ## &#39;Sem_25ago__31ago_2018&#39;, &#39;Sem_26may__01jun_2018&#39;, ## &#39;Sem_27oct__02nov_2018&#39;, &#39;Sem_28jul__03ago_2018&#39;, ## &#39;Sem_29sep__05oct_2018&#39;, &#39;Sem_30jun__06jul_2018&#39;, ## &#39;Sem_01jun_2019__07jun_2019&#39;, &#39;Sem_02feb_2019__08feb_2019&#39;, ## &#39;Sem_02mar_2019__08mar_2019&#39;, &#39;Sem_02nov_2019__08nov_2019&#39;, ## &#39;Sem_03ago_2019__09ago_2019&#39;, &#39;Sem_04may_2019__10may_2019&#39;, ## &#39;Sem_05oct_2019__11oct_2019&#39;, &#39;Sem_06abr_2019__12abr_2019&#39;, ## &#39;Sem_06jul_2019__12jul_2019&#39;, &#39;Sem_07dic_2019__13dic_2019&#39;, ## &#39;Sem_07sep_2019__13sep_2019&#39;, &#39;Sem_08jun_2019__14jun_2019&#39;, ## &#39;Sem_09feb_2019__15feb_2019&#39;, &#39;Sem_09mar_2019__15mar_2019&#39;, ## &#39;Sem_09nov_2019__15nov_2019&#39;, &#39;Sem_10ago_2019__16ago_2019&#39;, ## &#39;Sem_11may_2019__17may_2019&#39;, &#39;Sem_12ene_2019_18ene_2019&#39;, ## &#39;Sem_12oct_2019__18oct_2019&#39;, &#39;Sem_13abr_2019__17abr_2019&#39;, ## &#39;Sem_13jul_2019__19jul_2019&#39;, &#39;Sem_14dic_2019__20dic_2019&#39;, ## &#39;Sem_14sep_2019__20sep_2019&#39;, &#39;Sem_15jun_2019__21jun_2019&#39;, ## &#39;Sem_16feb_2019__22feb_2019&#39;, &#39;Sem_16mar_2019__22mar_2019&#39;, ## &#39;Sem_16nov_2019__22nov_2019&#39;, &#39;Sem_17ago_2019__23ago_2019&#39;, ## &#39;Sem_18may_2019_24may_2019&#39;, &#39;Sem_19ene_2019__25ene_2019&#39;, ## &#39;Sem_19oct_2019__25oct_2019&#39;, &#39;Sem_20abr_2019__26abr_2019&#39;, ## &#39;Sem_20jul_2019__26jul_2019&#39;, &#39;Sem_21dic_2019__27dic_2019&#39;, ## &#39;Sem_21sep_2019__27sep_2019&#39;, &#39;Sem_22jun_2019__28jun_2019&#39;, ## &#39;Sem_23feb_2019__01mar_2019&#39;, &#39;Sem_23mar_2019__29mar_2019&#39;, ## &#39;Sem_23nov_2019__29nov_2019&#39;, &#39;Sem_24ago_2019__30ago_2019&#39;, ## &#39;Sem_25may_2019__31may_2019&#39;, &#39;Sem_26ene_2019__01feb_2019&#39;, ## &#39;Sem_26oct_2019__01nov_2019&#39;, &#39;Sem_27abr_2019__03may_2019&#39;, ## &#39;Sem_27jul_2019__02ago_2019&#39;, &#39;Sem_28sep_2019__04oct_2019&#39;, ## &#39;Sem_29dic_2018_4ene_2019&#39;, &#39;Sem_29jun_2019__05jul_2019&#39;, ## &#39;Sem_30mar_2019__05abr_2019&#39;, &#39;Sem_30nov_2019__06dic_2019&#39;, ## &#39;Sem_31ago_2019__06sep_2019&#39;, &#39;Sem_5ene_2019_11ene_2019&#39;, ## &#39;Sem_01ago_2020__06ago_2020&#39;, &#39;Sem_01feb_2020__07feb_2020&#39;, ## &#39;Sem_01may_2020__08may_2020&#39;, &#39;Sem_04abr_2020__08abr_2020&#39;, ## &#39;Sem_04ene_2020__10ene_2020&#39;, &#39;Sem_04jul_2020__10jul_2020&#39;, ## &#39;Sem_05dic_2020__11dic_2020&#39;, &#39;Sem_05oct_2020__090ct_2020&#39;, ## &#39;Sem_05sep_2020__11sep_2020&#39;, &#39;Sem_06jun_2020__12jun_2020&#39;, ## &#39;Sem_07ago_2020__14ago_2020&#39;, &#39;Sem_07mar_2020__13mar_2020&#39;, ## &#39;Sem_07nov_2020__13nov_2020&#39;, &#39;Sem_08feb_2020__14feb_2020&#39;, ## &#39;Sem_09may_2020__15may_2020&#39;, &#39;Sem_11abr_2020__17abr_2020&#39;, ## &#39;Sem_11ene_2020__17ene_2020&#39;, &#39;Sem_11jul_2020__17jul_2020&#39;, ## &#39;Sem_12dic_2020__18dic_2020&#39;, &#39;Sem_12oct_2020__16oct_2020&#39;, ## &#39;Sem_12sep_2020__18sep_2020&#39;, &#39;Sem_13jun_2020__19jun_2020&#39;, ## &#39;Sem_14mar_2020__20mar_2020&#39;, &#39;Sem_14nov_2020__20nov_2020&#39;, ## &#39;Sem_15ago_2020__21ago_2020&#39;, &#39;Sem_15feb_2020__21feb_2020&#39;, ## &#39;Sem_16may_2020__22may_2020&#39;, &#39;Sem_17oct_2020__23oct_2020&#39;, ## &#39;Sem_18abr_2020__24abr_2020&#39;, &#39;Sem_18ene_2020__24ene_2020&#39;, ## &#39;Sem_18jul_2020__24jul_2020&#39;, &#39;Sem_19dic_2020__24dic_2020&#39;, ## &#39;Sem_19sep_2020__25sep_2020&#39;, &#39;Sem_20jun_2020__26jun_2020&#39;, ## &#39;Sem_21mar_2020__27mar_2020&#39;, &#39;Sem_21nov_2020__27nov_2020&#39;, ## &#39;Sem_22ago_2020__28ago_2020&#39;, &#39;Sem_22feb_2020__28feb_2020&#39;, ## &#39;Sem_23may_2020__29may_2020&#39;, &#39;Sem_24oct_2020__30oct_2020&#39;, ## &#39;Sem_25abr_2020__30abr_2020&#39;, &#39;Sem_25ene_2020__31ene_2020&#39;, ## &#39;Sem_25jul_2020__31jul_2020&#39;, &#39;Sem_26dic_2020__31dic_2020&#39;, ## &#39;Sem_26sep_2020__02oct_2020&#39;, &#39;Sem_27jun_2020__03jul_2020&#39;, ## &#39;Sem_28dic_2019__03ene_2020&#39;, &#39;Sem_28mar_2020__03abr_2020&#39;, ## &#39;Sem_28nov_2020__04dic_2020&#39;, &#39;Sem_29ago_2020__04sep_2020&#39;, ## &#39;Sem_29feb_2020__06mar_2020&#39;, &#39;Sem_30may_2020__05jun_2020&#39;, ## &#39;Sem_31oct_2020__06nov_2020&#39;, &#39;Sem_01may_2021_07may_2021.xls&#39;, ## &#39;Sem_02ene_2021__08ene_2021&#39;, &#39;Sem_02oct_2021_08oct_2021.xls&#39;, ## &#39;Sem_03abr_2021_09abr_2021.xls&#39;, &#39;Sem_03jul_2021_09jul_2021.xls&#39;, ## &#39;Sem_04dic_2021__10dic_2021&#39;, &#39;Sem_04sep_2021_10sep_2021.xls&#39;, ## &#39;Sem_05jun_2021_11jun_2021.xls&#39;, &#39;Sem_06feb_2021__12feb_2021&#39;, ## &#39;Sem_06mar_2021_12mar_2021.xls&#39;, &#39;Sem_07ago_2021_13ago_2021.xls&#39;, ## &#39;Sem_08may_2021_14may_2021.xls&#39;, &#39;Sem_09ene_2021__15ene_2021&#39;, ## &#39;Sem_09oct_2021_15oct_2021.xls&#39;, &#39;Sem_10abr_2021_16abr_2021.xls&#39;, ## &#39;Sem_10jul_2021_16jul_2021.xls&#39;, &#39;Sem_11dic_2021_17dic_2021&#39;, ## &#39;Sem_11sep_2021_17sep_2021.xls&#39;, &#39;Sem_12jun_2021_18jun_2021.xls&#39;, ## &#39;Sem_13feb_2021__19feb_2021&#39;, &#39;Sem_13mar_2021_19mar_2021.xls&#39;, ## &#39;Sem_13nov_2021__19nov_2021&#39;, &#39;Sem_14ago_2021_20ago_2021.xls&#39;, ## &#39;Sem_15may_2021_21may_2021.xls&#39;, &#39;Sem_16ene_2021__22ene_2021&#39;, ## &#39;Sem_16oct_2021_22oct_2021.xls&#39;, &#39;Sem_17abr_2021_23abr_2021.xls&#39;, ## &#39;Sem_17jul_2021_23jul_2021.xls&#39;, &#39;Sem_18dic_2021_24dic_2021&#39;, ## &#39;Sem_18sep_2021_24sep_2021.xls&#39;, &#39;Sem_19jun_2021_25jun_2021.xls&#39;, ## &#39;Sem_20feb_2021__26feb_2021&#39;, &#39;Sem_20mar_2021_26mar_2021.xls&#39;, ## &#39;Sem_20nov_2021__26nov_2021&#39;, &#39;Sem_21ago_2021_27ago_2021.xls&#39;, ## &#39;Sem_22may_2021_28may_2021.xls&#39;, &#39;Sem_23ene_2021__29ene_2021&#39;, ## &#39;Sem_23oct_2021_29oct_2021.xls&#39;, &#39;Sem_24abr_2021_30abr_2021.xls&#39;, ## &#39;Sem_24jul_2021_30jul_2021.xls&#39;, &#39;Sem_25dic_2021_31dic_2021&#39;, ## &#39;Sem_25sep_2021_01oct_2021.xls&#39;, &#39;Sem_26jun_2021_02jul_2021.xls&#39;, ## &#39;Sem_27feb_2021_05mar_2021&#39;, &#39;Sem_27mar_2021_31mar_2021.xls&#39;, ## &#39;Sem_27nov_2021__03dic_2021&#39;, &#39;Sem_28ago_2021_03sep_2021.xls&#39;, ## &#39;Sem_29may_2021_04jun_2021.xls&#39;, &#39;Sem_30ene_2021__05feb_2021&#39;, ## &#39;Sem_30oct_2021__05nov_2021.xls&#39;, &#39;Sem_31jul_2021_06ago_2021.xls&#39;, ## &#39;anex_01oct_al_07oct_2022&#39;, &#39;anex_02abr_al_08abr_2022&#39;, ## &#39;anex_02jul_al_08jul_2022&#39;, &#39;anex_03sep_al_08sep_2022&#39;, ## &#39;anex_04jun_al_10jun_2022&#39;, &#39;anex_05nov_al_11nov_2022&#39;, ## &#39;anex_06ago_al_12ago_2022&#39;, &#39;anex_07may_al_13may_2022&#39;, ## &#39;anex_08oct_al_14oct_2022&#39;, &#39;anex_09abr_al_13abr_2022&#39;, ## &#39;anex_09jul_al_15jul_2022&#39;, &#39;anex_10dic_al_16dic_2022&#39;, ## &#39;anex_10sep_al_16sep_2022&#39;, &#39;anex_11jun_al_17jun_2022&#39;, ## &#39;Anex_12mar_al_18mar_2022&#39;, &#39;anex_12nov_al_18nov_2022&#39;, ## &#39;anex_13ago_al_19ago_2022&#39;, &#39;anex_14may_al_20may_2022&#39;, ## &#39;anex_15oct_al_21oct_2022 (1)&#39;, &#39;anex_15oct_al_21oct_2022&#39;, ## &#39;anex_16abr_al_22abr_2022&#39;, &#39;anex_16jul_al_22jul_2022&#39;, ## &#39;anex_17dic_al_23dic_2022&#39;, &#39;anex_17sep_al_23sep_2022&#39;, ## &#39;anex_18jun_al_24jun_2022&#39;, &#39;anex_19mar_al_25mar_2022&#39;, ## &#39;anex_19nov_al_25nov_2022&#39;, &#39;anex_20ago_al_26ago_2022&#39;, ## &#39;anex_21may_al_27may_2022&#39;, &#39;anex_22oct_al_28oct_2022&#39;, ## &#39;anex_23abr_al_29abr_2022&#39;, &#39;anex_23jul_al_29jul_2022&#39;, ## &#39;anex_24dic_al_30dic_2022&#39;, &#39;anex_24sep_al_30sep_2022&#39;, ## &#39;anex_25jun_al_01jul_2022&#39;, &#39;anex_26mar_al_01abr_2022&#39;, ## &#39;anex_26nov_al_2dic_2022&#39;, &#39;anex_27ago_al_02sep_2022&#39;, ## &#39;anex_28may_al_03jun_2022&#39;, &#39;anex_29oct_al_04nov_2022&#39;, ## &#39;anex_30abr_al_06may_2022&#39;, &#39;anex_30jul_al_05ago_2022&#39;, ## &#39;anex_31dic_al_06ene_2023&#39;, &#39;anex_3dic_al_9dic_2022&#39;, ## &#39;Sem_01ene_2022_07ene_2022&#39;, &#39;Sem_05feb_2022_11feb_2022&#39;, ## &#39;Sem_05mar_2022_11mar_2022&#39;, &#39;Sem_08ene_2022_14ene_2022&#39;, ## &#39;Sem_12feb_2022_18feb_2022&#39;, &#39;Sem_15ene_2022_21ene_2022&#39;, ## &#39;Sem_19feb_2022_25feb_2022&#39;, &#39;Sem_22ene_2022_28ene_2022&#39;, ## &#39;Sem_26feb_2022_04mar_2022&#39;, &#39;Sem_29ene_2022_04feb_2022&#39;, nan], ## dtype=object) Al observar el contenido de la columna se puede aprecicar lo siguiente: Hay cuatro tipos de codificación de semana: Sem_10sep_16sep_2016.xls -&gt; El elemento inicial es Sem y demarca los dias referenciando una vez el año, algunos aun contienen la extensión del archivo. ’Sem_02feb_2019__08feb_2019’ -&gt; El elemento inicial es Sem, en este caso el año se referencia dos veces cada vez que se referencia el dia de la semana. ‘anex_02abr_al_08abr_2022’ -&gt; Se cambia el término Sem por anex y nuevamente el año aparece solo una vez. Sem_22sep__28sep_2018’ -&gt; caso similar al número 1 pero no cuenta con extensión. Adicionalmente se logra apreciar que las semanas se codifican de sábado a viernes teniendo inicio el sábado 02 de Enero de 2016 y finalizando el sábado 31 de 2022. Hay diferentes carácteres a en los rangos establecidos lo cual implica aplicar varias operaciones de limpieza para llegar a extraer fechas. Dado que a partir de la revisión del listado de fechas se ha logrado establecer el rango del periodo semanal y tambien encontrar generalidades de los formatos a nivel de semana, se seguirá la siguiente estrategia de trasnformación: Se eliminarán los ceros a la izquierda. Se eliminarán las extensiones de archivo y tambien se eliminarán componentes iniciales como las particulas “Sem”,“Anex”,“Al”,“int” con el fin de dejar unicamente fechas y años. Se generará temporalmente una variable longitud con el fin de conocer la variabilidad de carácteres y orientar el proceso de limpieza. Se cambiarán las iniciales de los meses por su respectivo número. Se realizará un split por medio del simbolo de separación “_” con el fin de tener listas en la columna periodo que permitan extraer elementos particulares para estructurar fechas. Se crearán funciones con elementos condicionales según la longitud del string con el fin de extraer la fecha inicial de la semana. Como primer paso se creará la función quitarcerosiniciales() la cual tendrá como objetivo remover los ceros a la izquierda que existan en los nombres de la columna: def quitarcerosiniciales(string): regex= &quot;^0+(?!$)&quot; string_salida= re.sub(regex,&quot;&quot;,string) return string_salida Seguidamente con la función anteriormente creada se aplicarán operaciones con funciones tipo lambda para remover partes de los strings de la columna que no sean relevantes para la extracción de la fecha: df_sipsa[&quot;periodo&quot;]= df_sipsa[&quot;periodo&quot;].apply(lambda x:str(x).replace(&quot;.xls&quot;,&quot;&quot;).replace(&quot;.xlsx&quot;,&quot;&quot;).replace(&quot;__&quot;,&quot;_&quot;).replace(&quot;Sem_&quot;,&quot;&quot;).replace(&quot;anex_&quot;,&quot;&quot;).replace(&quot;Anex_&quot;,&quot;&quot;).replace(&quot; (1)&quot;,&quot;&quot;).replace(&quot;_int&quot;,&quot;&quot;).replace(&quot;_al&quot;,&quot;&quot;)) df_sipsa[&quot;periodo&quot;]=df_sipsa[&quot;periodo&quot;].apply(lambda x:quitarcerosiniciales(x)) df_sipsa= df_sipsa[df_sipsa[&quot;periodo&quot;]!=&quot;nan&quot;] Al aplicar estas operaciones los registros de la columna periodo se ven de la siguiente manera: df_sipsa[&quot;periodo&quot;].unique() ## array([&#39;10dic_16dic_2016&#39;, &#39;10sep_16sep_2016&#39;, &#39;11jun_17jun_2016&#39;, ## &#39;12mar_18mar_2016&#39;, &#39;12nov_18nov_2016&#39;, &#39;13ago_19ago_2016&#39;, ## &#39;13feb_19feb_2016&#39;, &#39;14may_20may_2016&#39;, &#39;15oct_21oct_2016&#39;, ## &#39;16abr_22abr_2016&#39;, &#39;16ene_22ene_2016&#39;, &#39;16jul_22jul_2016&#39;, ## &#39;17dic_23dic_2016&#39;, &#39;17sep_23sept_2016&#39;, &#39;18jun_24jun_2016&#39;, ## &#39;19mar_23mar_2016&#39;, &#39;19nov_25nov_2016&#39;, &#39;1oct_7oct_2016&#39;, ## &#39;20ago_26ago_2016&#39;, &#39;20feb_26feb_2016&#39;, &#39;21may_27may_2016&#39;, ## &#39;22oct_28oct_2016&#39;, &#39;23abr_29abr_2016&#39;, &#39;23ene_29ene_2016&#39;, ## &#39;23jul_29jul_2016&#39;, &#39;24dic_30dic_2016&#39;, &#39;24jun_1jul_2016&#39;, ## &#39;24sep_30sep_2016&#39;, &#39;26mar_1abr_2016&#39;, &#39;26nov_2dic_2016&#39;, ## &#39;27ago_2sep_2016&#39;, &#39;27feb_4mar_2016&#39;, &#39;28may_3jun_2016&#39;, ## &#39;29oct_04nov_2016&#39;, &#39;2abr_8abr_2016&#39;, &#39;2ene_8ene_2016&#39;, ## &#39;2jul_8jul_2016&#39;, &#39;30abr_06may_2016&#39;, &#39;30ene_5feb_2016&#39;, ## &#39;30jul_5ago_2016&#39;, &#39;3dic_9dic_2016&#39;, &#39;3sep_9sep_2016&#39;, ## &#39;4jun_10jun_2016&#39;, &#39;5mar_11mar_2016&#39;, &#39;5nov_11nov_2016&#39;, ## &#39;6ago_12ago_2016&#39;, &#39;6feb_12feb_2016&#39;, &#39;7may_13may_2016&#39;, ## &#39;8oct_14oct_2016&#39;, &#39;9abr_15abr_2016&#39;, &#39;9ene_15ene_2016&#39;, ## &#39;9jul_15jul_2016&#39;, &#39;10jun_16jun_2017&#39;, &#39;11feb_17feb_2017&#39;, ## &#39;11mar_17mar_2017&#39;, &#39;11nov_17nov_2017&#39;, &#39;12ago_18ago_2017&#39;, ## &#39;13abr_21abr_2017&#39;, &#39;13may_19may_2017&#39;, &#39;14ene_2017_20ene_2017&#39;, ## &#39;14oct_2017_20oct_2017&#39;, &#39;15jul_21jul_2017&#39;, &#39;16dic_22dic_2017&#39;, ## &#39;16sep_22sep_2017&#39;, &#39;17jun_23jun_2017&#39;, &#39;18feb_24feb_2017&#39;, ## &#39;18mar_24mar_2016&#39;, &#39;18nov_24nov_2017&#39;, &#39;19ago_25ago_2017&#39;, ## &#39;1abr_7abr_2017&#39;, &#39;1jul_7jul_2017&#39;, &#39;20may_26may_2017&#39;, ## &#39;21ene_27ene_2017&#39;, &#39;21oct_27oct_2017&#39;, &#39;22abr_28abr_2017&#39;, ## &#39;22jul_28jul_2017&#39;, &#39;23dic_29dic_2017&#39;, &#39;23sep_29sep_2017&#39;, ## &#39;24jun_30jun_2017&#39;, &#39;25feb_3mar_2017&#39;, &#39;25mar_31mar_2017&#39;, ## &#39;25nov_1dic_2017&#39;, &#39;26ago_1sep_2017&#39;, &#39;27may_2jun_2017&#39;, ## &#39;28ene_3feb_2017&#39;, &#39;28oct_3nov_2017&#39;, &#39;29abr_5may_2017&#39;, ## &#39;29jul_4ago_2017&#39;, &#39;2dic_7dic_2017&#39;, &#39;2sep_8sep_2017&#39;, ## &#39;30sep_6oct_2017&#39;, &#39;31dic_2016_6ene_2017&#39;, &#39;3jun_9jun_2017&#39;, ## &#39;4feb_10feb_2017&#39;, &#39;4mar_10mar_2017&#39;, &#39;4nov_2017_10nov_2017&#39;, ## &#39;5ago_11ago_2017&#39;, &#39;6may_12may_2017&#39;, &#39;7ene_2017_13ene_2017&#39;, ## &#39;7oct_13oct_2017&#39;, &#39;8abr_12abr_2017&#39;, &#39;8jul_14jul_2017&#39;, ## &#39;9dic_15dic_2017&#39;, &#39;9sep_15sep_2017&#39;, &#39;3mar_09mar_2018&#39;, ## &#39;5may_11may_2018&#39;, &#39;7abr_13abr_2018&#39;, &#39;10feb_16feb_2018&#39;, ## &#39;10mar_16mar_2018&#39;, &#39;13ene_19ene_2018&#39;, &#39;14abr_20abr_2018&#39;, ## &#39;17feb_23feb_2018&#39;, &#39;17mar_23mar_2018&#39;, &#39;20ene_26ene_2018&#39;, ## &#39;21abr_27abr_2018&#39;, &#39;24feb_02mar_2018&#39;, &#39;24mar_28mar_2018&#39;, ## &#39;27ene_2feb_2018&#39;, &#39;28abr_04may_2018&#39;, &#39;30dic_5ene_2018&#39;, ## &#39;31mar_06abr_2018&#39;, &#39;3feb_9feb_2018&#39;, &#39;6ene_12ene_2018&#39;, ## &#39;1dic_07dic_2018&#39;, &#39;1sep_07sep_2018&#39;, &#39;2jun_08jun_2018&#39;, ## &#39;3nov_09nov_2018&#39;, &#39;4ago_10ago_2018&#39;, &#39;6oct_12oct_2018&#39;, ## &#39;7jul_13jul_2018&#39;, &#39;8dic_14dic_2018&#39;, &#39;8sep_14sep_2018&#39;, ## &#39;9jun_15jun_2018&#39;, &#39;10nov_16nov_2018&#39;, &#39;11ago_17ago_2018&#39;, ## &#39;12may_18may_2018&#39;, &#39;13oct_19oct_2018&#39;, &#39;14jul_19jul_2018&#39;, ## &#39;15dic_21dic_2018&#39;, &#39;15sep_21sep_2018&#39;, &#39;16jun_22jun_2018&#39;, ## &#39;17nov_23nov_2018&#39;, &#39;18ago_24ago_2018&#39;, &#39;19may_25may_2018&#39;, ## &#39;20jul_27jul_2018&#39;, &#39;20oct_26oct_2018&#39;, &#39;22dic_28dic_2018&#39;, ## &#39;22sep_28sep_2018&#39;, &#39;23jun_29jun_2018&#39;, &#39;24nov_30nov_2018&#39;, ## &#39;25ago_31ago_2018&#39;, &#39;26may_01jun_2018&#39;, &#39;27oct_02nov_2018&#39;, ## &#39;28jul_03ago_2018&#39;, &#39;29sep_05oct_2018&#39;, &#39;30jun_06jul_2018&#39;, ## &#39;1jun_2019_07jun_2019&#39;, &#39;2feb_2019_08feb_2019&#39;, ## &#39;2mar_2019_08mar_2019&#39;, &#39;2nov_2019_08nov_2019&#39;, ## &#39;3ago_2019_09ago_2019&#39;, &#39;4may_2019_10may_2019&#39;, ## &#39;5oct_2019_11oct_2019&#39;, &#39;6abr_2019_12abr_2019&#39;, ## &#39;6jul_2019_12jul_2019&#39;, &#39;7dic_2019_13dic_2019&#39;, ## &#39;7sep_2019_13sep_2019&#39;, &#39;8jun_2019_14jun_2019&#39;, ## &#39;9feb_2019_15feb_2019&#39;, &#39;9mar_2019_15mar_2019&#39;, ## &#39;9nov_2019_15nov_2019&#39;, &#39;10ago_2019_16ago_2019&#39;, ## &#39;11may_2019_17may_2019&#39;, &#39;12ene_2019_18ene_2019&#39;, ## &#39;12oct_2019_18oct_2019&#39;, &#39;13abr_2019_17abr_2019&#39;, ## &#39;13jul_2019_19jul_2019&#39;, &#39;14dic_2019_20dic_2019&#39;, ## &#39;14sep_2019_20sep_2019&#39;, &#39;15jun_2019_21jun_2019&#39;, ## &#39;16feb_2019_22feb_2019&#39;, &#39;16mar_2019_22mar_2019&#39;, ## &#39;16nov_2019_22nov_2019&#39;, &#39;17ago_2019_23ago_2019&#39;, ## &#39;18may_2019_24may_2019&#39;, &#39;19ene_2019_25ene_2019&#39;, ## &#39;19oct_2019_25oct_2019&#39;, &#39;20abr_2019_26abr_2019&#39;, ## &#39;20jul_2019_26jul_2019&#39;, &#39;21dic_2019_27dic_2019&#39;, ## &#39;21sep_2019_27sep_2019&#39;, &#39;22jun_2019_28jun_2019&#39;, ## &#39;23feb_2019_01mar_2019&#39;, &#39;23mar_2019_29mar_2019&#39;, ## &#39;23nov_2019_29nov_2019&#39;, &#39;24ago_2019_30ago_2019&#39;, ## &#39;25may_2019_31may_2019&#39;, &#39;26ene_2019_01feb_2019&#39;, ## &#39;26oct_2019_01nov_2019&#39;, &#39;27abr_2019_03may_2019&#39;, ## &#39;27jul_2019_02ago_2019&#39;, &#39;28sep_2019_04oct_2019&#39;, ## &#39;29dic_2018_4ene_2019&#39;, &#39;29jun_2019_05jul_2019&#39;, ## &#39;30mar_2019_05abr_2019&#39;, &#39;30nov_2019_06dic_2019&#39;, ## &#39;31ago_2019_06sep_2019&#39;, &#39;5ene_2019_11ene_2019&#39;, ## &#39;1ago_2020_06ago_2020&#39;, &#39;1feb_2020_07feb_2020&#39;, ## &#39;1may_2020_08may_2020&#39;, &#39;4abr_2020_08abr_2020&#39;, ## &#39;4ene_2020_10ene_2020&#39;, &#39;4jul_2020_10jul_2020&#39;, ## &#39;5dic_2020_11dic_2020&#39;, &#39;5oct_2020_090ct_2020&#39;, ## &#39;5sep_2020_11sep_2020&#39;, &#39;6jun_2020_12jun_2020&#39;, ## &#39;7ago_2020_14ago_2020&#39;, &#39;7mar_2020_13mar_2020&#39;, ## &#39;7nov_2020_13nov_2020&#39;, &#39;8feb_2020_14feb_2020&#39;, ## &#39;9may_2020_15may_2020&#39;, &#39;11abr_2020_17abr_2020&#39;, ## &#39;11ene_2020_17ene_2020&#39;, &#39;11jul_2020_17jul_2020&#39;, ## &#39;12dic_2020_18dic_2020&#39;, &#39;12oct_2020_16oct_2020&#39;, ## &#39;12sep_2020_18sep_2020&#39;, &#39;13jun_2020_19jun_2020&#39;, ## &#39;14mar_2020_20mar_2020&#39;, &#39;14nov_2020_20nov_2020&#39;, ## &#39;15ago_2020_21ago_2020&#39;, &#39;15feb_2020_21feb_2020&#39;, ## &#39;16may_2020_22may_2020&#39;, &#39;17oct_2020_23oct_2020&#39;, ## &#39;18abr_2020_24abr_2020&#39;, &#39;18ene_2020_24ene_2020&#39;, ## &#39;18jul_2020_24jul_2020&#39;, &#39;19dic_2020_24dic_2020&#39;, ## &#39;19sep_2020_25sep_2020&#39;, &#39;20jun_2020_26jun_2020&#39;, ## &#39;21mar_2020_27mar_2020&#39;, &#39;21nov_2020_27nov_2020&#39;, ## &#39;22ago_2020_28ago_2020&#39;, &#39;22feb_2020_28feb_2020&#39;, ## &#39;23may_2020_29may_2020&#39;, &#39;24oct_2020_30oct_2020&#39;, ## &#39;25abr_2020_30abr_2020&#39;, &#39;25ene_2020_31ene_2020&#39;, ## &#39;25jul_2020_31jul_2020&#39;, &#39;26dic_2020_31dic_2020&#39;, ## &#39;26sep_2020_02oct_2020&#39;, &#39;27jun_2020_03jul_2020&#39;, ## &#39;28dic_2019_03ene_2020&#39;, &#39;28mar_2020_03abr_2020&#39;, ## &#39;28nov_2020_04dic_2020&#39;, &#39;29ago_2020_04sep_2020&#39;, ## &#39;29feb_2020_06mar_2020&#39;, &#39;30may_2020_05jun_2020&#39;, ## &#39;31oct_2020_06nov_2020&#39;, &#39;1may_2021_07may_2021&#39;, ## &#39;2ene_2021_08ene_2021&#39;, &#39;2oct_2021_08oct_2021&#39;, ## &#39;3abr_2021_09abr_2021&#39;, &#39;3jul_2021_09jul_2021&#39;, ## &#39;4dic_2021_10dic_2021&#39;, &#39;4sep_2021_10sep_2021&#39;, ## &#39;5jun_2021_11jun_2021&#39;, &#39;6feb_2021_12feb_2021&#39;, ## &#39;6mar_2021_12mar_2021&#39;, &#39;7ago_2021_13ago_2021&#39;, ## &#39;8may_2021_14may_2021&#39;, &#39;9ene_2021_15ene_2021&#39;, ## &#39;9oct_2021_15oct_2021&#39;, &#39;10abr_2021_16abr_2021&#39;, ## &#39;10jul_2021_16jul_2021&#39;, &#39;11dic_2021_17dic_2021&#39;, ## &#39;11sep_2021_17sep_2021&#39;, &#39;12jun_2021_18jun_2021&#39;, ## &#39;13feb_2021_19feb_2021&#39;, &#39;13mar_2021_19mar_2021&#39;, ## &#39;13nov_2021_19nov_2021&#39;, &#39;14ago_2021_20ago_2021&#39;, ## &#39;15may_2021_21may_2021&#39;, &#39;16ene_2021_22ene_2021&#39;, ## &#39;16oct_2021_22oct_2021&#39;, &#39;17abr_2021_23abr_2021&#39;, ## &#39;17jul_2021_23jul_2021&#39;, &#39;18dic_2021_24dic_2021&#39;, ## &#39;18sep_2021_24sep_2021&#39;, &#39;19jun_2021_25jun_2021&#39;, ## &#39;20feb_2021_26feb_2021&#39;, &#39;20mar_2021_26mar_2021&#39;, ## &#39;20nov_2021_26nov_2021&#39;, &#39;21ago_2021_27ago_2021&#39;, ## &#39;22may_2021_28may_2021&#39;, &#39;23ene_2021_29ene_2021&#39;, ## &#39;23oct_2021_29oct_2021&#39;, &#39;24abr_2021_30abr_2021&#39;, ## &#39;24jul_2021_30jul_2021&#39;, &#39;25dic_2021_31dic_2021&#39;, ## &#39;25sep_2021_01oct_2021&#39;, &#39;26jun_2021_02jul_2021&#39;, ## &#39;27feb_2021_05mar_2021&#39;, &#39;27mar_2021_31mar_2021&#39;, ## &#39;27nov_2021_03dic_2021&#39;, &#39;28ago_2021_03sep_2021&#39;, ## &#39;29may_2021_04jun_2021&#39;, &#39;30ene_2021_05feb_2021&#39;, ## &#39;30oct_2021_05nov_2021&#39;, &#39;31jul_2021_06ago_2021&#39;, ## &#39;1oct_07oct_2022&#39;, &#39;2abr_08abr_2022&#39;, &#39;2jul_08jul_2022&#39;, ## &#39;3sep_08sep_2022&#39;, &#39;4jun_10jun_2022&#39;, &#39;5nov_11nov_2022&#39;, ## &#39;6ago_12ago_2022&#39;, &#39;7may_13may_2022&#39;, &#39;8oct_14oct_2022&#39;, ## &#39;9abr_13abr_2022&#39;, &#39;9jul_15jul_2022&#39;, &#39;10dic_16dic_2022&#39;, ## &#39;10sep_16sep_2022&#39;, &#39;11jun_17jun_2022&#39;, &#39;12mar_18mar_2022&#39;, ## &#39;12nov_18nov_2022&#39;, &#39;13ago_19ago_2022&#39;, &#39;14may_20may_2022&#39;, ## &#39;15oct_21oct_2022&#39;, &#39;16abr_22abr_2022&#39;, &#39;16jul_22jul_2022&#39;, ## &#39;17dic_23dic_2022&#39;, &#39;17sep_23sep_2022&#39;, &#39;18jun_24jun_2022&#39;, ## &#39;19mar_25mar_2022&#39;, &#39;19nov_25nov_2022&#39;, &#39;20ago_26ago_2022&#39;, ## &#39;21may_27may_2022&#39;, &#39;22oct_28oct_2022&#39;, &#39;23abr_29abr_2022&#39;, ## &#39;23jul_29jul_2022&#39;, &#39;24dic_30dic_2022&#39;, &#39;24sep_30sep_2022&#39;, ## &#39;25jun_01jul_2022&#39;, &#39;26mar_01abr_2022&#39;, &#39;26nov_2dic_2022&#39;, ## &#39;27ago_02sep_2022&#39;, &#39;28may_03jun_2022&#39;, &#39;29oct_04nov_2022&#39;, ## &#39;30abr_06may_2022&#39;, &#39;30jul_05ago_2022&#39;, &#39;31dic_06ene_2023&#39;, ## &#39;3dic_9dic_2022&#39;, &#39;1ene_2022_07ene_2022&#39;, &#39;5feb_2022_11feb_2022&#39;, ## &#39;5mar_2022_11mar_2022&#39;, &#39;8ene_2022_14ene_2022&#39;, ## &#39;12feb_2022_18feb_2022&#39;, &#39;15ene_2022_21ene_2022&#39;, ## &#39;19feb_2022_25feb_2022&#39;, &#39;22ene_2022_28ene_2022&#39;, ## &#39;26feb_2022_04mar_2022&#39;, &#39;29ene_2022_04feb_2022&#39;], dtype=object) Ahora el formato de los strings de periodo tienen un aspecto mucho más regular que permite realizar un trabajo más preciso para la extracción de fecha; para conocer que tan variados son los strings se crea una columna provisional denominada “longitud” la cual contiene el número de carácteres que tienen los strings de la columna: df_sipsa[&quot;longitud&quot;]= df_sipsa[&quot;periodo&quot;].apply(lambda x:len(x)) df_sipsa[&quot;longitud&quot;].value_counts() ## 16 535823 ## 21 527565 ## 15 293397 ## 20 236411 ## 14 60152 ## 17 4645 ## Name: longitud, dtype: int64 Aun hay una variedad importante de strings no obstante como se reviso con anterioridad el formato ahora solo contiene referencias de fechas y separadores de guion bajo, con el fin de no tener que manejar “textos” para el formateo de las fechas en las siguiente lineas de código se reemplaza el texto corto de los meses [ejemplo: “ene”,“feb”, etc..] por su forma númerica, seguidamente se eliminan otros simbolos que podrian interferir: df_sipsa[&quot;periodo&quot;]= df_sipsa[&quot;periodo&quot;].apply(lambda x:x.replace(&quot;ene&quot;,&quot;01&quot;)\\ .replace(&quot;feb&quot;,&quot;02&quot;).replace(&quot;mar&quot;,&quot;03&quot;)\\ .replace(&quot;abr&quot;,&quot;04&quot;).replace(&quot;may&quot;,&quot;05&quot;)\\ .replace(&quot;jun&quot;,&quot;06&quot;).replace(&quot;jul&quot;,&quot;07&quot;)\\ .replace(&quot;ago&quot;,&quot;08&quot;).replace(&quot;sep&quot;,&quot;09&quot;)\\ .replace(&quot;oct&quot;,&quot;10&quot;).replace(&quot;nov&quot;,&quot;11&quot;)\\ .replace(&quot;dic&quot;,&quot;12&quot;).replace(&quot;-&quot;,&quot;&quot;).replace(&quot;t&quot;,&quot;&quot;)) print(df_sipsa[&quot;longitud&quot;].value_counts(),&quot;\\n&quot;) ## 16 535823 ## 21 527565 ## 15 293397 ## 20 236411 ## 14 60152 ## 17 4645 ## Name: longitud, dtype: int64 print(df_sipsa[df_sipsa[&quot;longitud&quot;]==14][&quot;periodo&quot;][0:1]) ## 78986 110_710_2016 ## Name: periodo, dtype: object print(df_sipsa[df_sipsa[&quot;longitud&quot;]==15][&quot;periodo&quot;][0:1]) ## 120876 2406_107_2016 ## Name: periodo, dtype: object print(df_sipsa[df_sipsa[&quot;longitud&quot;]==16][&quot;periodo&quot;][0:1]) ## 0 1012_1612_2016 ## Name: periodo, dtype: object print(df_sipsa[df_sipsa[&quot;longitud&quot;]==17][&quot;periodo&quot;][0:1]) ## 60380 1709_2309_2016 ## Name: periodo, dtype: object print(df_sipsa[df_sipsa[&quot;longitud&quot;]==20][&quot;periodo&quot;][0:1]) ## 422874 3112_2016_601_2017 ## Name: periodo, dtype: object print(df_sipsa[df_sipsa[&quot;longitud&quot;]==21][&quot;periodo&quot;][0:1]) ## 273348 1401_2017_2001_2017 ## Name: periodo, dtype: object El procesamiento ha sido efectivo, ahora el string solo esta conformado por números y guiones, esto tambien permite evidenciar que las fechas están expresadas de diferentes maneras, por ende en algunos casos la longitud es variable y da lugar a los casos que se listan en el código anteriormente ejecutado. El hecho lograr el formato que solo tiene números y guiones bajos permite realizar una separación a través de un split que genere listas que independicen cada término de la información de fecha: df_sipsa[&quot;periodo&quot;] = df_sipsa[&quot;periodo&quot;].apply(lambda x:x.split(&quot;_&quot;)) El nuevo aspecto de los registros en la columna “periodo” es el siguiente: df_sipsa[&quot;periodo&quot;].head() ## 0 [1012, 1612, 2016] ## 1 [1012, 1612, 2016] ## 2 [1012, 1612, 2016] ## 3 [1012, 1612, 2016] ## 4 [1012, 1612, 2016] ## Name: periodo, dtype: object Al llegar a este nivel de procesamiento se puede apreciar claramente una diferenciación entre elementos como el año, el mes y el dia, dado que se conoce a priori que los datos tienen una frecuencia de generación semanal, resulta práctico tomar solo un dia como referencia de la semana esto permite que solo sea necesario procesar el primer término de las listas para obtener la fecha, siguiendo esta premisa se crea la función weekstart_conversor() que toma una lista de la columna periodo y realiza operaciones de slicing con el fin de obtener componentes relevantes para conocer la fecha a la cual se hace referencia: def weekstart_conversor(columna_periodo): longitud_lista= len(columna_periodo) longitud_primer_elemento= None if longitud_lista==3: primer_elemento= columna_periodo[0] anio= columna_periodo[2] longitud_primer_elemento= len(primer_elemento) if longitud_primer_elemento==3: primer_elemento= &quot;0&quot;+primer_elemento dia= primer_elemento[0:2] mes= primer_elemento[2:4] else: dia= primer_elemento[0:2] mes= primer_elemento[2:4] else: primer_elemento=columna_periodo[0] anio= columna_periodo[1] longitud_primer_elemento=len(primer_elemento) if longitud_primer_elemento==3: primer_elemento= &quot;0&quot;+primer_elemento dia= primer_elemento[0:2] mes= primer_elemento[2:4] else: dia= primer_elemento[0:2] mes= primer_elemento[2:4] week_start= dia+&quot;-&quot;+mes+&quot;-&quot;+anio week_start_date= datetime.strptime(week_start,&#39;%d-%m-%Y&#39;) return week_start_date Una vez construida la función se procede a aplicarla sobre el set de datos principal a través de una función tipo lambda: df_sipsa[&quot;dia_inicio_semana&quot;]= df_sipsa[&quot;periodo&quot;].apply(lambda x:weekstart_conversor(x)) Una vez aplicada la función se puede apreciar que la nueva columna “dia_inicio_semana” contiene ahora la fecha del primer dia de la semana que toma el boletín : df_sipsa[&quot;dia_inicio_semana&quot;].head() ## 0 2016-12-10 ## 1 2016-12-10 ## 2 2016-12-10 ## 3 2016-12-10 ## 4 2016-12-10 ## Name: dia_inicio_semana, dtype: datetime64[ns] Teniendo en cuenta que el proceso de normalización de la fecha ha sido exitoso se procede a filtrar unicamente las centrales de abasto de interés que en este caso son unicamente las que correspondan a Armenia y a Pereira, asi mismo solo resultan de interés los productos correspondientes a cebolla junca, habichuelas, tomate chonto y ahuyama;en la siguiente linea de código se ejecuta el filtrado del dataset: centrales_interes= [&#39;Armenia, Mercar&#39;,&#39;Pereira, La 41&#39;,&#39;Pereira, Mercasa&#39;,&#39;Armenia, Retiro&#39;] productos= [&#39;Cebolla junca&#39;,&#39;Tomate chonto&#39;,&#39;Ahuyama&#39;,&#39;Habichuela&#39;] df_sipsa_reducido= df_sipsa[df_sipsa[&quot;Mercado mayorista&quot;].isin(centrales_interes)==True] df_sipsa_reducido= df_sipsa_reducido[df_sipsa_reducido[&quot;Producto&quot;].isin(productos)==True] df_sipsa_reducido.drop(columns=[&quot;Tendencia&quot;,&quot;periodo&quot;,&quot;longitud&quot;],inplace=True) Ahora se cuenta con un set de datos con las siguientes columnas y extension: print(df_sipsa_reducido.shape) ## (4386, 6) df_sipsa_reducido.info ## &lt;bound method DataFrame.info of Mercado mayorista Precio mínimo ... Producto dia_inicio_semana ## 19 Armenia, Mercar 500 ... Ahuyama 2016-12-10 ## 37 Pereira, La 41 700 ... Ahuyama 2016-12-10 ## 38 Pereira, Mercasa 700 ... Ahuyama 2016-12-10 ## 297 Armenia, Mercar 1143 ... Cebolla junca 2016-12-10 ## 310 Pereira, La 41 1067 ... Cebolla junca 2016-12-10 ## ... ... ... ... ... ... ## 1654767 Pereira, La 41 2400 ... Habichuela 2022-01-29 ## 1654768 Pereira, Mercasa 2800 ... Habichuela 2022-01-29 ## 1655038 Armenia, Mercar 2727 ... Tomate chonto 2022-01-29 ## 1655055 Pereira, La 41 3000 ... Tomate chonto 2022-01-29 ## 1655056 Pereira, Mercasa 3000 ... Tomate chonto 2022-01-29 ## ## [4386 rows x 6 columns]&gt; El nuevo set de datos cuenta con 4386 registros y 6 columnas que corresponden exclusivamente a las centrales de abasto y productos de interés para el análisis. Como puede observarse ya los datos y las fechas estan filtradas y normalizadas no obstante hace falta que la fecha sea el indice del set de datos, esta condición es de suma importancia para poder llevar a cabo de manera más eficiente ejercicios posteriores, para cumplir esta condición a nivel de estructura de tabla, se crea una tabla de fechas que contiene los dias de inicio de semana que deberian contener las series de tiempo de los alimentos, esto se hace con la finalidad de tener un marco de fechas completo que permita determinar datos faltantes en el set de datos en revisión, para crear esta tabla de fechas se crea la función rango_fechas() que a partir de una fecha de inicio, una fecha de finalización y una determinada cantidad de años genera un listado de dias en formato yyyy-mm-dd from datetime import timedelta from datetime import date def rango_fechas(fecha_inicio,frecuencia,años): inicio= fecha_inicio periodos= int(round((365/frecuencia)*años,0)) lista= range(0,periodos+1) factor_suma= timedelta(days=frecuencia) lista_final=[] for i in lista: if i==0: fecha= inicio + factor_suma*0 lista_final.append(fecha) else: fecha= fecha + factor_suma lista_final.append(fecha) df_fechas_final= pd.DataFrame({&quot;Periodo&quot;:lista_final}) return df_fechas_final Se crea una variable de inicio con la primera fecha que muestra el set de datos, y en la función rango_fechas se introduce una frecuencia 7 (semanal) para un lapso de 7 años (2016-2022) start = datetime(2016,1,2) tabla_fechas_referencia= rango_fechas(start,7,7) tabla_fechas_referencia[&quot;Año&quot;]= tabla_fechas_referencia[&quot;Periodo&quot;].dt.year tabla_fechas_referencia[&quot;Año-mes&quot;]= tabla_fechas_referencia[&quot;Periodo&quot;].dt.to_period(&#39;M&#39;) tabla_fechas_referencia ## Periodo Año Año-mes ## 0 2016-01-02 2016 2016-01 ## 1 2016-01-09 2016 2016-01 ## 2 2016-01-16 2016 2016-01 ## 3 2016-01-23 2016 2016-01 ## 4 2016-01-30 2016 2016-01 ## .. ... ... ... ## 361 2022-12-03 2022 2022-12 ## 362 2022-12-10 2022 2022-12 ## 363 2022-12-17 2022 2022-12 ## 364 2022-12-24 2022 2022-12 ## 365 2022-12-31 2022 2022-12 ## ## [366 rows x 3 columns] Con esta operación realizada se procede a crear una función más,esta función es get_tidy_ts() la cual busca realizar una reindexación de la tabla origen a una estructura de tabla en donde el indice sea la fecha y las columnas sean los productos asociados a cada central de abastos: def get_tidy_ts(df,mercado,tabla_fechas): productos= df[df[&quot;Mercado mayorista&quot;]==mercado][&quot;Producto&quot;].unique() dataframe_list= [] for producto in productos: match_producto= df[(df[&quot;Producto&quot;]==producto)&amp;(df[&quot;Mercado mayorista&quot;]==mercado)]\\ [[&quot;Producto&quot;,&quot;dia_inicio_semana&quot;,&quot;Precio medio&quot;,&quot;Mercado mayorista&quot;]] match_producto[&quot;prod-merc&quot;]= match_producto[&quot;Producto&quot;]+&quot;_&quot;+match_producto[&quot;Mercado mayorista&quot;] match_producto.drop(columns={&quot;Mercado mayorista&quot;},inplace=True) calendar_merge= tabla_fechas.merge(right=match_producto,how=&quot;left&quot;,left_on=&quot;Periodo&quot;,right_on=&quot;dia_inicio_semana&quot;) calendar_merge.drop_duplicates(subset=[&quot;Periodo&quot;],keep=&#39;first&#39;,inplace=True) calendar_merge.drop(columns=[&quot;dia_inicio_semana&quot;,&quot;Año&quot;,&quot;Año-mes&quot;],inplace=True) calendar_merge= calendar_merge.pivot(index=&quot;Periodo&quot;,columns=&quot;prod-merc&quot;,values=&quot;Precio medio&quot;).drop(columns=np.nan) dataframe_list.append(calendar_merge) tidy_ts_df= pd.concat(dataframe_list, axis=1) return tidy_ts_df Antes de aplicar la función anterior se procede a simplificar los nombres de las centrales de abastos: simplificacion_centrales={&quot;Armenia, Mercar&quot;:&quot;axm_merc&quot;,&quot;Armenia, Retiro&quot;:&quot;axm_ret&quot;, &quot;Pereira, La 41&quot;:&quot;per_l41&quot;,&quot;Pereira, Mercasa&quot;:&quot;per_merca&quot;} df_sipsa_reducido[&quot;Mercado mayorista&quot;] = df_sipsa_reducido[&quot;Mercado mayorista&quot;].replace(simplificacion_centrales) Con la operación anterior realizada se procede a generar una dataset reformateado para los alimentos de interés en la ciudad de Armenia usando la función get_tidy_ts(): centrales_armenia= [&#39;axm_merc&#39;] lista_df_axm= [] for central in centrales_armenia: df_central2=get_tidy_ts(df_sipsa_reducido,central,tabla_fechas_referencia) lista_df_axm.append(df_central2) df_tseries_axm= pd.concat(lista_df_axm,axis=1) df_tseries_axm ## prod-merc Ahuyama_axm_merc ... Tomate chonto_axm_merc ## Periodo ... ## 2016-01-02 1300 ... 2383 ## 2016-01-09 1233 ... 2300 ## 2016-01-16 1233 ... 2600 ## 2016-01-23 1078 ... 1867 ## 2016-01-30 828 ... 1383 ## ... ... ... ... ## 2022-12-03 1267 ... 1970 ## 2022-12-10 1267 ... 2045 ## 2022-12-17 1267 ... 2197 ## 2022-12-24 1267 ... 2197 ## 2022-12-31 NaN ... NaN ## ## [366 rows x 4 columns] Ahora se cuenta con un set de datos con 366 registros y 4 columnas que corresponden a los precios de los productos de interesm, ahora repliquemos el mismo ejercicio para la ciudad de Pereira: centrales_pereira= [&#39;per_merca&#39;] lista_df_per= [] for central in centrales_pereira: df_central3=get_tidy_ts(df_sipsa_reducido,central,tabla_fechas_referencia) lista_df_per.append(df_central3) df_tseries_per= pd.concat(lista_df_per,axis=1) df_tseries_per ## prod-merc Ahuyama_per_merca ... Tomate chonto_per_merca ## Periodo ... ## 2016-01-02 1333 ... 2217 ## 2016-01-09 1167 ... 2844 ## 2016-01-16 1183 ... 3371 ## 2016-01-23 1117 ... 1900 ## 2016-01-30 967 ... 1738 ## ... ... ... ... ## 2022-12-03 1625 ... 3342 ## 2022-12-10 1542 ... 3450 ## 2022-12-17 1567 ... 3508 ## 2022-12-24 1633 ... 3333 ## 2022-12-31 NaN ... NaN ## ## [366 rows x 4 columns] Con los sets de datos en el formato adecuado, como ultimo ejercicio se procede a revisar la integridad de las series de tiempo en cada conjunto de datos haciendo uso del método isnull() de Python: df_tseries_axm.isnull().sum() ## prod-merc ## Ahuyama_axm_merc 11 ## Cebolla junca_axm_merc 12 ## Habichuela_axm_merc 11 ## Tomate chonto_axm_merc 12 ## dtype: int64 Se puede apreciar que para el caso de Armenia hay entre 11 y 12 registros faltantes por serie, lo cual significa que hay un 3% de valores faltantes por serie respecto al valor total de registros por producto. En el caso de Pereira se aprecia lo siguiente: df_tseries_per.isnull().sum() ## prod-merc ## Ahuyama_per_merca 11 ## Cebolla junca_per_merca 12 ## Habichuela_per_merca 11 ## Tomate chonto_per_merca 12 ## dtype: int64 Para el caso de Pereira tambien se cumple una condición de ausencia de entre 11 y 12 registros por serie de tiempo. Para solucionar el problema de valores faltantes y tener finalmente los sets de datos listo para la fase exploratoria se hace uso de la variable interpolate de pandas usando el método “time”, para poder aplicar esto de manera efectiva se realiza el casteo de las variables de precio con el fin de que pasen de formato string a formato númerico: df_tseries_axm= df_tseries_axm.apply(pd.to_numeric) ts_columns_axm= df_tseries_axm.columns for ts in ts_columns_axm: df_tseries_axm[ts]= df_tseries_axm[ts].interpolate(method=&quot;time&quot;) df_tseries_per= df_tseries_per.apply(pd.to_numeric) ts_columns_per= df_tseries_per.columns for ts in ts_columns_per: df_tseries_per[ts]= df_tseries_per[ts].interpolate(method=&quot;time&quot;) Finalmente al revisar nuevamente los valores vacios en las series se tiene lo siguiente: df_tseries_per.isnull().sum() ## prod-merc ## Ahuyama_per_merca 0 ## Cebolla junca_per_merca 0 ## Habichuela_per_merca 0 ## Tomate chonto_per_merca 0 ## dtype: int64 df_tseries_axm.isnull().sum() ## prod-merc ## Ahuyama_axm_merc 0 ## Cebolla junca_axm_merc 0 ## Habichuela_axm_merc 0 ## Tomate chonto_axm_merc 0 ## dtype: int64 Con la serie de operaciones realizadas ahora se cuenta con dos sets de datos en formato correcto para poder llevar a cabo procesos de análisis exploratorio que permitan determinar las propiedades de las series de los alimentos en la central mayorista más representativa de cada ciudad. "],["análisis-exploratorio-de-las-series-de-tiempo.html", "Capítulo 3 Análisis exploratorio de las series de tiempo 3.1 Análisis de series de tiempo de precio promedio mayorista semanal para la ciudad de Armenia 3.2 Análisis de series de tiempo de precio promedio mayorista semanal para la ciudad de Pereira", " Capítulo 3 Análisis exploratorio de las series de tiempo En esta sección se hará una exploración inicial de las series de tiempo para los productos en cada ciudad, se revisará la estructura de la serie, su estacionalidad, estacionariedad y su descomposición con el fin de determinar si requiere transformaciones o tratamiento adicional para la etapa de modelación. Para facilitar el proceso de definición la estacionalidad de las series se crean las siguiente funciones con el fin de formatear el resultado de los test Dickey Fuller Aumentado y KPSS: import statsmodels from statsmodels.tsa.stattools import adfuller from statsmodels.tsa.stattools import kpss from statsmodels.tsa.seasonal import seasonal_decompose from pandas.plotting import autocorrelation_plot from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.graphics import tsaplots def formated_adf(series): adf_result= adfuller(series,autolag=&quot;AIC&quot;) output_df= pd.DataFrame.from_dict({&quot;ADF Stat:&quot;:adf_result[0],&quot;p-value&quot;:adf_result[1],&quot;Crit_val1%:&quot;:adf_result[4][&quot;1%&quot;], &quot;Crit_val5%:&quot;:adf_result[4][&quot;5%&quot;],&quot;Crit_val10%&quot;:adf_result[4][&quot;10%&quot;],&quot;Interpretation&quot;:None},orient=&quot;index&quot;) output_df= output_df.rename(columns={0:&quot;Resultado&quot;}) adf_pval = adf_result[1] if adf_pval&gt;0.05: interp=&quot;La serie no es estacionaria&quot; else: interp=&quot;La serie es estacionaria&quot; output_df[&quot;Resultado&quot;][&quot;Interpretation&quot;]=interp return (output_df,adf_pval) def formated_kpss(series): kpss_result= kpss(series) output_df= pd.DataFrame.from_dict({&quot;KPSS Stat:&quot;: kpss_result[0],&quot;p-value&quot;: kpss_result[1],&quot;Lags&quot;: kpss_result[2], &quot;Crit_val10%:&quot;: kpss_result[3][&quot;10%&quot;],&quot;Crit_val5%&quot;: kpss_result[3][&quot;5%&quot;],&quot;Crit_val2.5%&quot;: kpss_result[3][&quot;2.5%&quot;], &quot;Crit_val1%&quot;: kpss_result[3][&quot;1%&quot;] ,&quot;Interpretation&quot;:None},orient=&quot;index&quot;) output_df= output_df.rename(columns={0:&quot;Resultado&quot;}) kpss_pval= kpss_result[1] if kpss_pval&lt;0.05: interp=&quot;La serie no es estacionaria&quot; else: interp=&quot;La serie es estacionaria&quot; output_df[&quot;Resultado&quot;][&quot;Interpretation&quot;]=interp return (output_df,kpss_pval) 3.1 Análisis de series de tiempo de precio promedio mayorista semanal para la ciudad de Armenia 3.1.1 Ahuyama Como primera medida se aisla la serie de tiempo y directamente se genera una primera diferencia, una segunda diferencia, y medias moviles de 5, 10 y 20 periodos para ver como se comporta en general la serie de tiempo y su tendencia: ahuyama_ts= series_armenia[[&quot;Ahuyama_axm_merc&quot;]] ahuyama_ts[&quot;SMA5&quot;]=ahuyama_ts[&quot;Ahuyama_axm_merc&quot;].rolling(5).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy ahuyama_ts[&quot;SMA10&quot;]=ahuyama_ts[&quot;Ahuyama_axm_merc&quot;].rolling(10).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy ahuyama_ts[&quot;SMA20&quot;]= ahuyama_ts[&quot;Ahuyama_axm_merc&quot;].rolling(20).mean() ahuyama_ts[&quot;diff1&quot;]= ahuyama_ts[&quot;Ahuyama_axm_merc&quot;].diff() ahuyama_ts[&quot;diff2&quot;]= ahuyama_ts[&quot;diff1&quot;].diff() import matplotlib.pyplot as plt plt.figure(figsize=(15,10)) plt.plot(ahuyama_ts.index,ahuyama_ts[[&quot;Ahuyama_axm_merc&quot;]],linewidth=1.1,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.plot(ahuyama_ts.index,ahuyama_ts[&quot;SMA5&quot;],linewidth=1,color=&quot;red&quot;,label=&quot;SMA5&quot;) plt.plot(ahuyama_ts.index,ahuyama_ts[&quot;SMA10&quot;],linewidth=1,color=&quot;green&quot;,label=&quot;SMA10&quot;) plt.plot(ahuyama_ts.index,ahuyama_ts[&quot;SMA20&quot;],linewidth=1,color=&quot;orange&quot;,label=&quot;SMA20&quot;) plt.title(&quot;Precios mayoristas promedio semanales de la ahuyama en la ciudad de Armenia&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() Al visualizar la serie de tiempo de los precios promedio semanales de la ahuyama se puede apreciar que no hay un patrón claro que demarque periodos o ciclos consistentes para el precio, al apreciar las medias moviles se puede apreciar que el suavizamiento de las mismas no termina generando evidencia clara de algún patrón estacional, lo único que se puede abstraer con claridad es que este producto ha tenido momentos de “pico de precio” alcanzando valores por encima de los 1200 pesos colombianos, este caso de alza de precio no ha sido exclusivo del periodo pos-pandemia sino que particularmente para este producto ya se habia presentado en el pasado. Ahora veamos un lag_plot con un rezago para esta serie de tiempo: plt.figure(figsize=(15,10)) pd.plotting.lag_plot(ahuyama_ts[[&quot;Ahuyama_axm_merc&quot;]],lag=1) plt.title(&quot;Lag plot serie de precios promedios mayoristas semanales Ahuyama&quot; ) plt.grid() plt.show() plt.close() A través del lag plot con un rezgo es posible apreciar que los datos de esta serie de tiempo exhiben un patrón lineal lo cual permite diagnosticar visualmente que la serie de tiempo en revisión tiene presencia de autocorrelación serial, seguidamente es claro que los datos no siguen un patrón aleatorio por lo tanto esta serie de tiempo no exhibe un patrón o dinámica aleatoria, finalmente teniendo en cuenta que cuando los datos presentan comportamiento periodico este tipo de gráfica toma comportamientos circulares o sinusuoidales, es posible inferir que efectivamente esta serie de tiempo no tiene estacionariedad clara. Demarcado lo anterior es conveniente revisar el comportamiento de la autocorrelación de la serie, para esto se genera un gráfico de función de autocorrelación (acf): plt.figure(figsize=(15,10)) tsaplots.plot_acf(ahuyama_ts[[&quot;Ahuyama_axm_merc&quot;]],title=&quot;Función de autocorrelación para serie de la Ahuyama&quot;) plt.grid() plt.show() plt.close(); La gráfica de la función de autocorrelación permite apreciar que hay correlación positiva entre las observaciones y el descenso hacia valores cercanos a 0 es lento al punto en el cual los primeros 15 rezagos resulta ser significativos, asi mismo es posible apreciar que el patrón de descenso es lineal por lo tanto no se aprecia alguna dinámica clara de estacionalidad, sin embargo el aspecto de la gráfica si lleva a inferir que la serie de tiempo no es estacionaria y por ende tiene presencia de tendencia dentro de su estructura. Para validar estos elementos que se han mencionado se procede a graficar la descomposición de la serie de tiempo con el fin de visualizar sus componentes: from statsmodels.tsa.seasonal import seasonal_decompose plt.figure(figsize=(15,10)) ahuy_decomp_axm= seasonal_decompose(ahuyama_ts[&quot;Ahuyama_axm_merc&quot;],model=&quot;multiplicative&quot;) ahuy_decomp_axm.plot() plt.show() plt.close(); La descomposición de la serie de tiempo permite evidenciar que efectivamente la serie cuenta con un componente de tendencia que se acentua especialmente al final, en lo que respecta a la estacionalidad se puede apreciar que los “periodos” de la serie son extensos y además fluactuantes, tal como lo evidencia el lag plot no hay un comportamiento marcado en términos sinusoidales, es evidente un comportamiento de disminución de los precios a inicio de año y de un comportamiento creciente a lo largo del transcurso del mismo. Ahora que se conoce más en detalle la estructura y comportamiento de la series, procedamos a verificar a partir de test estadísticos si esta serie es o no estacionaria, para esto se hará uso de las pruebas Dickey Fuller Aumentado y Kwiatkowski-Phillips-Schmidt-Shin (KPSS): formated_kpss(ahuyama_ts[&quot;Ahuyama_axm_merc&quot;]) ## ( Resultado ## KPSS Stat: 0.639845 ## p-value 0.019014 ## Lags 11.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie no es estacionaria, 0.019014071707958637) formated_adf(ahuyama_ts[&quot;Ahuyama_axm_merc&quot;]) ## ( Resultado ## ADF Stat: -2.197821 ## p-value 0.207037 ## Crit_val1%: -3.448646 ## Crit_val5%: -2.869602 ## Crit_val10% -2.571065 ## Interpretation La serie no es estacionaria, 0.2070372457079121) Al revisar ambos test estadísticos se puede apreciar que la serie de tiempo de la ahuyama no es estacionaria, por ende se puede decir que esta serie de tiempo tiene tendencia y por ende los cambios en el tiempo inciden o generan cambios en la forma de la serie, por lo tanto la tendencia que describe esta serie no es determinista y propiedades como su media y varianza son variables en el tiempo, asi las cosas, el nivel de integración de esta serie es mayor que 0, para poder determinar de manera precisa el nivel de integración es necesario diferenciar la serie de tiempo con el fin de validar si con una primera diferenciación logra convertirse en una serie estacionaria. plt.figure(figsize=(15,10)) plt.plot(ahuyama_ts[&quot;diff1&quot;],linewidth=1.2,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.title(&quot;Precios mayoristas promedio semanales de la ahuyama en la ciudad de Armenia (diff1)&quot;,fontsize=10) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() Ahora realicemos un lag plot respecto a la serie diferenciada con el fin de ver su comportamiento: plt.figure(figsize=(15,10)) pd.plotting.lag_plot(ahuyama_ts[&quot;diff1&quot;],lag=1) plt.title(&quot;Lag plot serie de precios promedios mayoristas semanales Ahuyama&quot; ) plt.show() plt.close() Con la aplicación de la primera diferencia se puede apreciar que el comportamiento lineal del lag plot de la serie original se pierde y varia hacia un patrón aleatorio en donde la nube de puntos no evidencia una dirección o trayectoria clara, esto da lugar a pensar que posiblemente el problema de autocorrelación tambien se encuentre corregido, para validar esto se genera nuevamente una gráfica acf() plt.figure(figsize=(15,10)) tsaplots.plot_acf(ahuyama_ts[[&quot;diff1&quot;]].dropna(),title=&quot;Función de autocorrelación para serie diferenciada de la Ahuyama&quot;) plt.show() plt.close(); La correlación entre las observaciones y sus rezagos tiende a 0 por lo tanto la serie diferenciada de la ahuyama carece de problemas de autocorrelación serial, por lo tanto el aumento de una observación anterior no afecta necesariamente las observaciones siguientes. El análisis gráfico da lugar a establecer que la primera diferencia de esta serie de tiempo es estacionaria, confirmemoslo aplicando las pruebas adf y KPSS: formated_kpss(ahuyama_ts[&quot;diff1&quot;].dropna()) ## ( Resultado ## KPSS Stat: 0.178315 ## p-value 0.1 ## Lags 1.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie es estacionaria, 0.1) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is greater than the p-value returned. formated_adf(ahuyama_ts[&quot;diff1&quot;].dropna()) ## ( Resultado ## ADF Stat: -9.0713 ## p-value 0.0 ## Crit_val1%: -3.448646 ## Crit_val5%: -2.869602 ## Crit_val10% -2.571065 ## Interpretation La serie es estacionaria, 4.294958554449737e-15) Al aplicar ambas pruebas la interpretación de los valores p conduce a concluir que la primera diferencia de la serie de tiempo de precios es estacionaria, por ende se puede concluir que la serie de tiempo para este producto tiene un nivel de integración 1. 3.1.2 Cebolla Junca Para el caso de la cebolla junca se sigue el mismo curso metodológico que en el caso de la Ahuyama, se aisla la serie, se calculan medias moviles, primera y segunda diferencia. cebollaj_ts= series_armenia[[&quot;Cebolla junca_axm_merc&quot;]] cebollaj_ts[&quot;SMA5&quot;]=cebollaj_ts[&quot;Cebolla junca_axm_merc&quot;].rolling(5).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy cebollaj_ts[&quot;SMA10&quot;]=cebollaj_ts[&quot;Cebolla junca_axm_merc&quot;].rolling(10).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy cebollaj_ts[&quot;SMA20&quot;]= cebollaj_ts[&quot;Cebolla junca_axm_merc&quot;].rolling(20).mean() cebollaj_ts[&quot;diff1&quot;]= cebollaj_ts[&quot;Cebolla junca_axm_merc&quot;].diff() cebollaj_ts[&quot;diff2&quot;]= cebollaj_ts[&quot;diff1&quot;].diff() Procedamos a visualizar la serie de tiempo y sus tres medias moviles: plt.plot(cebollaj_ts.index,cebollaj_ts[[&quot;Cebolla junca_axm_merc&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.plot(cebollaj_ts.index,cebollaj_ts[&quot;SMA5&quot;],linewidth=1,color=&quot;red&quot;,label=&quot;SMA5&quot;) plt.plot(cebollaj_ts.index,cebollaj_ts[&quot;SMA10&quot;],linewidth=1,color=&quot;green&quot;,label=&quot;SMA10&quot;) plt.plot(cebollaj_ts.index,cebollaj_ts[&quot;SMA20&quot;],linewidth=1,color=&quot;orange&quot;,label=&quot;SMA20&quot;) plt.title(&quot;Precios mayoristas promedio semanales de la cebolla junca en la ciudad de Armenia&quot;,fontsize=11) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() Al revisar la gráfica de la serie de tiempo de precios promedios semanales se puede apreciar que la cebolla junca presenta un patrón mucho más volatil que el de la Ahuyama, y además se puede observar con claridad procesos de ascenso y descenso que dan cuenta que un patrón estacional pero que no es totalmente regular, sin embargo en medio de su dinámica se puede apreciar una dinámica con dos picos de precio al año, uno al inicio y otro al final, este patrón preserva mayor regularidad hasta 2020, desde 2021 hasta adelante se puede apreciar un cambio en la dinámica de precio en donde el mismo mantiene máximos cercanos a 2500 y llega continuamente a minimos cercanos a los 1700 pesos. Si se toma en cuenta todo el movimiento histórico del precio se puede evidenciar un aumento generalizado, a 2017 el precio alcanzó minimos cercanos a los 600 pesos y para el 2022 el precio nunca descendio por debajo de los 1500 pesos. Ahora veamos un lag_plot con un rezago para esta serie de tiempo: pd.plotting.lag_plot(cebollaj_ts[[&quot;Cebolla junca_axm_merc&quot;]],lag=3) plt.show() plt.close() El lag plot considerando tres rezagos describe una tendencia lineal positiva pero dispersa, dentro de la nube de puntos que se presenta no hay patrones claramente sinusoidales por lo tanto no logra evidenciarse con claridad algun patron de estacionalidad claramente definido, por otro lado no se encuentran en la gráfica puntos altamente desviados de la trayectoria, no obstante la linealidad indica presencia de autocorrelación lo cual genera indicios de la existencia de tendencia dentro de la serie lo cual demarcaria que esta no es estacionaria, con el fin de esclarecer las propiedades de la serie se continua ahondando en sus propiedades. Como parte de la exploración de propiedades de la serie se genera una gráfica de la función de autocorrelación de la serie de tiempo: plt.figure(figsize=(15,10)) tsaplots.plot_acf(cebollaj_ts[[&quot;Cebolla junca_axm_merc&quot;]],title=&quot;Función de autocorrelación para serie de la cebolla junca&quot;) plt.show() plt.close(); El patrón descendente progreviso y fuera de los intervalos de confianza de la gráfica de autocorrelación permiten evidenciar que hay presencia de autocorrelación serial en la serie, esto implica que observaciones anteriores están presentando influencia en las observaciones siguientes. En lo que respecta a estacionalidad no se logra evidenciar un patrón lo suficientemente claro como para confirmarla, con el fin de seguir conociendo las propiedades de la serie se procede a revisar su descomposición: plt.figure(figsize=(15,10)) ceboll_decomp_axm= seasonal_decompose(cebollaj_ts[[&quot;Cebolla junca_axm_merc&quot;]],model=&quot;multiplicative&quot;) ceboll_decomp_axm.plot() plt.show() plt.close(); La descomposición permite evidenciar que la serie en términos de tendencia tiene un comportamiento creciente aunque con comportamiento variable, por ejemplo entre 2019 y 2020 el comportamiento tendencial entra en un periodo de estabilidad hasta entrar en un periodo de descenso hacia finales de 2020 y que persiste en 2021y 2022, en lo que respecta a estacionalidad se puede confirmar lo mencionado al inicio de la visualización de la serie y es que hay dos momentos en el comportamiento de los precios a lo largo del tiempo tales momentos corresponden a ascensos puntuales del precios que se presentan a inicios de año y un poco despues de la mitad del año. Los elementos revisados al momento darian pie para decir que estamos ante una serie no estacionaria, con autocorrelación y una una estacionalidad existente pero irregular. En aras de seguir validando las propiedades de la serie se aplican los test adf y kpss para validar si la serie es o no estacionaria: formated_kpss(cebollaj_ts[&quot;Cebolla junca_axm_merc&quot;]) ## ( Resultado ## KPSS Stat: 0.655618 ## p-value 0.01758 ## Lags 11.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie no es estacionaria, 0.017580164973978154) formated_adf(cebollaj_ts[&quot;Cebolla junca_axm_merc&quot;]) ## ( Resultado ## ADF Stat: -3.387717 ## p-value 0.011385 ## Crit_val1%: -3.449119 ## Crit_val5%: -2.86981 ## Crit_val10% -2.571176 ## Interpretation La serie es estacionaria, 0.011384809184102474) Al revisar los resultados de los test estadísticos se puede apreciar que ambos conducen a conclusiones distintas, por lo tanto la revisión no es concluyente, sin embargo se le da prelación al hecho de que la series no es estacionaria, por lo tanto se procede a probar la primera diferencia de la serie con el fin de validar si al aplicar esta transformación el resultado de ambas pruebas resulta concluyente. Veamos gráficamente la primera diferencia de la serie: plt.figure(figsize=(15,10)) plt.plot(cebollaj_ts[&quot;diff1&quot;],linewidth=1.2,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.title(&quot;Precios mayoristas promedio semanales de la cebolla junca en la ciudad de Armenia (diff1)&quot;,fontsize=10) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() Al revisar la primera diferencia de la serie es posible apreciar que el componente tendencial ha desaparecido y ahora se tiene una serie de tiempo que se asemeja más a una serie de tiempo estacionaria, se aprecia claramente que no hay un componente tendencial y además tambien es posible ver que en cierta forma se suaviza la dinámica estaciona, no obstante se puede apreciar claramente que hay algunos cambios considerablemente bruscos a mediados de 2016, finales de 2017, y a inicios de 2021. Ahora se procede a revisar el lag plot de la primera diferencia: pd.plotting.lag_plot(cebollaj_ts[[&quot;diff1&quot;]].dropna(),lag=3) plt.show() plt.close() Se puede apreciar que ahora el lag plot no describe una trayectoria lineal sino que demarca un patrón mucho más aleatorio que distribuye los puntos en diferentes secciones del plano, en este orden de ideas la forma de la gráfica indica que se ha superado al autocorrelación por medio de la primera dferencia. Ahora veamos la función a autocorrelación: plt.figure(figsize=(15,10)) tsaplots.plot_acf(cebollaj_ts[[&quot;diff1&quot;]].dropna(),title=&quot;Función de autocorrelación para serie diferenciada de la cebolla junca&quot;) plt.show() plt.close(); Al observar la gráfica acf se puede apreciar que el problema de autocorrelación ha logrado moderarse, solo se presentan algunas observaciones que superan los intervalos de confianza, no obstante estas observaciones ya no siguen un patron lineal decreciente como si ocurria con anterioridad. Ahora veamos el resultado de los test adf y kpss sobre la primera diferencia: formated_kpss(cebollaj_ts[&quot;diff1&quot;].dropna()) ## ( Resultado ## KPSS Stat: 0.021394 ## p-value 0.1 ## Lags 7.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie es estacionaria, 0.1) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is greater than the p-value returned. formated_adf(cebollaj_ts[&quot;diff1&quot;].dropna()) ## ( Resultado ## ADF Stat: -7.198846 ## p-value 0.0 ## Crit_val1%: -3.449337 ## Crit_val5%: -2.869906 ## Crit_val10% -2.571227 ## Interpretation La serie es estacionaria, 2.394794788439695e-10) Se puede apreciar que una vez aplicada la primera diferencia sobre la serie es posible concluir que esta primera diferencia resulta ser estacionaria y se superan los resultados disimiles en los test estadisticos. Con esta revisión es posible concluir que la serie de tiempo de la cebolla junca para la ciudad de Armenia es una serie de tiempo con nivel de Integración 1 es decir I(1), por lo tanto para que esta serie llegue a ser estacionaria requiere que sea calculada su primera diferencia. 3.1.3 Habichuela Para el caso de la habichuela se continua el mismo ciclo metodológico calculando previamente medidas moviles y primera y segunda diferencia. habichuela_ts= series_armenia[[&quot;Habichuela_axm_merc&quot;]] habichuela_ts[&quot;SMA5&quot;]=habichuela_ts[&quot;Habichuela_axm_merc&quot;].rolling(5).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy habichuela_ts[&quot;SMA10&quot;]=habichuela_ts[&quot;Habichuela_axm_merc&quot;].rolling(10).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy habichuela_ts[&quot;SMA20&quot;]= habichuela_ts[&quot;Habichuela_axm_merc&quot;].rolling(20).mean() habichuela_ts[&quot;diff1&quot;]= habichuela_ts[&quot;Habichuela_axm_merc&quot;].diff() habichuela_ts[&quot;diff2&quot;]= habichuela_ts[&quot;diff1&quot;].diff() Inicialmente se procede a visualizar la serie de tiempo y las medias moviles calculadas. plt.plot(habichuela_ts.index,habichuela_ts[[&quot;Habichuela_axm_merc&quot;]],linewidth=1.1,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.plot(habichuela_ts.index,habichuela_ts[&quot;SMA5&quot;],linewidth=1,color=&quot;red&quot;,label=&quot;SMA5&quot;) plt.plot(habichuela_ts.index,habichuela_ts[&quot;SMA10&quot;],linewidth=1,color=&quot;green&quot;,label=&quot;SMA10&quot;) plt.plot(habichuela_ts.index,habichuela_ts[&quot;SMA20&quot;],linewidth=1,color=&quot;orange&quot;,label=&quot;SMA20&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() Al observar la serie de timepo de la habichuela se puede apreciar un comportamiento que demarca cierta volatilidad de precio durante el año presentando inclusive dos o inclusive tres picos de precio anuales, durante el periodo 2016-2021 la dinámica de precio conserva una estabilidad de minimos y máximos entre 900 y 3900 pesos mostrando dinámicas de pico de precios despues de la mitad del año. En el año 2022 del precio mayorista promedio alcanza máximos históricos superando la barrera de los 5000 pesos en el primer semestre de 2022 y en el segundo semestre tomando un comportamiento a la baja que no vuelve a los niveles de minímo histórico sino que se queda en un rango entre 2500 y 3500. El comportamiento de la serie permite apreciar que posiblemente sin el alza vertiginosa de 2022 estariamos ante una serie practicamente estacionaria, sin embargo este cambio en los niveles es una realidad para esta serie de tiempo y supone de hecho un cambio relevante en la varianza de la misma. Conocido el comportamiento anterior de la serie se procede a visualizar un lag plot de la misma para identificar elementos como la autocorrelación, outlies y estacionalidad: pd.plotting.lag_plot(habichuela_ts[&quot;Habichuela_axm_merc&quot;],lag=3) plt.show() plt.close() El lag plot de la serie de la habichuela describe una trayectoria lineal dispersa que se acompaña por observaciones dispersas, este comportamiento permite demarcar que muy probablemente hay presencia de autocorrelación moderada y además hay una presencia de outliers o datos atipicos que se ven representados por las observaciones que se encuentran alejadas de la nube de puntos principal. Ahora procedamos a revisar la gráfica de la función de autocorrelación con el fin de validar la presencia de autocorrelación serial: plt.figure(figsize=(15,10)) tsaplots.plot_acf(habichuela_ts[&quot;Habichuela_axm_merc&quot;],title=&quot;Función de autocorrelación para serie de la habichuela&quot;) plt.show() plt.close(); Al revisar la gráfica acf se puede apreciar que hay un patrón lineal decreciente con observaciones que estan fuera de los intervalos de confianza lo cual demarca que a lo largo del tiempo las observaciones no se comportan de manera independiente, esta dinámica es persistente y se acompaña por un leve comportamiento estacional a partir de la observación 10 en donde se puede apreciar un leve comportamiento periodico en la gráfica. El análisis gráfico da pie para decir que la serie presenta autocorrelación serial, no parece ser estacionaria y además presenta indicios de presentar estacionalidad. Conocido lo anterior se procede a revisar la descomposición de la serie de tiempo con el fin de apreciar el comportamiento de sus componentes: plt.figure(figsize=(15,10)) habichue_decomp_axm= seasonal_decompose(habichuela_ts[&quot;Habichuela_axm_merc&quot;],model=&quot;multiplicative&quot;) habichue_decomp_axm.plot() plt.show() plt.close(); Al revisar la descomposición de la serie se logra validar la tendencia creciente de precios en el tiempo, y además a nivel estacional se puede confirmar la dinámica de dos picos en donde el más pronunciado como se puede apreciar se encuentra a despues del primer semestre del año y se acompaña por otro pico a inicio de año que se muestra menor al del segundo semestre. Ahora se procede a aplicar las pruebas adf y kpss con el fin de validar si la serie es estacionaria o no: formated_kpss(habichuela_ts[&quot;Habichuela_axm_merc&quot;]) ## ( Resultado ## KPSS Stat: 1.808661 ## p-value 0.01 ## Lags 10.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie no es estacionaria, 0.01) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is smaller than the p-value returned. formated_adf(habichuela_ts[&quot;Habichuela_axm_merc&quot;]) ## ( Resultado ## ADF Stat: -2.786733 ## p-value 0.060194 ## Crit_val1%: -3.448906 ## Crit_val5%: -2.869716 ## Crit_val10% -2.571126 ## Interpretation La serie no es estacionaria, 0.060193942433346984) Al revisar los resultados e interpretaciones de los p-valores de las pruebas se puede apreciar que las serie no resulta ser estacionaria, por lo tanto se hace necesario explorar la primera diferencia de la misma con el fin de validar si aplicando esta diferenciación se logra una serie estacionaria: plt.figure(figsize=(15,10)) plt.plot(habichuela_ts[&quot;diff1&quot;],linewidth=1.2,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.title(&quot;Precios mayoristas promedio semanales de la habichuela en la ciudad de Armenia (diff1)&quot;,fontsize=10) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() Al aplicar la primera diferencia se puede apreciar claramente la magnitud de las variaciones a lo largo del tiempo, en este caso es posible ver los picos de precio que se comentaban al inicio del análisis de la serie de tiempo, la dinámica de cambio de precio más brusco se presenta en el primer semestre de 2022 presentando variaciones cercanas a los 2000 pesos (tanto al alza como a la baja) Ahora veamos el lag plot de la primera diferencia: pd.plotting.lag_plot(habichuela_ts[[&quot;diff1&quot;]].dropna(),lag=3) plt.show() plt.close() Al observar el lag plot se puede apreciar que la distribución de los puntos pierde su tendencia lineal lo cual indica una posible correción de la autocorrelación, a su vez tambien es posible apreciar que la dinámica de valores atipicos persiste como se analizaba con anterioridad. Ahora procedamos a revisar la gráfica de la función de autocorrelación con el fin de validar si efectivamente se ha corregido la autocorrelación serial: plt.figure(figsize=(15,10)) tsaplots.plot_acf(habichuela_ts[[&quot;diff1&quot;]].dropna(),title=&quot;Función de autocorrelación para serie diferenciada de la habichuela&quot;) plt.show() plt.close(); La aplicación de la primera diferencia permite evidenciar que la primera diferencia ha suavizado el problema de autocorrelación, no osbtante se puede apreciar que persiste ya que aun hay varias observaciones que sobrepasan los intervalos de confianza, sin embargo a pesar de que persiste la autocorrelación, es posible evidenciar que el patrón lineal descendente al inicio de la gráfica se ha corregido, por ende es probable que se este ante una serie estacionaria pero con presencia de autocorrelación serial, procedamos a validar la estacionariedad a partir de los test adf y kpss sobre la serie diferenciada. formated_kpss(habichuela_ts[&quot;diff1&quot;].dropna()) ## ( Resultado ## KPSS Stat: 0.054074 ## p-value 0.1 ## Lags 33.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie es estacionaria, 0.1) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is greater than the p-value returned. formated_adf(habichuela_ts[&quot;diff1&quot;].dropna()) ## ( Resultado ## ADF Stat: -7.530991 ## p-value 0.0 ## Crit_val1%: -3.449227 ## Crit_val5%: -2.869857 ## Crit_val10% -2.571201 ## Interpretation La serie es estacionaria, 3.579056467995607e-11) Al revisar los p-valores de las pruebas adf y kpss se puede apreciar que despues de aplicada una primera diferencia se logra que la serie de tiempo de la habichuela para la ciudad de Armenia sea estacionaria, esto llevar a concluir que esta serie tiene un orden de integración I(1) no obstant es preciso resaltar que es una serie que aun conserva algunos atributos como la autocorrelación serial y posiblemente tambien problemas de heterocedasticidad ligados a los cambios bruscos en la varianza que han podido evidenciarse al aplicar la primera diferencia. 3.1.4 Tomate Chonto Para el caso del tomate chonto se continua con el proceso de análisis calculando medias moviles, primera y segunda diferencia con el fin de ser visualizadas. tomate_ts= series_armenia[[&quot;Tomate chonto_axm_merc&quot;]] tomate_ts[&quot;SMA5&quot;]=tomate_ts[&quot;Tomate chonto_axm_merc&quot;].rolling(5).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy tomate_ts[&quot;SMA10&quot;]=tomate_ts[&quot;Tomate chonto_axm_merc&quot;].rolling(10).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy tomate_ts[&quot;SMA20&quot;]= tomate_ts[&quot;Tomate chonto_axm_merc&quot;].rolling(20).mean() tomate_ts[&quot;diff1&quot;]= tomate_ts[&quot;Tomate chonto_axm_merc&quot;].diff() tomate_ts[&quot;diff2&quot;]= tomate_ts[&quot;diff1&quot;].diff() Inicialmente se visualiza la serie de tiempo del tomate chonto con sus respectivas medias moviles de 5, 10y 20 periodos: plt.plot(tomate_ts.index,tomate_ts[[&quot;Tomate chonto_axm_merc&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.plot(tomate_ts.index,tomate_ts[&quot;SMA5&quot;],linewidth=1,color=&quot;red&quot;,label=&quot;SMA5&quot;) plt.plot(tomate_ts.index,tomate_ts[&quot;SMA10&quot;],linewidth=1,color=&quot;green&quot;,label=&quot;SMA10&quot;) plt.plot(tomate_ts.index,tomate_ts[&quot;SMA20&quot;],linewidth=1,color=&quot;orange&quot;,label=&quot;SMA20&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() Al revisar la serie de tiempo se puede apreciar una dinámica precio que hasta el añ0 2021 oscilo entre los 800 (como minimo) y 3000 pesos como máximo, esta dinámica del precio mayorista cambia de manera abrupta a inicios de 2022 en donde supera la barrera de los 3000 y cerca a la mitad del año alcanza precios por encima de 5000 pesos. En términos de estacionalidad se puede apreciar que el precio tiene dos momentos de picos anuales que están precedidos por disminuciones de precio, los picos se generan mayormente en el primer semestre el año, en las ultimas semanas del año el precio sufren una estabilización a la baja. Si se observa la transición de los precios minimos es posible observar que desde 2029 en adelante los precios nunca volvieron a estar por debajo de los 1000 pesos y hacia finales de 2021 alcanza un minimo de 1200 pesos el cual se mantiene durante 2022 en donde inclusive tal minimo aumenta a 1500. Partiendo de las observaciones anteriores es claro que hay una dinámica de tendencia creciente en los precios teniendo en cuenta el histórico disponible. Ahora veamos un lag plot de esta serie de tiempo: pd.plotting.lag_plot(tomate_ts[&quot;Tomate chonto_axm_merc&quot;],lag=3) plt.show() plt.close() En términos de patrones en el lag plot no se conforma totalmente un comportamiento lineal claro, se puede apreciar que los precios en su mayoria están concentrados sin embargo si hay evidencia suficiente de outliers que se aprecian muy claramente en el gráfico en la parte derecha del plano. Con base a lo mencionado dado que la relación lineal no es fuerte podria encontrarse en esta serie un caso de autocorrelación serial moderada. plt.figure(figsize=(15,10)) tsaplots.plot_acf(tomate_ts[&quot;Tomate chonto_axm_merc&quot;],title=&quot;Función de autocorrelación para serie diferenciada del tomate&quot;) plt.show() plt.close(); Al apreciar la gráfica de la función de autocorrelación se aprecia un descenso lineal de varias observaciones que están fuera del intervalo de confianza lo cual representa un problema claro de autocorrelación serial, para este caso no se evidencia o no hay indicios de un componente estacional claro, pero el descenso lineal si pertmite demarcar una serie no estacionaria. plt.figure(figsize=(15,10)) tomate_decomp_axm= seasonal_decompose(tomate_ts[&quot;Tomate chonto_axm_merc&quot;],model=&quot;multiplicative&quot;) tomate_decomp_axm.plot() plt.show() plt.close(); La descomposiición de la serie de tiempo permite evidenciar lo mencionado al principio de análisis de la serie, se puede confirmar claramente la tendencia creciente de los precios, y en términos de estacionalidad puede apreciarse la dinámica de los dos picos durante el año, al observar el patrón y su transición es posible apreciar que el periodo termina siendo practicamente anual. Ahora veamos por medio de los test estadisticos si efectivamente esta serie es estacionaria: formated_kpss(tomate_ts[&quot;Tomate chonto_axm_merc&quot;]) ## ( Resultado ## KPSS Stat: 1.008584 ## p-value 0.01 ## Lags 10.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie no es estacionaria, 0.01) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is smaller than the p-value returned. formated_adf(tomate_ts[&quot;Tomate chonto_axm_merc&quot;]) ## ( Resultado ## ADF Stat: -4.11231 ## p-value 0.000924 ## Crit_val1%: -3.448544 ## Crit_val5%: -2.869557 ## Crit_val10% -2.571041 ## Interpretation La serie es estacionaria, 0.0009241291591843274) Al revisar la interpretación de los p-valores de los test es posible identificar que los resultados no son consistentes, en test adf reporta que la serie es estacionaria, y el test kpss reporta que la serie no es estacionaria, dadas estas condiciones es conveniente revisar el comportamiento de la primera diferencia y ver como se comporta la misma al aplicar las pruebas. Veamos el comportamiento gráfico de la primera diferencia: plt.plot(tomate_ts.index,tomate_ts[[&quot;diff1&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() Es posible apreciar que la serie diferenciada muestra un comportamiento más estable y menos tendencial, sin embargo es claramente visible el alza vertiginosa de precio al inicio del año 2022 lo cual representa una distorsión considerable de la varianza pero que resulta corta y no se sostiene en el tiempo, en este orden de ideas se aprecia que la primera diferencia corrige la tendencia pero igualmente queda el impacto de los outliers asociados al comportamiento del precio en este caso del tomate chonto. Ahora procedamos a revisar el lagplot de la serie: pd.plotting.lag_plot(tomate_ts[[&quot;diff1&quot;]].dropna(),lag=3) plt.show() plt.close() En este caso ya no se describe una tendencia lineal sino que se muestra una distribución mucho más variada de los puntos, sin embargo las observaciones aisladas persisten. se puede observar que la autocorrelación logra ser superada, lo cual puede validar con la gráfica de la función de autocorrelación: plt.figure(figsize=(15,10)) tsaplots.plot_acf(tomate_ts[[&quot;diff1&quot;]].dropna(),title=&quot;Función de autocorrelación para serie diferenciada del tomate&quot;) plt.show() plt.close(); Se puede apreciar que luego de aplicada la primera diferencia el problema pronunciado de autocorrelación serial evidenciado en la serie sin diferenciar resulta ser corregido dando lugar a que la mayoria de observaciones se encuentren dentro de los intervalos de confianza. Ahora validemos si la serie diferenciada es estacionaria aplicando los test adf y kpss: print(formated_kpss(tomate_ts[&quot;diff1&quot;].dropna())) ## ( Resultado ## KPSS Stat: 0.04944 ## p-value 0.1 ## Lags 19.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie es estacionaria, 0.1) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is greater than the p-value returned. print(formated_adf(tomate_ts[&quot;diff1&quot;].dropna())) ## ( Resultado ## ADF Stat: -13.90463 ## p-value 0.0 ## Crit_val1%: -3.448544 ## Crit_val5%: -2.869557 ## Crit_val10% -2.571041 ## Interpretation La serie es estacionaria, 5.652345805531328e-26) Al revisar los p-valores de los test se puede concluir que la primera diferencia de la serie del tomate chonto resulta ser estacionaria, asi las cosas es posible definir esta serie como I(1) es decir tiene un nivel de integración que implica estacionariedad si y solo si la serie es diferenciada una vez. 3.2 Análisis de series de tiempo de precio promedio mayorista semanal para la ciudad de Pereira Para el análisis de las series de tiempo de Pereira se aplicará el mismo proceso realizado con las series de tiemp de Armenia, inicialmente se visualizará la serie de tiempo y sus medias moviles, posteriormente se validarán visualmente algunas propiedades por medio de lag-plot, gráfica de función de autocorrelación y descomposición de la serie. Adicional a lo antes mencionado se aplican los test KPSS y ADF con el fin de validar si las series revisada es o no estacionaria, y en caso de no serlo se hace una revisión de la primera diferencia con el fin de validar si a partir de la misma se corrige el carácter no-estacionario de la serie, todo este proceso tiene como finalidad conocer propiedades generales de la serie y a su vez determinar su nivel de integración, lo cual será util para procesos de modelación posteriores. 3.2.1 Ahuyama Inicialmente se calculan las medias moviles de 5,10 y 20 periodos, y la primera y segunda diferencia de la serie. ahuyama_ts_per= series_pereira[[&quot;Ahuyama_per_merca&quot;]] ahuyama_ts_per[&quot;SMA5&quot;]=ahuyama_ts_per[&quot;Ahuyama_per_merca&quot;].rolling(5).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy ahuyama_ts_per[&quot;SMA10&quot;]=ahuyama_ts_per[&quot;Ahuyama_per_merca&quot;].rolling(10).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy ahuyama_ts_per[&quot;SMA20&quot;]= ahuyama_ts_per[&quot;Ahuyama_per_merca&quot;].rolling(20).mean() ahuyama_ts_per[&quot;diff1&quot;]= ahuyama_ts_per[&quot;Ahuyama_per_merca&quot;].diff() ahuyama_ts_per[&quot;diff2&quot;]= ahuyama_ts_per[&quot;diff1&quot;].diff() A partir de lo anterior se procede a visualizar las serie de tiempo de la Ahuyama para la ciudad de Pereira: plt.plot(ahuyama_ts_per.index,ahuyama_ts_per[[&quot;Ahuyama_per_merca&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.plot(ahuyama_ts_per.index,ahuyama_ts_per[&quot;SMA5&quot;],linewidth=1,color=&quot;red&quot;,label=&quot;SMA5&quot;) plt.plot(ahuyama_ts_per.index,ahuyama_ts_per[&quot;SMA10&quot;],linewidth=1,color=&quot;green&quot;,label=&quot;SMA10&quot;) plt.plot(ahuyama_ts_per.index,ahuyama_ts_per[&quot;SMA20&quot;],linewidth=1,color=&quot;orange&quot;,label=&quot;SMA20&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() Al observar la serie de tiempo de la ahuyama para la ciudad de Pereira se puede apreciar que esta serie particularmente tiene una patrón de movimiento mucho más claro respecto a la serie que se evidencia para la ciudad de Armenia, se puede apreciar una dinámica de picos de precio y descensos, para el caso de Pereira los picos de precio se ubican hacia la mitad del año y en otros casos hacia final de año como ocurre en 2019, el periodo más estable a nivel de precio fue entre 2018 y mediados de 2019 durante este periodo el precio se mantuvo entre más o menos 900 pesos y 1100 pesos. En términos generales de la cierrre se aprecia que el precio minimo alcanzado fue de cerca de 500 pesos y el máximo se alcanza a finales de 2023 en donde el precio llega por encima de los 1600. La observación general de la gráfica permite identificar una tendencia creciente que lleva a determinar a nivel de observación que esta serie tiene tendencia creciente. Con el fin de seguir validando las propiedades de la serie se procede a revisar el lag-plot de la misma: pd.plotting.lag_plot(ahuyama_ts_per[&quot;Ahuyama_per_merca&quot;],lag=3) plt.show() plt.close() Al observar el lag-plot se puede evidenciar una tendencia lineal positiva con relativamente baja dispersión, se presentan puntos ligeramente separados y además no hay evidencias de un patrón sinusoidal o circular que de cuenta de un componente estacional claro. El lag plot observado permite establecer que la serie de tiempo revisada cuenta con autocorrelación serial y que además no corresponde a un patrón de datos aleatorio, seguidamente la forma de la gráfica permite demarcar que la dinámica de la serie no se ajusta o relaciona a valores sinusoidales. con el fin de seguir validando las propiedades de la serie, a continuación se visualiza la función de autocorrelación de la serie: plt.figure(figsize=(15,10)) tsaplots.plot_acf(ahuyama_ts_per[&quot;Ahuyama_per_merca&quot;],title=&quot;Función de autocorrelación para serie de la Ahuyama (Pereira)&quot;) plt.show() plt.close(); La función de autocorrelación permite evidenciar de manera clara la autocorrelación mencionada con anterioridad, se pueden apreciar 15 observaciones que sobrepasan los intervalos de confianza, seguidamente es posible apreciar que el descenso de las observaciones es lineal lo cual permite tambien establecer visualmente que hay un componente tendencial ligado a la autocorrelación serial. Ahora veamos la descomposición de la serie de tiempo: plt.figure(figsize=(15,10)) ahuy_decomp_per= seasonal_decompose(ahuyama_ts_per[&quot;Ahuyama_per_merca&quot;],model=&quot;multiplicative&quot;) ahuy_decomp_per.plot() plt.show() plt.close(); Al observar la descomposición de la serie se puede apreciar el componente tendencial creciente que se mencionaba en la primera revisión de la serie, y adicionalmente se puede apreciar un componente estacional que no es totalmente regular que se caracteriza por presentar dos picos a lo largo del tiempo. Los elementos mostrados en la descomposición especialmente relacionados a aspectos de tendencia permitan apreciar que la serie no parece ser estacionaria ,con el fin de confirmarlo se efectuan las pruebas adf y kpss: formated_kpss(ahuyama_ts_per[&quot;Ahuyama_per_merca&quot;]) ## ( Resultado ## KPSS Stat: 1.609521 ## p-value 0.01 ## Lags 11.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie no es estacionaria, 0.01) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is smaller than the p-value returned. formated_adf(ahuyama_ts_per[&quot;Ahuyama_per_merca&quot;]) ## ( Resultado ## ADF Stat: -2.471486 ## p-value 0.122567 ## Crit_val1%: -3.448646 ## Crit_val5%: -2.869602 ## Crit_val10% -2.571065 ## Interpretation La serie no es estacionaria, 0.12256716026283593) Al revisar los p-valores de los test se puede apreciar con claridad que ambos coinciden en establecer que la serie de precios semanales promedio de la Ahuyama en Pereira no describen una serie de tiempo estacionaria, dado esto se hace necesario calcular la primera diferencia con el fin de validar si por medio de la misma es posible convertir la serie en estacionaria. plt.figure(figsize=(15,10)) plt.plot(ahuyama_ts_per[&quot;diff1&quot;],linewidth=1.2,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.title(&quot;Precios mayoristas promedio semanales de la ahuyama en la ciudad de Pereira (diff1)&quot;,fontsize=10) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() Al obtener una primera diferencia se pueden apreciar con claridad los cambios bruscos que sufre la serie a lo largo del tiempo, por ejemplo en el periodo 2018-2019 hay bastante estabilidad en la varianza del precio, esto cambia de forma considerable hacia adelante en donde la volatilidad del mismo incrementa de forma considerable. Más allá de las variaciones es posible apreciar que la diferenciación de la serie permite corregir el componente tendencial de la serie lo cual hace que se asemeje más a una serie de tipo estacionaria. pd.plotting.lag_plot(ahuyama_ts_per[&quot;diff1&quot;].dropna(),lag=3) plt.show() plt.close() Al observar el lag plot se puede apreciar una nube de puntos que en este caso no describe un patrón lineal claro lo cual indica que ahora estamos ante una serie con un comportamiento mucho más aleatorio y con una menor presencia de autoccorelación serial respecto a la serie inicial. Ahora veamos el aspecto de la función de autocorrelación: plt.figure(figsize=(15,10)) tsaplots.plot_acf(ahuyama_ts_per[&quot;diff1&quot;].dropna(),title=&quot;Función de autocorrelación para serie de la Ahuyama (Pereira) diff1&quot;) plt.show() plt.close(); Se puede apreciar que aun hay observaciones que sobrepasan los intervalos de confianza lo cual indica que aun hay presencia de autocorrelación, no obstante esta autocorrelación se perfila como debil teniendo en cuenta que se presenta solo en 3 observaciones y no resulta ser continua o secuencial como en el caso de la serie sin diferenciar. Con el fin de validar si la primer diferencia ha sido efectiva para convertir esta serie en estacionaria, se procede a efectuar nuevamente los test adf y kpss: formated_kpss(ahuyama_ts_per[&quot;diff1&quot;].dropna()) ## ( Resultado ## KPSS Stat: 0.191095 ## p-value 0.1 ## Lags 3.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie es estacionaria, 0.1) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is greater than the p-value returned. formated_adf(ahuyama_ts_per[&quot;diff1&quot;].dropna()) ## ( Resultado ## ADF Stat: -6.60208 ## p-value 0.0 ## Crit_val1%: -3.448958 ## Crit_val5%: -2.869739 ## Crit_val10% -2.571138 ## Interpretation La serie es estacionaria, 6.696671678664486e-09) Al revisar los p-valores de los test realizados se puede evidenciar que ambos coinciden en concluir que la primera diferencia de la serie es estacionaria, en este orden de ideas se puede concluir que la serie de precios semanales mayoristas de la Ahuyama para la ciudad de Pereira tiene un nivel de integración 1, es decir es una serie I(1) y por ende para llegar a ser estacionaria requiere ser diferenciada una vez. 3.2.2 Cebolla Junca Al igual que con series anteriores inicialmente se calculan medias moviles y primera y segunda diferencia de la serie de tiempo: cebollaj_ts_per= series_pereira[[&quot;Cebolla junca_per_merca&quot;]] cebollaj_ts_per[&quot;SMA5&quot;]=cebollaj_ts_per[&quot;Cebolla junca_per_merca&quot;].rolling(5).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy cebollaj_ts_per[&quot;SMA10&quot;]=cebollaj_ts_per[&quot;Cebolla junca_per_merca&quot;].rolling(10).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy cebollaj_ts_per[&quot;SMA20&quot;]= cebollaj_ts_per[&quot;Cebolla junca_per_merca&quot;].rolling(20).mean() cebollaj_ts_per[&quot;diff1&quot;]= cebollaj_ts_per[&quot;Cebolla junca_per_merca&quot;].diff() cebollaj_ts_per[&quot;diff2&quot;]= cebollaj_ts_per[&quot;diff1&quot;].diff() A partir de los cálculos se procede a visualizar la serie principal y sus medias moviles de 5,10 y 20 periodos: plt.plot(cebollaj_ts_per.index,cebollaj_ts_per[[&quot;Cebolla junca_per_merca&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.plot(cebollaj_ts_per.index,cebollaj_ts_per[&quot;SMA5&quot;],linewidth=1,color=&quot;red&quot;,label=&quot;SMA5&quot;) plt.plot(cebollaj_ts_per.index,cebollaj_ts_per[&quot;SMA10&quot;],linewidth=1,color=&quot;green&quot;,label=&quot;SMA10&quot;) plt.plot(cebollaj_ts_per.index,cebollaj_ts_per[&quot;SMA20&quot;],linewidth=1,color=&quot;orange&quot;,label=&quot;SMA20&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() Al revisar el comportamiento de la cebolla junca se puede apreciar que tiene un precio volatil con momentos continuos y repetitivos pero que no resultan ser del todo uniformes por ejemplo, los picos de 2021 tienen un alza de precio mucho más leve a la presentada en 2016, 2019, y 2022, esto da lugar a demarcar que hay cierta estacionalidad demarcada por dos picos de precio anuales pero que no resulta ser estable u homogenea. Como observación adicional, es posible apreciar que los a mediados y finales de 2022 son los más pronunciados en la historia del precio y muestran la ruptura del techo de 2500 pesos. En términos generales se aprecia que los minimos de la serie son crecientes lo cual da lugar a establecer que hay presencia de un componente tendencial en la serie de tiempo. Con el fin de seguir identificando caracteristicas de la serie se genera su respectivo lag-plot: pd.plotting.lag_plot(cebollaj_ts_per[&quot;Cebolla junca_per_merca&quot;],lag=3) plt.show() plt.close() El lag plot evidencia una tendencia lineal creciente pero dispersa lo cual da cuenta de una autocorrelación serial moderada, tambien es posible apreciar multiples puntos dispersos lo cual indica dipersión de valores e inclusive presencia de outliers, adicionalmente no se aprecia algun patrón circular que de cuenta de presencia de comportamiento sinusoidal en la serie de tiempo. Ahora con el fin de entende más la dinámica de la autocorrelación, se procede a graficar la autocorrelación serial de la serie: plt.figure(figsize=(15,10)) tsaplots.plot_acf(cebollaj_ts_per[&quot;Cebolla junca_per_merca&quot;].dropna(),title=&quot;Función de autocorrelación para serie de la cebolla junca (Pereira)&quot;) plt.show() plt.close(); Se aprecia que varias observaciones sobrepasan los intervalos de confianza y dan cuenta de una dinámica de autocorrelación serial que se acompaña de un patrón lineal progresivo de descenso que tambien da cuenta de una dinámica tendencial (no estacionaria). Con el fin de seguir comprendiendo la estructura de la serie de procede a revisar su descomposición: plt.figure(figsize=(15,10)) cebollajper_decomp_per= seasonal_decompose(cebollaj_ts_per[&quot;Cebolla junca_per_merca&quot;],model=&quot;multiplicative&quot;) cebollajper_decomp_per.plot() plt.show() plt.close(); Se puede apreciar que la tendencia es creciente en el largo plazo y además en términos de estacionalidad se puede apreciar que hay dos picos de precio en el transcurso del año, uno cerca a mitad del año y otro pico que se intersecta entre el final y el inicio del año siguiente. En sintesis se puede apreciar que hay un componente claro de tendencia por lo tanto hay evidencia de una dinámica estacionaria. En aras de validar se aplican los test estadísticos adf y kpss: formated_kpss(cebollaj_ts_per[&quot;Cebolla junca_per_merca&quot;]) ## ( Resultado ## KPSS Stat: 0.748478 ## p-value 0.01 ## Lags 11.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie no es estacionaria, 0.01) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is smaller than the p-value returned. formated_adf(cebollaj_ts_per[&quot;Cebolla junca_per_merca&quot;]) ## ( Resultado ## ADF Stat: -2.213215 ## p-value 0.201484 ## Crit_val1%: -3.449227 ## Crit_val5%: -2.869857 ## Crit_val10% -2.571201 ## Interpretation La serie no es estacionaria, 0.20148415375247736) Al revisar los p-valores de los test estadisticos se puede apreciar que ambos coindicen en que las series de tiempo no son estacionarias, dada esta situación se hace necesario revisar la primera diferencia de esta serie de tiempo: plt.plot(cebollaj_ts_per.index,cebollaj_ts_per[[&quot;diff1&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() La primera diferencia de la serie muestra con claridad la volatilidad del precio, como se analizaba desde la serie principal hay puntos particulares de variación que demarcan alzas significativas de precio, en general se puede apreciar una corrección del componente tendencial, ahora procedamos a revisar el lag plot de la serie de tiempo: pd.plotting.lag_plot(cebollaj_ts_per[&quot;diff1&quot;].dropna(),lag=3) plt.show() plt.close() El lag plot muestra una dinámica de dispersión que no indica una tendencia clara, esto indica posiblemente una corrección del componente de autocorrelación, para validar esto se genera la gráfica de la función de autocorrelación: plt.figure(figsize=(15,10)) tsaplots.plot_acf(cebollaj_ts_per[&quot;diff1&quot;].dropna(),title=&quot;Función de autocorrelación para serie de la cebolla junca (Pereira) diff1&quot;) plt.show() plt.close(); Al observar la gráfica de función de autocorrelación, se puede observar que tiene 3 observaciones que pasan el umbral de los intervalos de confianza, por lo tanto aun hay algunas observaciones que pasan los umbrales, no obstante no resultan existir observaciones seguidas como en el caso anterior, en sintesis aun hay evidencias de autocorrelación, no obstante esta es mucho más debil, y además las observaciones que presentan autocorrelación no se asocian a un patrón que demarque una tendencia. Ahora se procede a validar si la serie es estacionaria o no estacionaria por medio de las pruebas estadisticas kpss y adf: formated_kpss(cebollaj_ts_per[&quot;diff1&quot;].dropna()) ## ( Resultado ## KPSS Stat: 0.04641 ## p-value 0.1 ## Lags 3.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie es estacionaria, 0.1) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is greater than the p-value returned. formated_adf(cebollaj_ts_per[&quot;diff1&quot;].dropna()) ## ( Resultado ## ADF Stat: -7.509995 ## p-value 0.0 ## Crit_val1%: -3.449227 ## Crit_val5%: -2.869857 ## Crit_val10% -2.571201 ## Interpretation La serie es estacionaria, 4.0392417097819305e-11) Al aplicar los test estadísticos se puede apreciar que ambos coinciden en que la serie de tiempo es estacionaria, teniendo en cuenta esto se puede concluir que la serie de tiempo de precios semanales de la cebolla junca se vuelve estacionaria al aplica la primera diferencia sobre la misma, esto indica que la serie es I(1). 3.2.3 Habichuela Para este caso de la habichuela se sigue el mismo proceso, se calculan las medias moviles de periodos 5,10 y 20; y además se calculan la primera y segunda diferencia de la serie de tiempo. habichuela_ts_per= series_pereira[[&quot;Habichuela_per_merca&quot;]] habichuela_ts_per[&quot;SMA5&quot;]=habichuela_ts_per[&quot;Habichuela_per_merca&quot;].rolling(5).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy habichuela_ts_per[&quot;SMA10&quot;]=habichuela_ts_per[&quot;Habichuela_per_merca&quot;].rolling(10).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy habichuela_ts_per[&quot;SMA20&quot;]= habichuela_ts_per[&quot;Habichuela_per_merca&quot;].rolling(20).mean() habichuela_ts_per[&quot;diff1&quot;]= habichuela_ts_per[&quot;Habichuela_per_merca&quot;].diff() habichuela_ts_per[&quot;diff2&quot;]= habichuela_ts_per[&quot;diff1&quot;].diff() Una vez generados los cálculos anteriormente mencionados se procede a visualizar la serie de tiempo y las medias moviles: plt.plot(habichuela_ts_per.index,habichuela_ts_per[[&quot;Habichuela_per_merca&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.plot(habichuela_ts_per.index,habichuela_ts_per[&quot;SMA5&quot;],linewidth=1,color=&quot;red&quot;,label=&quot;SMA5&quot;) plt.plot(habichuela_ts_per.index,habichuela_ts_per[&quot;SMA10&quot;],linewidth=1,color=&quot;green&quot;,label=&quot;SMA10&quot;) plt.plot(habichuela_ts_per.index,habichuela_ts_per[&quot;SMA20&quot;],linewidth=1,color=&quot;orange&quot;,label=&quot;SMA20&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() Al revisar las series de tiempo de la habichuela para Pereira se puede apreciar una dinámica de precio bastante volatil,los periodos de pico entre cada año resultan ser variables y de hecho se pueden presentar más de dos picos de precio durante un año. Al revisar los máximos y minimos se puede apreciar que el minimo alcanzado por el precio es de 700 pesos y el máximo supera los 5000 pesos, esto representa un rango de variación de 4600 pesos durante toda la ventana de variación de precios, esto representa una volatilidad elevada que tiene sus manifestaciones más fuertes a inicios de 2020 y a inicios de 2022 en donde el precio llega a superar la barrera de los 5000 pesos. Esta revisión gráfica permite evidenciar que la serie es altamente volatil, que tiene una tendencia creciente y que su varianza es elevada y su estacionalidad irregular. Con el fin de seguir conociendo la serie se procede a revisar el lag plot de la misma: pd.plotting.lag_plot(habichuela_ts_per[&quot;Habichuela_per_merca&quot;],lag=3) plt.show() plt.close() Al revisar el lag plot de la serie de tiempo se puede apreciar que describe un patrón lineal disperso mostrando varios puntos alejados de la nube de puntos principal que da cuenta de posibles outliers o valores atipicos, la forma de la nube de puntos da cuenta de una autocorrelación moderada dentro de la distribución. La distribución de los puntos no muestra ningún patrón circular por lo tanto se descarta la presencia de patrones estacionales sinuosoidales lo cual es consistente teniendo en cuenta que la dinámica estacional es variable y no presenta una dinámica fija reconocible como en casos anteriores, asi las cosas, se evidencia a través del lag plot la presencia de correlación serial moderada y presencia de outliers dentro de la serie. Con el fin de validar el problema de autocorrelación seria se genera la gráfica de la función de autocorrelación: plt.figure(figsize=(15,10)) tsaplots.plot_acf(habichuela_ts_per[&quot;Habichuela_per_merca&quot;],title=&quot;Función de autocorrelación para serie de la habichuela (Pereira)&quot;) plt.show() plt.close(); Al observar la función de autocorrelación se puede evidenciar que la autocorrelación resulta ser más fuerte que lo evidenciado en el lag plot, la mayoria de las observaciones están por encima de los intervalos de confianza y además en las primera observaciones se aprecia un patrón decreciente que genera evidencia respecto a una dinámica tendencial, siendo consecuentes con esto se evidencia con claridad que la serie de tiempo analizada tiene una presencia marcada de autocorrelación serial. Con el fin de seguir conociendo las propiedades de la serie se procede a revisar su descomposición: plt.figure(figsize=(15,10)) habic_decomp_per= seasonal_decompose(habichuela_ts_per[&quot;Habichuela_per_merca&quot;],model=&quot;multiplicative&quot;) habic_decomp_per.plot() plt.show() plt.close(); La descomposición permite evidenciar que la dinámica tendencial es mucho más marcada al final de la serie de tiempo, a partir de 2021 comienza a tener una dinámica marcada al alza, antes de este momento la dinámica tendencial es muy leve, en lo que respecta a la estacionalidad es posible confirmar una dinámica bastante irregular en donde hay diversidad de picos y descensos que no permiten demarcar momentos muy claro respecto a la dinámica de la serie en el tiempo, finalmente en lo que respecta a los residuales es posible evidenciar que hacia 2020 hay amplias desviaciones. Los elementos hasta ahora revisados dan evidencia visual suficiente para determinar que la serie es no estacionaria y que además presenta problemas relevantes de autocorrelación serial, no obstante con el fin de confirmar estas afirmaciones se aplican las pruebas adf y kpss: formated_kpss(habichuela_ts_per[&quot;Habichuela_per_merca&quot;]) ## ( Resultado ## KPSS Stat: 1.661752 ## p-value 0.01 ## Lags 10.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie no es estacionaria, 0.01) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is smaller than the p-value returned. formated_adf(habichuela_ts_per[&quot;Habichuela_per_merca&quot;]) ## ( Resultado ## ADF Stat: -1.492563 ## p-value 0.537187 ## Crit_val1%: -3.449282 ## Crit_val5%: -2.869881 ## Crit_val10% -2.571214 ## Interpretation La serie no es estacionaria, 0.5371870947440931) Al aplicar las pruebas se puede evidenciar que ambos test conciden al interpretar sus p-valores en que la serie no es estacionaria, por ende se hace necesario calcular una primera diferencia y determinar si con esta transformación se logra estacionariedad: plt.plot(habichuela_ts_per.index,habichuela_ts_per[[&quot;diff1&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() Al aplicar la primera diferencia de la serie se puede apreciar que esta muestra con bastante claridad la volatilidad de la serie de tiempo, tambien se aprecia de forma muy clara que el momento con variaciones más bruscas resulta ser el inicio del año 2020 en donde se presentan ascensos y descensos bruscos del precio en un periodo de tiempo muy reducido, en e marco del periodo se presentan picos de cambio positivos de más de 2000 pesos en un solo periodo y descensos por el mismo valor, en general se puede apreciar una dinámica mucho más aleatoria y alejada de la presencia de tendencia, no obstante es necesario seguir revisando demás elementos para validar si se ha logrado estacionariedad. pd.plotting.lag_plot(habichuela_ts_per[&quot;diff1&quot;].dropna(),lag=3) plt.show() plt.close() Al observar el lag plot se puede observar que la forma linea de la serie inicial ya no esta presente y ahora se muestra una nube de puntos con alta dispersión, tambien es posible apreciar los valores atipicos que se separan considerablemente de los valores que se encuentran en la nube de puntos principal. Ahora veamos la función de autocorrelación: plt.figure(figsize=(15,10)) tsaplots.plot_acf(habichuela_ts_per[&quot;diff1&quot;].dropna(),title=&quot;Función de autocorrelación para serie de la habichuela (Pereira) diff1&quot;) plt.show() plt.close(); Se puede apreciar que la autocorrelación ha cedido sin embargo aun persisten algunas observaciones que están fuera de los intervalos de confianza, ahora verifiquemos si la serie es estacionaria a partir de las pruebas adf y kpss: formated_kpss(habichuela_ts_per[&quot;diff1&quot;].dropna()) ## ( Resultado ## KPSS Stat: 0.5 ## p-value 0.041667 ## Lags 364.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie no es estacionaria, 0.041666666666668406) formated_adf(habichuela_ts_per[&quot;diff1&quot;].dropna()) ## ( Resultado ## ADF Stat: -7.843374 ## p-value 0.0 ## Crit_val1%: -3.449282 ## Crit_val5%: -2.869881 ## Crit_val10% -2.571214 ## Interpretation La serie es estacionaria, 5.856078659735087e-12) Al revisar las pruebas se puede apreciar que los test no convergen en una misma respuesta, es decir, en este caso la interpretación del p-valor del test kpss indica que la serie no es estacionaria y el p-valor del test adf indica que es estacionaria, en este caso se hace necesario revisar la segunda diferencia de la serie con el fin de validar si se logra tener finalmente una serie estacionaria: plt.plot(habichuela_ts_per.index,habichuela_ts_per[[&quot;diff2&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() La segunda diferencia sigue resaltando la volatilidad de la serie y resalta los movimientos negativos de la misma, no obstante en general se logra mayor estabilidad en el rango de 2000 y -2000 sin embargo resalta el descenso hacia -4000 debido a la segunda diferencia, ahora se revisará brevemente el lag plot y la función de autocorrelación de esta segunda diferencia: pd.plotting.lag_plot(habichuela_ts_per[&quot;diff2&quot;].dropna(),lag=3) plt.show() plt.close() En general se logra apreciar que persiste la nube de puntos con un comportamiento más aleaotorio y alejado de una dinámica o patrón que muestre una tendencia, y asi mismo es posible apreciar la persistencia de outliers que están ligados a los cambios bruscos de precio que ha sufrido este producto en la ventana de tiempo analizada. plt.figure(figsize=(15,10)) tsaplots.plot_acf(habichuela_ts_per[&quot;diff2&quot;].dropna(),title=&quot;Función de autocorrelación para serie de la habichuela (Pereira) diff2&quot;) plt.show() plt.close(); La función de autocorrrelación permite apreciar que aun con una segunda diferencia la autocorrelación persiste mostrando aun varias observaciones por fuera de los intervalos de confianza. Finalmente se procede a realizar nuevamente las pruebas kpss y adf para validar si la segunda diferencia de la serie resulta ser estacionaria: formated_kpss(habichuela_ts_per[&quot;diff2&quot;].dropna()) ## ( Resultado ## KPSS Stat: 0.070635 ## p-value 0.1 ## Lags 29.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie es estacionaria, 0.1) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is greater than the p-value returned. formated_adf(habichuela_ts_per[&quot;diff2&quot;].dropna()) ## ( Resultado ## ADF Stat: -8.8016 ## p-value 0.0 ## Crit_val1%: -3.449337 ## Crit_val5%: -2.869906 ## Crit_val10% -2.571227 ## Interpretation La serie es estacionaria, 2.1049934525909725e-14) Al analizar los p-valores de los test se logra apreciar que al aplicar una segunda diferencia sobre la serie se logra finalmente obtener una serie estacionaria, esto permite concluir que la serie de la habichuela para la ciudad de Pereira resulta ser una serie de tiempo con nivel de integración 2 I(2) lo cual significa que para que esta serie sea estacionaria es necesario aplicar una segunda diferencia sobre la serie de tiempo. 3.2.4 Tomate Chonto Finalmente se aplica la misma metodologia a la serie del tomate chonto para la ciudad de Pereira calculando las respectivas medias moviles y la primera y segunda diferencia: tomate_ts_per= series_pereira[[&quot;Tomate chonto_per_merca&quot;]] tomate_ts_per[&quot;SMA5&quot;]=tomate_ts_per[&quot;Tomate chonto_per_merca&quot;].rolling(5).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy tomate_ts_per[&quot;SMA10&quot;]=tomate_ts_per[&quot;Tomate chonto_per_merca&quot;].rolling(10).mean() ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy tomate_ts_per[&quot;SMA20&quot;]= tomate_ts_per[&quot;Tomate chonto_per_merca&quot;].rolling(20).mean() tomate_ts_per[&quot;diff1&quot;]= tomate_ts_per[&quot;Tomate chonto_per_merca&quot;].diff() tomate_ts_per[&quot;diff2&quot;]= tomate_ts_per[&quot;diff1&quot;].diff() A partir de lo anteriormente calculado se procede a visualizar la serie y las medias moviles: plt.plot(tomate_ts_per.index,tomate_ts_per[[&quot;Tomate chonto_per_merca&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.plot(tomate_ts_per.index,tomate_ts_per[&quot;SMA5&quot;],linewidth=1,color=&quot;red&quot;,label=&quot;SMA5&quot;) plt.plot(tomate_ts_per.index,tomate_ts_per[&quot;SMA10&quot;],linewidth=1,color=&quot;green&quot;,label=&quot;SMA10&quot;) plt.plot(tomate_ts_per.index,tomate_ts_per[&quot;SMA20&quot;],linewidth=1,color=&quot;orange&quot;,label=&quot;SMA20&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() Al observar la serie de tiempo se puede identificar hasta finales de 2021 una oscilación de precios entre los 900 pesos y los 3300 pesos como valores máximo, a inicios de 2022 es posible evidenciar una alza notoria en donde primero se alcanza un precio de 3500 y posteriormente hacia mediados del año en mención se llega a un valor por encima de los 5000 pesos y luego se desciende a valores cercanos a los 3500 llegando hacia Julio y Agosto a niveles de precio cercanos a los 1800. En términos de estacionalidad se pueden identificar como minimo dos picos anuales, sin embargo esta dinámica no es fija o persistente por el contrario se torna variable en algunos periodos en donde pueden llegar a presentarse más de dos picos de precio en el año, asi mismo la amplitud de los incrementos o movimientos del precio no convergen siempre a un punto común, por ejemplo durante 2020 los picos solo llegaron como máximo a un precio de 2500 mientras en años anteriorees habian superado el umbral de los 3000. En concordancia a lo mencionado la visualización de la serie permite demarcar que hay presencia de tendencia creciente pero no constante o con velocidad uniforme, la mayor aceleración de la serie se dentro del tramo de 2022, seguidamente la estacionalidad no resulta ser regular. Con el fin de ahondar más en las caracteristicas de la serie, se procede a revisar su lag plot: pd.plotting.lag_plot(tomate_ts_per[&quot;Tomate chonto_per_merca&quot;],lag=3) plt.show() plt.close() A partir del lag plot es posible apreciar que la serie tiene una tendencia lineal dispersa y que además dentro de la misma resaltan outliers que se encuentran muy alejados de la nube de puntos principal, es preciso resalar que la tendencia lineal de la nube no es fuerte y por lo tanto representa una dinámica de autocorrelación serial moderada. Con el fin de conocer un poco más la naturaleza de la autocorrelación se procede a gráficar la función de autocorrelación de la serie: plt.figure(figsize=(15,10)) tsaplots.plot_acf(tomate_ts_per[&quot;Tomate chonto_per_merca&quot;],title=&quot;Función de autocorrelación para serie del tomate chonto (Pereira)&quot;) plt.show() plt.close(); Se puede apreciar que 7 observaciones están por encima de los intervalor de confianza, y que estos elementos tienen una dinámica decreciente que evidencia cierta linealidad lo cual implica no solo presencia de autocorrelación lineal sino que tambien indica la presencia de tendencia, sin embargo la autocorrelación no es persistente en toda la serie y se evidencia que a partir de la observación 8 tiene una clara convergencia a 0. Ahora veamos la descomposición de esta serie de tiempo: plt.figure(figsize=(15,10)) tomate_decomp_per= seasonal_decompose(tomate_ts_per[&quot;Tomate chonto_per_merca&quot;],model=&quot;multiplicative&quot;) tomate_decomp_per.plot() plt.show() plt.close(); La revisión de la serie de tiempo permite confirmar algunos elementos mencionados con anterioridad, el primero son las observaciones relacionadas a la tendencia, la descomposición muestra con claridad que la tendencia creciente se acentua especialmente a finales de 2021, y a su vez se aprecia que el componente estacional efectivamente es irregular y presenta varios picos a través del tiempo. Los elementos revisados hasta ahora dan pie para demarcar que la serie no es estacionaria, no obstante con el fin de confirmar esto se procede a aplicar las pruebas adf y kpss: formated_kpss(tomate_ts_per[&quot;Tomate chonto_per_merca&quot;]) ## ( Resultado ## KPSS Stat: 0.76546 ## p-value 0.01 ## Lags 10.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie no es estacionaria, 0.01) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is smaller than the p-value returned. formated_adf(tomate_ts_per[&quot;Tomate chonto_per_merca&quot;]) ## ( Resultado ## ADF Stat: -4.184461 ## p-value 0.0007 ## Crit_val1%: -3.448544 ## Crit_val5%: -2.869557 ## Crit_val10% -2.571041 ## Interpretation La serie es estacionaria, 0.0006999628040134919) A partir de la revisión de los p-valores de las pruebas estadisticas es posible evidenciar que no hay convergencia respecto a las conclusiones de las pruebas, dada esta situación se procede a calcular la primera diferencia de la serie con el fin de validar si al aplicar la misma se logra que la serie sea estacionaria: plt.plot(tomate_ts_per.index,tomate_ts_per[[&quot;diff1&quot;]],linewidth=1.2,marker=&quot;o&quot;,color=&quot;blue&quot;,label=&quot;Serie ppal&quot;) plt.ylabel(&quot;Precio (pesos colombianos)&quot;, fontsize=12) plt.xlabel(&quot;Dia de referencia - Inicio semana&quot;,fontsize= 12) plt.xticks(rotation=&#39;vertical&#39;) ## (array([16436., 16801., 17167., 17532., 17897., 18262., 18628., 18993., ## 19358.]), [Text(16436.0, 0, &#39;2015&#39;), Text(16801.0, 0, &#39;2016&#39;), Text(17167.0, 0, &#39;2017&#39;), Text(17532.0, 0, &#39;2018&#39;), Text(17897.0, 0, &#39;2019&#39;), Text(18262.0, 0, &#39;2020&#39;), Text(18628.0, 0, &#39;2021&#39;), Text(18993.0, 0, &#39;2022&#39;), Text(19358.0, 0, &#39;2023&#39;)]) plt.legend(loc=&quot;lower right&quot;) plt.grid(alpha=0.5,linewidth=1) plt.show() plt.close() La visualización de la primera diferencia de la serie permite apreciar claramente los cambios de precio que se asocian a los picos y tambien a los descenso del precio del producto, particularmente se puede apreciar un comportamiento mucho más aleatoria y tambien desmarcado de una dinámica tendencial perceptible. En aras de validar esto se procede a visualizar el lag plot de la serie de tiempo: pd.plotting.lag_plot(tomate_ts_per[&quot;diff1&quot;],lag=3) plt.show() plt.close() Se puede apreciar que la nube de puntos toma una forma dispersa que se aleja del patrón lineal de la serie sin diferenciar, y a su vez se aprecia con claridad una cantidad considerable de puntos que se alejan de la nube de puntos principal lo cual demarca la presencia de varios valores extremos dentro de la serie diferenciada, los cuales se asocian a los puntos de incremento y descenso de las series de tiempo. plt.figure(figsize=(15,10)) tsaplots.plot_acf(tomate_ts_per[&quot;diff1&quot;].dropna(),title=&quot;Función de autocorrelación para serie del tomate chonto (Pereira) diff1&quot;) plt.show() plt.close(); Al apreciar la gráfica de la función de autocorrelación es posible evidenciar que la mayoria de observaciones ahora convergen a 0, sin embargo aun hay una leve presencia de autocorrelación serial ligada a 3 observaciones que están por encima de los intervalos de confianza establecidos. Finalmente se aplican las pruebas estadisticas respectivas ante las cuales se evidencia lo siguiente: formated_kpss(tomate_ts_per[&quot;diff1&quot;].dropna()) ## ( Resultado ## KPSS Stat: 0.132635 ## p-value 0.1 ## Lags 37.0 ## Crit_val10%: 0.347 ## Crit_val5% 0.463 ## Crit_val2.5% 0.574 ## Crit_val1% 0.739 ## Interpretation La serie es estacionaria, 0.1) ## ## &lt;string&gt;:2: InterpolationWarning: The test statistic is outside of the range of p-values available in the ## look-up table. The actual p-value is greater than the p-value returned. formated_adf(tomate_ts_per[&quot;diff1&quot;].dropna()) ## ( Resultado ## ADF Stat: -15.258001 ## p-value 0.0 ## Crit_val1%: -3.448544 ## Crit_val5%: -2.869557 ## Crit_val10% -2.571041 ## Interpretation La serie es estacionaria, 4.905027695947852e-28) El análisis de los p-valores de los test permite identificar ambas pruebas coinciden en demarcar la primera diferencia de la serie como estacionaria, esto lleva a concluir que la serie de tiempo del tomate chonto es I(1) es decir, esta serie llega a ser estacionaria una vez se le aplica la primera diferencia. "],["modelación-para-el-pronóstico-de-las-series-de-tiempo.html", "Capítulo 4 Modelación para el pronóstico de las series de tiempo 4.1 Definición de funciones base para la fase de modelación 4.2 Modelos de suavizamiento Exponencial (Simple,Doble y Triple) 4.3 Modelo ARIMA", " Capítulo 4 Modelación para el pronóstico de las series de tiempo En esta sección se explorarán diferentes modelos para plantear el pronóstico de las series de tiempo de los alimentos escogidos, cada uno de los modelos implementados tendrá un subtitulo correspondiente y al final del capitulo se resumiran los resultados de los modelos implementados de manera consolidada con el fin de determinar cual resulta ser el modelo que presenta un mejor nivel de ajuste. 4.1 Definición de funciones base para la fase de modelación Esta sección tiene la finalidad de definir funciones con la finalidad de simplificar la fase de modelación, en esta sección se calculan elementos como la estacionalidad, y se crean funciones que calculan métricas y gráficas de forma automática para cada modelo a aplicar. 4.1.1 Función para separación de la serie en conjunto de test y entrenamiento import pandas as pd from matplotlib import pyplot as plt import plotly.graph_objects as go import pmdarima from pmdarima.arima import auto_arima from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing def train_test_ts(serie,prop_test): train= serie.iloc[:-int(len(serie)*prop_test)] test= serie.iloc[-int(len(serie)*prop_test):] return train, test 4.1.2 Funciones para aplicación de modelos de suavizamiento exponencial, generación de métricas y gráfica comparativa con plotly def plot_fcast(series, alimento, prop_test, fcast1:list[float], fcast2:list[float], fcast3:list[float], title:str ): serie_train,serie_test= train_test_ts(series[alimento],prop_test) fig=go.Figure() fig.add_trace(go.Scatter(x=serie_train.index,y=serie_train,name=&quot;Train&quot;)) fig.add_trace(go.Scatter(x=serie_test.index,y=serie_test,name=&quot;Test&quot;)) fig.add_trace(go.Scatter(x=serie_test.index,y=fcast1,name=&quot;Simple&quot;)) fig.add_trace(go.Scatter(x=serie_test.index,y=fcast2,name=&quot;Holt&#39;s Linear&quot;)) fig.add_trace(go.Scatter(x=serie_test.index,y=fcast3,name=&quot;Holt Winter&#39;s&quot;)) fig.update_layout(template=&quot;simple_white&quot;, font=dict(size=12), title_text=title, width=700, title_x=0.5, height=400, xaxis_title=&#39;Fecha&#39;, yaxis_title=&#39;Precio (Pesos Colombianos)&#39;) return fig.show() def expon_smooth_mod(serie,alimento,prop_test): serie_train,serie_test= train_test_ts(serie[alimento],prop_test) simple_exp= SimpleExpSmoothing(serie_train).fit(optimized=True) forecast_simple= simple_exp.forecast(len(serie_test)) MAE1= np.mean(np.abs(serie_test-forecast_simple)) MSE1= np.mean(np.square(serie_test-forecast_simple)) RMSE1= np.sqrt(np.mean(np.square(serie_test-forecast_simple))) MAPE1= np.mean(np.abs((serie_test-forecast_simple)/serie_test)*100) double_exp= Holt(serie_train,damped_trend=True).fit(optimized=True) forecast_holt= double_exp.forecast(len(serie_test)) MAE2= np.mean(np.abs(serie_test-forecast_holt)) MSE2= np.mean(np.square(serie_test-forecast_holt)) RMSE2= np.sqrt(np.mean(np.square(serie_test-forecast_holt))) MAPE2= np.mean(np.abs((serie_test-forecast_holt)/serie_test)*100) hw_exp= ExponentialSmoothing(serie_train,trend=&quot;add&quot;,seasonal=&quot;add&quot;,seasonal_periods=52,damped_trend=True).fit(optimized=True) forecast_hw= hw_exp.forecast(len(serie_test)) MAE3= np.mean(np.abs(serie_test-forecast_hw)) MSE3= np.mean(np.square(serie_test-forecast_hw)) RMSE3= np.sqrt(np.mean(np.square(serie_test-forecast_hw))) MAPE3= np.mean(np.abs((serie_test-forecast_hw)/serie_test)*100) results_dict= {&quot;Modelo&quot;:[&quot;Suavizamiento simple&quot;,&quot;Suavizamiento de Holt&quot;,&quot;Holt-Winters&quot;], &quot;MAE&quot;:[MAE1,MAE2,MAE3], &quot;MSE&quot;:[MSE1,MSE2,MSE3], &quot;RMSE&quot;:[RMSE1,RMSE2,RMSE3], &quot;MAPE&quot;:[MAPE1,MAPE2,MAPE3]} results_df= pd.DataFrame.from_dict(results_dict,orient=&quot;columns&quot;) return results_df,forecast_simple,forecast_holt,forecast_hw 4.1.3 Funciones para aplicación de modelo ARIMA, generación de métricas y gráfica comparativa con plotly def plot_fcas_arimat(series, alimento, prop_test, fcast1:list[float], conf_inttbl, title:str): serie_train,serie_test= train_test_ts(series[alimento],prop_test) upper= conf_inttbl[&quot;Lower&quot;] lower= conf_inttbl[&quot;Upper&quot;] fig=go.Figure() fig.add_trace(go.Scatter(x=serie_train.index,y=serie_train,name=&quot;Train&quot;)) fig.add_trace(go.Scatter(x=serie_test.index,y=serie_test,name=&quot;Test&quot;)) fig.add_trace(go.Scatter(x=serie_test.index,y=upper,name=&quot;Limite superior cofint 95%&quot;,mode=&quot;lines&quot;,marker=dict(color=&quot;#444&quot;),showlegend=False)) fig.add_trace(go.Scatter(x=serie_test.index,y=lower,name=&quot;Limite inferior cofint 95%&quot;,mode=&quot;lines&quot;,fillcolor=&quot;rgba(68, 68, 68, 0.3)&quot;,fill=&#39;tonexty&#39;,marker=dict(color=&quot;#444&quot;),showlegend=False)) fig.add_trace(go.Scatter(x=serie_test.index,y=fcast1,name=&quot;Gráfica pronóstico modelo &quot;+title)) fig.update_layout(template=&quot;simple_white&quot;, font=dict(size=12), title_text=title, width=700, title_x=0.5, height=400, xaxis_title=&#39;Fecha&#39;, yaxis_title=&#39;Precio (Pesos Colombianos)&#39;) return fig.show() def perf_auto_arima(serie,alimento,prop_test,fits): serie_train,serie_test= train_test_ts(serie[alimento].dropna(),prop_test) #mod_aut_arima= auto_arima(serie_train,start_p=0,d=None,start_q=0,test=&quot;kpss&quot;, #max_p=5, max_d=2, max_q=5, start_P=0,D=None,start_Q=0, #max_P=5,max_D=5,max_Q=5,m=52,seasonal=True,trace=True,supress_warnings=True,stepwise=True #,n_fits=fits,information_criterion=&quot;aic&quot;,maxiter=500) mod_aut_arima= auto_arima(serie_train,test=&quot;kpss&quot;,seasonal_test=&#39;ocsb&#39;,seasonal=True,m=52, error_action=&#39;ignore&#39;,supress_warnings=True,stepwise=True,trace=True).fit(serie_train) arima_forecast,conf_int= mod_aut_arima.predict(n_periods=len(serie_test),return_conf_int=True,alpha=0.05) resume= mod_aut_arima.summary() opt_mod_name= resume.tables[0][1][1].data conf_int_tbl= pd.DataFrame(conf_int,columns=[&quot;Lower&quot;,&quot;Upper&quot;],index=serie_test.index) MAE1= np.mean(np.abs(serie_test-arima_forecast)) MSE1= np.mean(np.square(serie_test-arima_forecast)) RMSE1= np.sqrt(np.mean(np.square(serie_test-arima_forecast))) MAPE1= np.mean(np.abs((serie_test-arima_forecast)/serie_test)*100) results_dict= {&quot;Modelo&quot;:[opt_mod_name], &quot;MAE&quot;:[MAE1], &quot;MSE&quot;:[MSE1], &quot;RMSE&quot;:[RMSE1], &quot;MAPE&quot;:[MAPE1]} results_df= pd.DataFrame.from_dict(results_dict,orient=&quot;columns&quot;) diagnosis= mod_aut_arima.plot_diagnostics(figsize=(16, 8)) return results_df,resume,arima_forecast,conf_int_tbl,opt_mod_name,diagnosis 4.2 Modelos de suavizamiento Exponencial (Simple,Doble y Triple) 4.2.1 Pronóstico serie de tiempo de la Ahuyama (Ciudad de Armenia) fr_ahuy_axm= expon_smooth_mod(series_armenia,&quot;Ahuyama_axm_merc&quot;,0.2) plot_fcast(series_armenia,&quot;Ahuyama_axm_merc&quot;,0.20,fr_ahuy_axm[1],fr_ahuy_axm[2],fr_ahuy_axm[3],&quot;Pronóstico precio Ahuyama axm (métodos de suavizamiento)&quot;) Al comparar el desempeño de los modelos se tiene lo siguiente fr_ahuy_axm[0] ## Modelo MAE MSE RMSE MAPE ## 0 Suavizamiento simple 327.740346 124505.317640 352.853111 27.193449 ## 1 Suavizamiento de Holt 145.036919 28941.466278 170.121916 12.713245 ## 2 Holt-Winters 147.629499 33234.040198 182.302058 12.501107 4.2.2 Pronóstico serie de tiempo de la Cebolla Junca (Ciudad de Armenia) fr_cebollj_axm= expon_smooth_mod(series_armenia,&quot;Cebolla junca_axm_merc&quot;,0.2) plot_fcast(series_armenia,&quot;Cebolla junca_axm_merc&quot;,0.20,fr_cebollj_axm[1],fr_cebollj_axm[2],fr_cebollj_axm[3],&quot;Pronóstico precio cebolla junca axm (métodos de suavizamiento)&quot;) fr_cebollj_axm[0] ## Modelo MAE MSE RMSE MAPE ## 0 Suavizamiento simple 138.515249 35259.629492 187.775476 7.062118 ## 1 Suavizamiento de Holt 805.666284 822927.669209 907.153608 41.301963 ## 2 Holt-Winters 358.452387 181683.135359 426.243047 18.604651 4.2.3 Pronóstico serie de tiempo de la Habichuela (Ciudad de Armenia) fr_habich_axm= expon_smooth_mod(series_armenia,&quot;Habichuela_axm_merc&quot;,0.2) ## C:\\Users\\DAVID\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:917: ConvergenceWarning: ## ## Optimization failed to converge. Check mle_retvals. plot_fcast(series_armenia,&quot;Habichuela_axm_merc&quot;,0.20,fr_habich_axm[1],fr_habich_axm[2],fr_habich_axm[3],&quot;Pronóstico precio habichuela axm (métodos de suavizamiento)&quot;) fr_habich_axm[0] ## Modelo MAE MSE RMSE MAPE ## 0 Suavizamiento simple 628.802617 6.527327e+05 807.918757 22.828726 ## 1 Suavizamiento de Holt 867.884736 1.037529e+06 1018.591696 33.933986 ## 2 Holt-Winters 678.195263 7.784774e+05 882.313656 28.571685 4.2.4 Pronóstico serie de tiempo del tomate chonto (Ciudad de Armenia) fr_tomate_axm= expon_smooth_mod(series_armenia,&quot;Tomate chonto_axm_merc&quot;,0.2) plot_fcast(series_armenia,&quot;Tomate chonto_axm_merc&quot;,0.20,fr_tomate_axm[1],fr_tomate_axm[2],fr_tomate_axm[3],&quot;Pronóstico precio tomate chonto axm (métodos de suavizamiento)&quot;) fr_tomate_axm[0] ## Modelo MAE MSE RMSE MAPE ## 0 Suavizamiento simple 519.243849 615597.627912 784.600298 19.369871 ## 1 Suavizamiento de Holt 533.862147 636610.299388 797.878624 19.953212 ## 2 Holt-Winters 361.463952 307941.482524 554.924754 13.665090 4.2.5 Pronóstico serie de tiempo de la Ahuyama (Ciudad de Pereira) fr_ahuy_per= expon_smooth_mod(series_pereira,&quot;Ahuyama_per_merca&quot;,0.2) plot_fcast(series_pereira,&quot;Ahuyama_per_merca&quot;,0.20,fr_ahuy_per[1],fr_ahuy_per[2],fr_ahuy_per[3],&quot;Pronóstico precio Ahuyama per (métodos de suavizamiento)&quot;) fr_ahuy_per[0] ## Modelo MAE MSE RMSE MAPE ## 0 Suavizamiento simple 137.952267 24562.873357 156.725471 10.600972 ## 1 Suavizamiento de Holt 245.036365 118496.950958 344.233861 16.610203 ## 2 Holt-Winters 115.006853 17964.465921 134.031586 8.501754 4.2.6 Pronóstico serie de tiempo de la Cebolla Junca (Ciudad de Pereira) fr_ceboll_per= expon_smooth_mod(series_pereira,&quot;Cebolla junca_per_merca&quot;,0.2) plot_fcast(series_pereira,&quot;Cebolla junca_per_merca&quot;,0.20,fr_ceboll_per[1],fr_ceboll_per[2],fr_ceboll_per[3],&quot;Pronóstico precio cebolla per (métodos de suavizamiento)&quot;) fr_ceboll_per[0] ## Modelo MAE MSE RMSE MAPE ## 0 Suavizamiento simple 430.513964 307550.740634 554.572575 24.904458 ## 1 Suavizamiento de Holt 435.264770 257080.385044 507.030951 28.045675 ## 2 Holt-Winters 394.820793 289176.734529 537.751555 21.023638 4.2.7 Pronóstico serie de tiempo de la Habichuela (Ciudad de Pereira) fr_habich_per= expon_smooth_mod(series_pereira,&quot;Habichuela_per_merca&quot;,0.2) ## C:\\Users\\DAVID\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:917: ConvergenceWarning: ## ## Optimization failed to converge. Check mle_retvals. ## ## C:\\Users\\DAVID\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:917: ConvergenceWarning: ## ## Optimization failed to converge. Check mle_retvals. plot_fcast(series_pereira,&quot;Habichuela_per_merca&quot;,0.20,fr_habich_per[1],fr_habich_per[2],fr_habich_per[3],&quot;Pronóstico precio habichuela per (métodos de suavizamiento)&quot;) fr_habich_per[0] ## Modelo MAE MSE RMSE MAPE ## 0 Suavizamiento simple 1188.046127 1.815866e+06 1347.540898 34.836441 ## 1 Suavizamiento de Holt 1193.722017 1.837356e+06 1355.491042 34.852479 ## 2 Holt-Winters 1037.210218 1.381595e+06 1175.412556 32.074182 4.2.8 Pronóstico serie de tiempo del tomate chonto (Ciudad de Pereira) fr_tomat_per= expon_smooth_mod(series_pereira,&quot;Tomate chonto_per_merca&quot;,0.2) plot_fcast(series_pereira,&quot;Tomate chonto_per_merca&quot;,0.20,fr_tomat_per[1],fr_tomat_per[2],fr_tomat_per[3],&quot;Pronóstico precio tomate per (métodos de suavizamiento)&quot;) fr_tomat_per[0] ## Modelo MAE MSE RMSE MAPE ## 0 Suavizamiento simple 659.685588 878273.377545 937.162407 22.930786 ## 1 Suavizamiento de Holt 684.356162 954078.732164 976.769539 23.407191 ## 2 Holt-Winters 448.334651 422896.050403 650.304583 15.858199 4.3 Modelo ARIMA 4.3.1 Pronóstico serie de tiempo de la Ahuyama (Ciudad de Armenia) fr_arima_ahuy_axm= perf_auto_arima(series_armenia,&quot;Ahuyama_axm_merc&quot;,0.20,50) ## Performing stepwise search to minimize aic ## ARIMA(2,1,2)(1,0,1)[52] intercept : AIC=inf, Time=11.57 sec ## ARIMA(0,1,0)(0,0,0)[52] intercept : AIC=3148.431, Time=0.03 sec ## ARIMA(1,1,0)(1,0,0)[52] intercept : AIC=3139.775, Time=2.35 sec ## ARIMA(0,1,1)(0,0,1)[52] intercept : AIC=3139.497, Time=3.41 sec ## ARIMA(0,1,0)(0,0,0)[52] : AIC=3146.699, Time=0.03 sec ## ARIMA(0,1,1)(0,0,0)[52] intercept : AIC=3137.550, Time=0.19 sec ## ARIMA(0,1,1)(1,0,0)[52] intercept : AIC=3139.501, Time=4.67 sec ## ARIMA(0,1,1)(1,0,1)[52] intercept : AIC=inf, Time=5.14 sec ## ARIMA(1,1,1)(0,0,0)[52] intercept : AIC=3139.490, Time=0.26 sec ## ARIMA(0,1,2)(0,0,0)[52] intercept : AIC=3139.473, Time=0.30 sec ## ARIMA(1,1,0)(0,0,0)[52] intercept : AIC=3137.838, Time=0.10 sec ## ARIMA(1,1,2)(0,0,0)[52] intercept : AIC=3141.136, Time=0.60 sec ## ARIMA(0,1,1)(0,0,0)[52] : AIC=3135.787, Time=0.09 sec ## ARIMA(0,1,1)(1,0,0)[52] : AIC=3137.738, Time=1.08 sec ## ARIMA(0,1,1)(0,0,1)[52] : AIC=3137.734, Time=1.81 sec ## ARIMA(0,1,1)(1,0,1)[52] : AIC=inf, Time=3.74 sec ## ARIMA(1,1,1)(0,0,0)[52] : AIC=3137.721, Time=0.10 sec ## ARIMA(0,1,2)(0,0,0)[52] : AIC=3137.703, Time=0.09 sec ## ARIMA(1,1,0)(0,0,0)[52] : AIC=3136.054, Time=0.03 sec ## ARIMA(1,1,2)(0,0,0)[52] : AIC=3139.366, Time=0.18 sec ## ## Best model: ARIMA(0,1,1)(0,0,0)[52] ## Total fit time: 35.789 seconds fr_arima_ahuy_axm[5] plt.show() plot_fcas_arimat(series_armenia,&quot;Ahuyama_axm_merc&quot;,0.20,fr_arima_ahuy_axm[2],fr_arima_ahuy_axm[3],fr_arima_ahuy_axm[4]) fr_arima_ahuy_axm[0] ## Modelo MAE MSE RMSE MAPE ## 0 SARIMAX(0, 1, 1) 366.573962 151489.891256 389.217023 30.584237 4.3.2 Pronóstico serie de tiempo de la cebolla junca (Ciudad de Armenia) 4.3.3 Pronóstico serie de tiempo de la habichuela (Ciudad de Armenia) 4.3.4 Pronóstico serie de tiempo del tomate chonto (Ciudad de Armenia) 4.3.5 Pronóstico serie de tiempo de la Ahuyama (Ciudad de Pereira) 4.3.6 Pronóstico serie de tiempo de la cebolla junca (Ciudad de Pereira) 4.3.7 Pronóstico serie de tiempo de la habichuela (Ciudad de Pereira) 4.3.8 Pronóstico serie de tiempo del tomate chonto (Ciudad de Pereira) "],["evaluación-y-ajuste-de-hiperparámetros-de-los-modelos.html", "Capítulo 5 Evaluación y ajuste de hiperparámetros de los modelos", " Capítulo 5 Evaluación y ajuste de hiperparámetros de los modelos "],["contraste-entre-series-de-tiempo-y-proyección-de-precios-minoristas.html", "Capítulo 6 Contraste entre series de tiempo y proyección de precios minoristas", " Capítulo 6 Contraste entre series de tiempo y proyección de precios minoristas "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
